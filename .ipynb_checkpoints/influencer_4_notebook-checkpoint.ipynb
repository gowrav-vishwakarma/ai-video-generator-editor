{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa804cb3-83bc-443b-aa53-f5c9cd96b599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0414d7be2d44ae0a6c493c9fb5e8f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating script and prompts for topic: A serene moment in a Japanese Zen garden\n",
      "LLM Response:\n",
      " {\n",
      "  \"narration\": [\n",
      "    {\n",
      "      \"scene\": 1,\n",
      "      \"text\": \"In this serene moment, we find ourselves in a Japanese Zen garden. The gentle sound of water flowing fills the air as we take in the peaceful surroundings.\",\n",
      "      \"duration_estimate\": 5.5\n",
      "    },\n",
      "    {\n",
      "      \"scene\": 2,\n",
      "      \"text\": \"The raked sand is a masterpiece, a work of art that has been meticulously crafted to evoke a sense of calm and tranquility.\",\n",
      "      \"duration_estimate\": 4.5\n",
      "    }\n",
      "  ],\n",
      "  \"visuals\": [\n",
      "    {\n",
      "      \"scene\": 1,\n",
      "      \"prompt\": \"A close-up of a raked sand garden, featuring a tranquil pond surrounded by rocks and carefully placed plants. The camera should pan out to show the entire garden, revealing the meticulous attention to detail in every aspect of the scene. The lighting should be soft and diffused, creating a serene and calming mood. The image should be captured in a cinematic style, with a shallow depth of field to blur the background and draw the viewer's attention to the focal point of the scene.\"\n",
      "    },\n",
      "    {\n",
      "      \"scene\": 2,\n",
      "      \"prompt\": \"A bird's-eye view of the same garden, showcasing the intricate design and symmetry of the raked sand patterns. The camera should slowly zoom in to reveal the fine details of the sand, highlighting the texture and depth of the design. The lighting should be bright and even, creating a realistic and immersive experience for the viewer. The image should be captured in a realistic style, with a deep depth of field to capture the entire scene in sharp detail.\"\n",
      "    }\n",
      "  ],\n",
      "  \"hashtags\": [\"zen_garden\", \"japanese_zen_garden\", \"peaceful_moment\"]\n",
      "}\n",
      "Extracted JSON block for parsing:\n",
      "{\n",
      "  \"narration\": [\n",
      "    {\n",
      "      \"scene\": 1,\n",
      "      \"text\": \"In this serene moment, we find ourselves in a Japanese Zen garden. The gentle sound of water flowing fills the air as we take in the peaceful surroundings.\",\n",
      "      \"duration_estimate\": 5.5\n",
      "    },\n",
      "    {\n",
      "      \"scene\": 2,\n",
      "      \"text\": \"The raked sand is a masterpiece, a work of art that has been meticulously crafted to evoke a sense of calm and tranquility.\",\n",
      "      \"duration_estimate\": 4.5\n",
      "    }\n",
      "  ],\n",
      "  \"visuals\": [\n",
      "    {\n",
      "      \"scene\": 1,\n",
      "      \"prompt\": \"A close-up of a raked sand garden, featuring a tranquil pond surrounded by rocks and carefully placed plants. The camera should pan out to show the entire garden, revealing the meticulous attention to detail in every aspect of the scene. The lighting should be soft and diffused, creating a serene and calming mood. The image should be captured in a cinematic style, with a shallow depth of field to blur the background and draw the viewer's attention to the focal point of the scene.\"\n",
      "    },\n",
      "    {\n",
      "      \"scene\": 2,\n",
      "      \"prompt\": \"A bird's-eye view of the same garden, showcasing the intricate design and symmetry of the raked sand patterns. The camera should slowly zoom in to reveal the fine details of the sand, highlighting the texture and depth of the design. The lighting should be bright and even, creating a realistic and immersive experience for the viewer. The image should be captured in a realistic style, with a deep depth of field to capture the entire scene in sharp detail.\"\n",
      "    }\n",
      "  ],\n",
      "  \"hashtags\": [\"zen_garden\", \"japanese_zen_garden\", \"peaceful_moment\"]\n",
      "}\n",
      "VRAM cleared and memory collected.\n",
      "Loading TTS model...\n",
      "Loading T2I pipeline (SDXL)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626de742fe2c42e494068e81172ffbeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading I2V pipeline (SVD)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c7f9a403574e0db2d716d3add08f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Scene 1/2 ---\n",
      "Generating audio for scene 0: \"In this serene moment, we find ourselves in a Japa...\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (101 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"with a shallow depth of field to blur the background and draw the viewer's attention to the focal point of the scene.\"]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (101 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"with a shallow depth of field to blur the background and draw the viewer's attention to the focal point of the scene.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio for scene 0 saved to my_video_project/scene_0_audio.wav\n",
      "Actual audio duration for scene 0: 16.62s\n",
      "Scene 0: Audio duration 16.62s. Needs 6 video chunk(s).\n",
      "  Generating video chunk 1/6 for scene 0...\n",
      "Scene 0, Chunk 0: Generating keyframe image for: \"A close-up of a raked sand garden, featuring a tra...\"\n",
      "Scene 0, Chunk 0: SDXL target image size: 536x960\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f248dfde4c448dbb51a1915dbf5861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyframe image for scene 0, chunk 0 saved to my_video_project/scene_0_chunk_0_keyframe.png\n",
      "Scene 0, Chunk 0: Generating video from image (25 frames) using SVD...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18997c9c9bc041cfa0e3489dfc30b54a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"with a shallow depth of field to blur the background and draw the viewer's attention to the focal point of the scene.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"with a shallow depth of field to blur the background and draw the viewer's attention to the focal point of the scene.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD video chunk for scene 0, chunk 0 saved to my_video_project/scene_0_chunk_0_svd.mp4\n",
      "{'video_found': True, 'audio_found': False, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf61.1.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1024, 576], 'bitrate': 659, 'fps': 10.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]', 'encoder': 'Lavc61.3.100 libx264'}}], 'input_number': 0}], 'duration': 2.5, 'bitrate': 662, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1024, 576], 'video_bitrate': 659, 'video_fps': 10.0, 'video_duration': 2.5, 'video_n_frames': 25}\n",
      "/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i my_video_project/scene_0_chunk_0_svd.mp4 -loglevel error -f image2pipe -vf scale=1024:576 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "  Generating video chunk 2/6 for scene 0...\n",
      "Scene 0, Chunk 1: Generating keyframe image for: \"A close-up of a raked sand garden, featuring a tra...\"\n",
      "Scene 0, Chunk 1: SDXL target image size: 536x960\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d5759140d540afbab9e94b3fc99b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyframe image for scene 0, chunk 1 saved to my_video_project/scene_0_chunk_1_keyframe.png\n",
      "Scene 0, Chunk 1: Generating video from image (25 frames) using SVD...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b3e5d46ebd4527a7b1383b13d2b54f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"with a shallow depth of field to blur the background and draw the viewer's attention to the focal point of the scene.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"with a shallow depth of field to blur the background and draw the viewer's attention to the focal point of the scene.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD video chunk for scene 0, chunk 1 saved to my_video_project/scene_0_chunk_1_svd.mp4\n",
      "{'video_found': True, 'audio_found': False, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf61.1.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1024, 576], 'bitrate': 599, 'fps': 10.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]', 'encoder': 'Lavc61.3.100 libx264'}}], 'input_number': 0}], 'duration': 2.5, 'bitrate': 602, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1024, 576], 'video_bitrate': 599, 'video_fps': 10.0, 'video_duration': 2.5, 'video_n_frames': 25}\n",
      "/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i my_video_project/scene_0_chunk_1_svd.mp4 -loglevel error -f image2pipe -vf scale=1024:576 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "  Generating video chunk 3/6 for scene 0...\n",
      "Scene 0, Chunk 2: Generating keyframe image for: \"A close-up of a raked sand garden, featuring a tra...\"\n",
      "Scene 0, Chunk 2: SDXL target image size: 536x960\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d61fd168c54ac88364ca2b9d31054f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyframe image for scene 0, chunk 2 saved to my_video_project/scene_0_chunk_2_keyframe.png\n",
      "Scene 0, Chunk 2: Generating video from image (25 frames) using SVD...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c62ee693af4a6e98d4a94d1cc626e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"with a shallow depth of field to blur the background and draw the viewer's attention to the focal point of the scene.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"with a shallow depth of field to blur the background and draw the viewer's attention to the focal point of the scene.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD video chunk for scene 0, chunk 2 saved to my_video_project/scene_0_chunk_2_svd.mp4\n",
      "{'video_found': True, 'audio_found': False, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf61.1.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1024, 576], 'bitrate': 801, 'fps': 10.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]', 'encoder': 'Lavc61.3.100 libx264'}}], 'input_number': 0}], 'duration': 2.5, 'bitrate': 805, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1024, 576], 'video_bitrate': 801, 'video_fps': 10.0, 'video_duration': 2.5, 'video_n_frames': 25}\n",
      "/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i my_video_project/scene_0_chunk_2_svd.mp4 -loglevel error -f image2pipe -vf scale=1024:576 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "  Generating video chunk 4/6 for scene 0...\n",
      "Scene 0, Chunk 3: Generating keyframe image for: \"A close-up of a raked sand garden, featuring a tra...\"\n",
      "Scene 0, Chunk 3: SDXL target image size: 536x960\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1ef73e749c4938843301596dd9cb27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyframe image for scene 0, chunk 3 saved to my_video_project/scene_0_chunk_3_keyframe.png\n",
      "Scene 0, Chunk 3: Generating video from image (25 frames) using SVD...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00812072d53f403a96361affd647609c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"with a shallow depth of field to blur the background and draw the viewer's attention to the focal point of the scene.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"with a shallow depth of field to blur the background and draw the viewer's attention to the focal point of the scene.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD video chunk for scene 0, chunk 3 saved to my_video_project/scene_0_chunk_3_svd.mp4\n",
      "{'video_found': True, 'audio_found': False, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf61.1.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1024, 576], 'bitrate': 704, 'fps': 10.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]', 'encoder': 'Lavc61.3.100 libx264'}}], 'input_number': 0}], 'duration': 2.5, 'bitrate': 707, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1024, 576], 'video_bitrate': 704, 'video_fps': 10.0, 'video_duration': 2.5, 'video_n_frames': 25}\n",
      "/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i my_video_project/scene_0_chunk_3_svd.mp4 -loglevel error -f image2pipe -vf scale=1024:576 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "  Generating video chunk 5/6 for scene 0...\n",
      "Scene 0, Chunk 4: Generating keyframe image for: \"A close-up of a raked sand garden, featuring a tra...\"\n",
      "Scene 0, Chunk 4: SDXL target image size: 536x960\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "035091b5a8404479979a849d33b0a295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyframe image for scene 0, chunk 4 saved to my_video_project/scene_0_chunk_4_keyframe.png\n",
      "Scene 0, Chunk 4: Generating video from image (25 frames) using SVD...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0177b4eac7e3414fb8b2651ddbca1a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"with a shallow depth of field to blur the background and draw the viewer's attention to the focal point of the scene.\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"with a shallow depth of field to blur the background and draw the viewer's attention to the focal point of the scene.\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD video chunk for scene 0, chunk 4 saved to my_video_project/scene_0_chunk_4_svd.mp4\n",
      "{'video_found': True, 'audio_found': False, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf61.1.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1024, 576], 'bitrate': 945, 'fps': 10.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]', 'encoder': 'Lavc61.3.100 libx264'}}], 'input_number': 0}], 'duration': 2.5, 'bitrate': 949, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1024, 576], 'video_bitrate': 945, 'video_fps': 10.0, 'video_duration': 2.5, 'video_n_frames': 25}\n",
      "/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i my_video_project/scene_0_chunk_4_svd.mp4 -loglevel error -f image2pipe -vf scale=1024:576 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "  Generating video chunk 6/6 for scene 0...\n",
      "Scene 0, Chunk 5: Generating keyframe image for: \"A close-up of a raked sand garden, featuring a tra...\"\n",
      "Scene 0, Chunk 5: SDXL target image size: 536x960\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4772f196c4e411e8ae797cf771ec24b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyframe image for scene 0, chunk 5 saved to my_video_project/scene_0_chunk_5_keyframe.png\n",
      "Scene 0, Chunk 5: Generating video from image (25 frames) using SVD...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40d6164c8464041bbac046089ca7fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD video chunk for scene 0, chunk 5 saved to my_video_project/scene_0_chunk_5_svd.mp4\n",
      "{'video_found': True, 'audio_found': False, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf61.1.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1024, 576], 'bitrate': 424, 'fps': 10.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]', 'encoder': 'Lavc61.3.100 libx264'}}], 'input_number': 0}], 'duration': 2.5, 'bitrate': 428, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1024, 576], 'video_bitrate': 424, 'video_fps': 10.0, 'video_duration': 2.5, 'video_n_frames': 25}\n",
      "/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i my_video_project/scene_0_chunk_5_svd.mp4 -loglevel error -f image2pipe -vf scale=1024:576 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "Assembling video for scene 0 from 6 sub-clips to match duration 16.62s.\n",
      "{'video_found': True, 'audio_found': False, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf61.1.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1024, 576], 'bitrate': 659, 'fps': 10.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]', 'encoder': 'Lavc61.3.100 libx264'}}], 'input_number': 0}], 'duration': 2.5, 'bitrate': 662, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1024, 576], 'video_bitrate': 659, 'video_fps': 10.0, 'video_duration': 2.5, 'video_n_frames': 25}\n",
      "/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i my_video_project/scene_0_chunk_0_svd.mp4 -loglevel error -f image2pipe -vf scale=1024:576 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "{'video_found': True, 'audio_found': False, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf61.1.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1024, 576], 'bitrate': 599, 'fps': 10.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]', 'encoder': 'Lavc61.3.100 libx264'}}], 'input_number': 0}], 'duration': 2.5, 'bitrate': 602, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1024, 576], 'video_bitrate': 599, 'video_fps': 10.0, 'video_duration': 2.5, 'video_n_frames': 25}\n",
      "/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i my_video_project/scene_0_chunk_1_svd.mp4 -loglevel error -f image2pipe -vf scale=1024:576 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "{'video_found': True, 'audio_found': False, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf61.1.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1024, 576], 'bitrate': 801, 'fps': 10.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]', 'encoder': 'Lavc61.3.100 libx264'}}], 'input_number': 0}], 'duration': 2.5, 'bitrate': 805, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1024, 576], 'video_bitrate': 801, 'video_fps': 10.0, 'video_duration': 2.5, 'video_n_frames': 25}\n",
      "/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i my_video_project/scene_0_chunk_2_svd.mp4 -loglevel error -f image2pipe -vf scale=1024:576 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "{'video_found': True, 'audio_found': False, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf61.1.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1024, 576], 'bitrate': 704, 'fps': 10.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]', 'encoder': 'Lavc61.3.100 libx264'}}], 'input_number': 0}], 'duration': 2.5, 'bitrate': 707, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1024, 576], 'video_bitrate': 704, 'video_fps': 10.0, 'video_duration': 2.5, 'video_n_frames': 25}\n",
      "/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i my_video_project/scene_0_chunk_3_svd.mp4 -loglevel error -f image2pipe -vf scale=1024:576 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "{'video_found': True, 'audio_found': False, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf61.1.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1024, 576], 'bitrate': 945, 'fps': 10.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]', 'encoder': 'Lavc61.3.100 libx264'}}], 'input_number': 0}], 'duration': 2.5, 'bitrate': 949, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1024, 576], 'video_bitrate': 945, 'video_fps': 10.0, 'video_duration': 2.5, 'video_n_frames': 25}\n",
      "/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i my_video_project/scene_0_chunk_4_svd.mp4 -loglevel error -f image2pipe -vf scale=1024:576 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "{'video_found': True, 'audio_found': False, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf61.1.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1024, 576], 'bitrate': 424, 'fps': 10.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]', 'encoder': 'Lavc61.3.100 libx264'}}], 'input_number': 0}], 'duration': 2.5, 'bitrate': 428, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1024, 576], 'video_bitrate': 424, 'video_fps': 10.0, 'video_duration': 2.5, 'video_n_frames': 25}\n",
      "/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i my_video_project/scene_0_chunk_5_svd.mp4 -loglevel error -f image2pipe -vf scale=1024:576 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "{'video_found': True, 'audio_found': False, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf61.1.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1024, 576], 'bitrate': 659, 'fps': 10.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]', 'encoder': 'Lavc61.3.100 libx264'}}], 'input_number': 0}], 'duration': 2.5, 'bitrate': 662, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1024, 576], 'video_bitrate': 659, 'video_fps': 10.0, 'video_duration': 2.5, 'video_n_frames': 25}\n",
      "/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i my_video_project/scene_0_chunk_0_svd.mp4 -loglevel error -f image2pipe -vf scale=1024:576 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "Assembled video for scene 0 saved to my_video_project/scene_0_assembled_video.mp4 with duration 16.62s.\n",
      "--- Scene 1 processing took 359.54s ---\n",
      "\n",
      "--- Processing Scene 2/2 ---\n",
      "Generating audio for scene 1: \"The raked sand is a masterpiece, a work of art tha...\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', with a deep depth of field to capture the entire scene in sharp detail.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', with a deep depth of field to capture the entire scene in sharp detail.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio for scene 1 saved to my_video_project/scene_1_audio.wav\n",
      "Actual audio duration for scene 1: 8.50s\n",
      "Scene 1: Audio duration 8.50s. Needs 3 video chunk(s).\n",
      "  Generating video chunk 1/3 for scene 1...\n",
      "Scene 1, Chunk 0: Generating keyframe image for: \"A bird's-eye view of the same garden, showcasing t...\"\n",
      "Scene 1, Chunk 0: SDXL target image size: 536x960\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc3e178faea4303bfb687789d637607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyframe image for scene 1, chunk 0 saved to my_video_project/scene_1_chunk_0_keyframe.png\n",
      "Scene 1, Chunk 0: Generating video from image (25 frames) using SVD...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70533eaa7e5b4ef984b010964383cbcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', with a deep depth of field to capture the entire scene in sharp detail.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', with a deep depth of field to capture the entire scene in sharp detail.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD video chunk for scene 1, chunk 0 saved to my_video_project/scene_1_chunk_0_svd.mp4\n",
      "{'video_found': True, 'audio_found': False, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf61.1.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1024, 576], 'bitrate': 859, 'fps': 10.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]', 'encoder': 'Lavc61.3.100 libx264'}}], 'input_number': 0}], 'duration': 2.5, 'bitrate': 862, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1024, 576], 'video_bitrate': 859, 'video_fps': 10.0, 'video_duration': 2.5, 'video_n_frames': 25}\n",
      "/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i my_video_project/scene_1_chunk_0_svd.mp4 -loglevel error -f image2pipe -vf scale=1024:576 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "  Generating video chunk 2/3 for scene 1...\n",
      "Scene 1, Chunk 1: Generating keyframe image for: \"A bird's-eye view of the same garden, showcasing t...\"\n",
      "Scene 1, Chunk 1: SDXL target image size: 536x960\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21ee5021fc34e468271e03f78fe8e19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyframe image for scene 1, chunk 1 saved to my_video_project/scene_1_chunk_1_keyframe.png\n",
      "Scene 1, Chunk 1: Generating video from image (25 frames) using SVD...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88fb60301c4348cf8c7c0cf5549788f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', with a deep depth of field to capture the entire scene in sharp detail.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', with a deep depth of field to capture the entire scene in sharp detail.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD video chunk for scene 1, chunk 1 saved to my_video_project/scene_1_chunk_1_svd.mp4\n",
      "{'video_found': True, 'audio_found': False, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf61.1.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1024, 576], 'bitrate': 1398, 'fps': 10.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]', 'encoder': 'Lavc61.3.100 libx264'}}], 'input_number': 0}], 'duration': 2.5, 'bitrate': 1401, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1024, 576], 'video_bitrate': 1398, 'video_fps': 10.0, 'video_duration': 2.5, 'video_n_frames': 25}\n",
      "/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i my_video_project/scene_1_chunk_1_svd.mp4 -loglevel error -f image2pipe -vf scale=1024:576 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "  Generating video chunk 3/3 for scene 1...\n",
      "Scene 1, Chunk 2: Generating keyframe image for: \"A bird's-eye view of the same garden, showcasing t...\"\n",
      "Scene 1, Chunk 2: SDXL target image size: 536x960\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b94e406075c4e23933b610c639fcb2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyframe image for scene 1, chunk 2 saved to my_video_project/scene_1_chunk_2_keyframe.png\n",
      "Scene 1, Chunk 2: Generating video from image (25 frames) using SVD...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed05d78fe7442b19c68d5ff2aa705ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD video chunk for scene 1, chunk 2 saved to my_video_project/scene_1_chunk_2_svd.mp4\n",
      "{'video_found': True, 'audio_found': False, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf61.1.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1024, 576], 'bitrate': 1198, 'fps': 10.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]', 'encoder': 'Lavc61.3.100 libx264'}}], 'input_number': 0}], 'duration': 2.5, 'bitrate': 1201, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1024, 576], 'video_bitrate': 1198, 'video_fps': 10.0, 'video_duration': 2.5, 'video_n_frames': 25}\n",
      "/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i my_video_project/scene_1_chunk_2_svd.mp4 -loglevel error -f image2pipe -vf scale=1024:576 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "Assembling video for scene 1 from 3 sub-clips to match duration 8.50s.\n",
      "{'video_found': True, 'audio_found': False, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf61.1.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1024, 576], 'bitrate': 859, 'fps': 10.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]', 'encoder': 'Lavc61.3.100 libx264'}}], 'input_number': 0}], 'duration': 2.5, 'bitrate': 862, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1024, 576], 'video_bitrate': 859, 'video_fps': 10.0, 'video_duration': 2.5, 'video_n_frames': 25}\n",
      "/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i my_video_project/scene_1_chunk_0_svd.mp4 -loglevel error -f image2pipe -vf scale=1024:576 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "{'video_found': True, 'audio_found': False, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf61.1.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1024, 576], 'bitrate': 1398, 'fps': 10.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]', 'encoder': 'Lavc61.3.100 libx264'}}], 'input_number': 0}], 'duration': 2.5, 'bitrate': 1401, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1024, 576], 'video_bitrate': 1398, 'video_fps': 10.0, 'video_duration': 2.5, 'video_n_frames': 25}\n",
      "/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i my_video_project/scene_1_chunk_1_svd.mp4 -loglevel error -f image2pipe -vf scale=1024:576 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "{'video_found': True, 'audio_found': False, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf61.1.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1024, 576], 'bitrate': 1198, 'fps': 10.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]', 'encoder': 'Lavc61.3.100 libx264'}}], 'input_number': 0}], 'duration': 2.5, 'bitrate': 1201, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1024, 576], 'video_bitrate': 1198, 'video_fps': 10.0, 'video_duration': 2.5, 'video_n_frames': 25}\n",
      "/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i my_video_project/scene_1_chunk_2_svd.mp4 -loglevel error -f image2pipe -vf scale=1024:576 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "{'video_found': True, 'audio_found': False, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf61.1.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1024, 576], 'bitrate': 859, 'fps': 10.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]', 'encoder': 'Lavc61.3.100 libx264'}}], 'input_number': 0}], 'duration': 2.5, 'bitrate': 862, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1024, 576], 'video_bitrate': 859, 'video_fps': 10.0, 'video_duration': 2.5, 'video_n_frames': 25}\n",
      "/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i my_video_project/scene_1_chunk_0_svd.mp4 -loglevel error -f image2pipe -vf scale=1024:576 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "Assembled video for scene 1 saved to my_video_project/scene_1_assembled_video.mp4 with duration 8.50s.\n",
      "--- Scene 2 processing took 177.31s ---\n",
      "VRAM cleared and memory collected.\n",
      "VRAM cleared and memory collected.\n",
      "Assembling final reel...\n",
      "An unhandled error occurred in main_automation_flow: cannot access local variable 'font_path' where it is not associated with a value\n",
      "Final cleanup: Ensuring all models are cleared from VRAM...\n",
      "Cleanup finished.\n",
      "Video generation failed or produced no output.\n",
      "Total execution time: 9.42 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_52182/1963835955.py\", line 762, in main_automation_flow\n",
      "    final_video_path = assemble_final_reel(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_52182/1963835955.py\", line 448, in assemble_final_reel\n",
      "    if not os.path.exists(font_path):\n",
      "                          ^^^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'font_path' where it is not associated with a value\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Developed by Gowrav Vishwakarma \n",
    "# https://www.linkedin.com/in/gowravvishwakarma/\n",
    "\n",
    "# main_script.py\n",
    "import os\n",
    "# Set PyTorch memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import math # For math.ceil\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple, Dict, Any\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from diffusers import StableDiffusionXLPipeline, StableVideoDiffusionPipeline, DiffusionPipeline\n",
    "from diffusers.utils import load_image, export_to_video\n",
    "from TTS.api import TTS # Coqui TTS\n",
    "from moviepy import VideoFileClip, AudioFileClip, concatenate_videoclips, TextClip, CompositeVideoClip\n",
    "from moviepy.audio.AudioClip import concatenate_audioclips, AudioClip \n",
    "from moviepy.video.fx.Crop import Crop # Correct import based on your docs\n",
    "import gc # Garbage collection\n",
    "import numpy as np\n",
    "import time # For unique filenames if needed, and timing\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ContentConfig:\n",
    "    \"\"\"Configuration for content generation\"\"\"\n",
    "    # Video settings\n",
    "    target_video_length_hint: float = 30.0  # Target total video length for LLM guidance\n",
    "    \n",
    "    # Max duration a single video chunk generated by the model can be\n",
    "    # (e.g., SVD generates ~2-4 secs effectively, ModelScope similar)\n",
    "    model_max_video_chunk_duration: float = 3.0 \n",
    "    \n",
    "    target_resolution: tuple = (1080, 1920)  # Instagram Reel 9:16\n",
    "    fps: int = 8\n",
    "    \n",
    "    # Scene settings (for LLM guidance)\n",
    "    min_scenes: int = 2 # Min narration/visual scene pairs from LLM\n",
    "    max_scenes: int = 5 # Max narration/visual scene pairs from LLM\n",
    "    # LLM hint for how long each narration part should ideally be\n",
    "    max_scene_narration_duration_hint: float = 5.0 \n",
    "    \n",
    "    # Model settings\n",
    "    use_svd_flow: bool = True  # Use SDXL -> SVD flow instead of direct T2V\n",
    "    \n",
    "    # Output settings\n",
    "    output_dir: str = \"instagram_content\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- 0. CONFIGURATION ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# LLM paths\n",
    "LLM_MODEL_ID = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "# TTS model\n",
    "TTS_MODEL_ID = \"tts_models/multilingual/multi-dataset/xtts_v2\"\n",
    "# T2I model\n",
    "T2I_MODEL_ID = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "T2I_REFINER_ID = \"stabilityai/stable-diffusion-xl-refiner-1.0\" # Optional\n",
    "# I2V model (SVD)\n",
    "I2V_MODEL_ID = \"stabilityai/stable-video-diffusion-img2vid-xt\"\n",
    "# T2V model (alternative)\n",
    "# T2V_MODEL_ID = \"damo-vilab/text-to-video-ms-1.7b\" # Example if using ModelScope\n",
    "T2V_MODEL_ID = \"cerspense/zeroscope_v2_576w\" # Another option, or use original THUDM/CogVideoX-5b if available/preferred\n",
    "\n",
    "# --- 1. INITIALIZE MODELS (Load only when needed or keep loaded if VRAM allows) ---\n",
    "def load_llm():\n",
    "    print(\"Loading LLM...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_ID)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        LLM_MODEL_ID, torch_dtype=torch.float16, device_map=\"auto\"\n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_tts():\n",
    "    print(\"Loading TTS model...\")\n",
    "    return TTS(model_name=TTS_MODEL_ID, progress_bar=True).to(DEVICE)\n",
    "\n",
    "def load_t2i_pipeline():\n",
    "    print(\"Loading T2I pipeline (SDXL)...\")\n",
    "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "        T2I_MODEL_ID, torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    "    ).to(DEVICE)\n",
    "    # Optional: Load refiner\n",
    "    # refiner = DiffusionPipeline.from_pretrained(\n",
    "    #     T2I_REFINER_ID, text_encoder_2=pipe.text_encoder_2, vae=pipe.vae,\n",
    "    #     torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\"\n",
    "    # ).to(DEVICE)\n",
    "    # return pipe, refiner\n",
    "    return pipe, None \n",
    "\n",
    "def load_i2v_pipeline(): # SVD\n",
    "    print(\"Loading I2V pipeline (SVD)...\")\n",
    "    pipe = StableVideoDiffusionPipeline.from_pretrained(\n",
    "        I2V_MODEL_ID, torch_dtype=torch.float16, variant=\"fp16\"\n",
    "    ).to(DEVICE)\n",
    "    pipe.enable_model_cpu_offload()\n",
    "    return pipe\n",
    "\n",
    "def load_t2v_pipeline():\n",
    "    print(f\"Loading T2V pipeline ({T2V_MODEL_ID})...\")\n",
    "    pipe = DiffusionPipeline.from_pretrained(\n",
    "        T2V_MODEL_ID, torch_dtype=torch.float16 \n",
    "        # variant=\"fp16\" # if applicable\n",
    "    ).to(DEVICE)\n",
    "    pipe.enable_model_cpu_offload()\n",
    "    return pipe\n",
    "\n",
    "# --- UTILITY TO CLEAR VRAM ---\n",
    "def clear_vram(*models_or_pipelines):\n",
    "    for item in models_or_pipelines:\n",
    "        if item is None:\n",
    "            continue\n",
    "        if hasattr(item, 'cpu') and callable(getattr(item, 'cpu')):\n",
    "            item.cpu()\n",
    "        elif hasattr(item, 'model') and hasattr(item.model, 'cpu') and callable(getattr(item.model, 'cpu')):\n",
    "            item.model.cpu()\n",
    "    # Aggressively clear\n",
    "    del models_or_pipelines\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    gc.collect()\n",
    "    print(\"VRAM cleared and memory collected.\")\n",
    "\n",
    "\n",
    "# --- 2. TEXT GENERATION (SCRIPTING) ---\n",
    "def generate_script_from_llm(topic: str, model, tokenizer, config: ContentConfig) -> Tuple[List[Dict[str, Any]], List[str], List[str]]:\n",
    "    print(f\"Generating script and prompts for topic: {topic}\")\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are an AI assistant creating content for a short video. \"\n",
    "                \"Your response must be in valid JSON format: \"\n",
    "                \"{\\\"narration\\\": [{\\\"scene\\\": 1, \\\"text\\\": \\\"narration_text\\\", \\\"duration_estimate\\\": float_seconds}], \"\n",
    "                \"\\\"visuals\\\": [{\\\"scene\\\": 1, \\\"prompt\\\": \\\"visual_prompt_for_scene\\\"}], \"\n",
    "                \"\\\"hashtags\\\": [\\\"tag1\\\", \\\"tag2\\\"]}\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "            Create content for a short video about \"{topic}\".\n",
    "            The video should be engaging and the total narration should be around {config.target_video_length_hint} seconds long.\n",
    "            Each narration segment should ideally be around {config.max_scene_narration_duration_hint} seconds.\n",
    "            Generate between {config.min_scenes} and {config.max_scenes} narration/visual scenes.\n",
    "            \n",
    "            Return your response in this exact JSON format:\n",
    "            {{\n",
    "                \"narration\": [\n",
    "                    {{\"scene\": 1, \"text\": \"First scene narration text.\", \"duration_estimate\": {config.max_scene_narration_duration_hint}}},\n",
    "                    {{\"scene\": 2, \"text\": \"Second scene narration text.\", \"duration_estimate\": {config.max_scene_narration_duration_hint}}}\n",
    "                ],\n",
    "                \"visuals\": [\n",
    "                    {{\"scene\": 1, \"prompt\": \"Detailed visual prompt for scene 1, focusing on key elements, style (e.g., cinematic, anime, realistic), camera angles, and mood.\"}},\n",
    "                    {{\"scene\": 2, \"prompt\": \"Detailed visual prompt for scene 2, similarly descriptive.\"}}\n",
    "                ],\n",
    "                \"hashtags\": [\"relevantTag1\", \"relevantTag2\", \"relevantTag3\"]\n",
    "            }}\n",
    "            \n",
    "            Ensure the 'duration_estimate' is a float representing seconds.\n",
    "            Each visual prompt should be detailed and suitable for image/video generation models.\n",
    "            The number of narration entries must match the number of visual entries.\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "    generation_kwargs = {\n",
    "        \"input_ids\": tokenized_chat, \"max_new_tokens\": 1536, \"do_sample\": True,\n",
    "        \"top_k\": 50, \"top_p\": 0.95, \"temperature\": 0.7, \"pad_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "\n",
    "    outputs = model.generate(**generation_kwargs)\n",
    "    decoded_output = tokenizer.decode(outputs[0][tokenized_chat.shape[-1]:], skip_special_tokens=True)\n",
    "    print(\"LLM Response:\\n\", decoded_output)\n",
    "    \n",
    "    import re\n",
    "    match = re.search(r'\\{[\\s\\S]*\\}', decoded_output) # More robust regex for finding first { to last }\n",
    "    \n",
    "    if match:\n",
    "        json_text_to_parse = match.group(0)\n",
    "        print(f\"Extracted JSON block for parsing:\\n{json_text_to_parse}\")\n",
    "    else:\n",
    "        json_text_to_parse = decoded_output # Fallback if no clear block\n",
    "        print(f\"Could not extract a clear JSON block, attempting to parse full output.\")\n",
    "\n",
    "    try:\n",
    "        response_data = json.loads(json_text_to_parse)\n",
    "        narration_data = sorted(response_data.get(\"narration\", []), key=lambda x: x[\"scene\"])\n",
    "        visuals_data = sorted(response_data.get(\"visuals\", []), key=lambda x: x[\"scene\"])\n",
    "        \n",
    "        narration_scenes = [{\"text\": s[\"text\"], \"duration_estimate\": float(s.get(\"duration_estimate\", config.max_scene_narration_duration_hint))} for s in narration_data]\n",
    "        visual_prompts = [s[\"prompt\"] for s in visuals_data]\n",
    "        hashtags = response_data.get(\"hashtags\", [])\n",
    "\n",
    "        if len(narration_scenes) != len(visual_prompts):\n",
    "            raise ValueError(\"Mismatch between number of narration and visual scenes.\")\n",
    "        if not (config.min_scenes <= len(narration_scenes) <= config.max_scenes):\n",
    "            raise ValueError(f\"Scene count {len(narration_scenes)} outside range [{config.min_scenes}, {config.max_scenes}].\")\n",
    "            \n",
    "        return narration_scenes, visual_prompts, hashtags\n",
    "    except (json.JSONDecodeError, ValueError, KeyError) as e:\n",
    "        print(f\"Error parsing LLM response: {e}. Using fallback content.\")\n",
    "        fallback_duration = config.max_scene_narration_duration_hint\n",
    "        narration_scenes = [\n",
    "            {\"text\": f\"An introduction to {topic}.\", \"duration_estimate\": fallback_duration},\n",
    "            {\"text\": \"Exploring details and implications.\", \"duration_estimate\": fallback_duration}\n",
    "        ]\n",
    "        visual_prompts = [\n",
    "            f\"Cinematic overview of {topic}, vibrant colors, high detail.\",\n",
    "            f\"Close-up abstract representation related to {topic}, thought-provoking.\"\n",
    "        ]\n",
    "        hashtags = [f\"#{topic.replace(' ', '')}\", \"#AIvideo\", \"#GeneratedContent\"]\n",
    "        return narration_scenes, visual_prompts, hashtags\n",
    "\n",
    "\n",
    "# --- 3. AUDIO GENERATION ---\n",
    "def generate_audio_for_scene(text: str, tts_model, output_dir: str, scene_idx: int, speaker_wav: Optional[str] = None) -> Tuple[str, float]:\n",
    "    print(f\"Generating audio for scene {scene_idx}: \\\"{text[:50]}...\\\"\")\n",
    "    output_path = os.path.join(output_dir, f\"scene_{scene_idx}_audio.wav\")\n",
    "    \n",
    "    if \"xtts\" in TTS_MODEL_ID.lower():\n",
    "        if speaker_wav and os.path.exists(speaker_wav):\n",
    "            tts_model.tts_to_file(text, speaker_wav=speaker_wav, language=\"en\", file_path=output_path)\n",
    "        else:\n",
    "            if speaker_wav: print(f\"Warning: Speaker WAV {speaker_wav} not found for XTTS. Using default voice.\")\n",
    "            else: print(\"Warning: XTTS model selected but no speaker_wav provided. Using default voice.\")\n",
    "            tts_model.tts_to_file(text, language=\"en\", file_path=output_path) # Default voice\n",
    "    else:\n",
    "        tts_model.tts_to_file(text, file_path=output_path)\n",
    "    \n",
    "    print(f\"Audio for scene {scene_idx} saved to {output_path}\")\n",
    "    \n",
    "    # Get actual audio duration\n",
    "    try:\n",
    "        with AudioFileClip(output_path) as audio_clip:\n",
    "            duration = audio_clip.duration\n",
    "        # A small buffer, Coqui TTS sometimes clips the very end.\n",
    "        # This is optional, can be removed if not an issue.\n",
    "        duration += 0.1 \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting duration for {output_path}: {e}. Assuming 0 duration.\")\n",
    "        duration = 0.0\n",
    "        # Fallback: if file is corrupt or empty, it might cause issues later.\n",
    "        # Consider creating a tiny valid silent wav if duration is 0 to prevent crashes.\n",
    "        if not os.path.exists(output_path) or os.path.getsize(output_path) == 0:\n",
    "             # Create a minimal valid WAV file if generation failed catastrophically\n",
    "            from scipy.io import wavfile\n",
    "            samplerate = 22050 # A common samplerate\n",
    "            silence_data = np.zeros(int(0.1 * samplerate), dtype=np.int16) # 0.1s silence\n",
    "            wavfile.write(output_path, samplerate, silence_data)\n",
    "            print(f\"Wrote fallback silent audio to {output_path}\")\n",
    "            duration = 0.1\n",
    "\n",
    "\n",
    "    print(f\"Actual audio duration for scene {scene_idx}: {duration:.2f}s\")\n",
    "    return output_path, duration\n",
    "\n",
    "\n",
    "# --- 4. VISUAL GENERATION (PER CHUNK) ---\n",
    "def generate_image_then_video_chunk(\n",
    "    image_prompt: str, i2v_pipe, t2i_pipe, refiner_pipe,\n",
    "    scene_idx: int, chunk_idx: int, target_chunk_duration: float, config: ContentConfig\n",
    ") -> str:\n",
    "    # Generate Image (Keyframe)\n",
    "    print(f\"Scene {scene_idx}, Chunk {chunk_idx}: Generating keyframe image for: \\\"{image_prompt[:50]}...\\\"\")\n",
    "    keyframe_image_path = os.path.join(config.output_dir, f\"scene_{scene_idx}_chunk_{chunk_idx}_keyframe.png\")\n",
    "\n",
    "    # Calculate base dimensions (half of target resolution)\n",
    "    base_width = config.target_resolution[0] // 2\n",
    "    base_height = config.target_resolution[1] // 2\n",
    "\n",
    "    # Ensure dimensions are divisible by 8 for SDXL\n",
    "    # We can round down to the nearest multiple of 8\n",
    "    sdxl_width = (base_width // 8) * 8\n",
    "    sdxl_height = (base_height // 8) * 8\n",
    "\n",
    "    # Make sure dimensions are not too small (e.g., SDXL might have min requirements like 256 or 512)\n",
    "    # This is a sensible default, adjust if your model needs larger minimums\n",
    "    sdxl_width = max(sdxl_width, 512 if sdxl_width > sdxl_height else 256) # Example min logic\n",
    "    sdxl_height = max(sdxl_height, 512 if sdxl_height > sdxl_width else 256) # Example min logic\n",
    "    # Ensure they are still multiples of 8 after max()\n",
    "    sdxl_width = (sdxl_width // 8) * 8\n",
    "    sdxl_height = (sdxl_height // 8) * 8\n",
    "\n",
    "\n",
    "    print(f\"Scene {scene_idx}, Chunk {chunk_idx}: SDXL target image size: {sdxl_width}x{sdxl_height}\")\n",
    "\n",
    "    image = t2i_pipe(\n",
    "        prompt=image_prompt, num_inference_steps=30, guidance_scale=7.5,\n",
    "        width=sdxl_width, height=sdxl_height\n",
    "    ).images[0]\n",
    "\n",
    "    # if refiner_pipe: image = refiner_pipe(prompt=image_prompt, image=image[None, :]).images[0]\n",
    "    image.save(keyframe_image_path)\n",
    "    print(f\"Keyframe image for scene {scene_idx}, chunk {chunk_idx} saved to {keyframe_image_path}\")\n",
    "\n",
    "    # Prepare image for SVD (load and resize if needed)\n",
    "    loaded_image = load_image(keyframe_image_path)\n",
    "    # SVD has its own preferred resolutions, often 1024x576 or 576x1024.\n",
    "    # The image generated by SDXL will be loaded and SVD pipeline will handle resizing internally\n",
    "    # if needed, or you can explicitly resize it here if you know SVD's exact optimal input.\n",
    "    # For SVD-XT, common input sizes are around 1024x576 or 576x1024.\n",
    "    # If your sdxl_width/height are far from this, SVD might resize, potentially affecting quality.\n",
    "    # Example explicit resize for SVD if needed:\n",
    "    # if sdxl_width > sdxl_height: # Landscape-ish\n",
    "    #     svd_input_image = loaded_image.resized((1024, 576)) # Example\n",
    "    # else: # Portrait-ish\n",
    "    #     svd_input_image = loaded_image.resized((576, 1024)) # Example\n",
    "    # But usually, just passing `loaded_image` is fine.\n",
    "    svd_input_image = loaded_image\n",
    "\n",
    "\n",
    "    num_frames = max(8, int(target_chunk_duration * config.fps))\n",
    "    num_frames = min(num_frames, 25) # SVD-XT typically max 25 frames\n",
    "\n",
    "    print(f\"Scene {scene_idx}, Chunk {chunk_idx}: Generating video from image ({num_frames} frames) using SVD...\")\n",
    "    video_frames = i2v_pipe(\n",
    "        svd_input_image, decode_chunk_size=4, num_frames=num_frames, motion_bucket_id=127,\n",
    "        fps=config.fps, noise_aug_strength=0.02\n",
    "    ).frames[0]\n",
    "\n",
    "    video_chunk_path = os.path.join(config.output_dir, f\"scene_{scene_idx}_chunk_{chunk_idx}_svd.mp4\")\n",
    "    export_to_video(video_frames, video_chunk_path, fps=config.fps)\n",
    "    print(f\"SVD video chunk for scene {scene_idx}, chunk {chunk_idx} saved to {video_chunk_path}\")\n",
    "    return video_chunk_path\n",
    "\n",
    "def generate_direct_video_chunk(\n",
    "    video_prompt: str, t2v_pipe, \n",
    "    scene_idx: int, chunk_idx: int, target_chunk_duration: float, config: ContentConfig\n",
    ") -> str:\n",
    "    print(f\"Scene {scene_idx}, Chunk {chunk_idx}: Generating direct video for: \\\"{video_prompt[:50]}...\\\"\")\n",
    "    \n",
    "    num_frames = max(8, int(target_chunk_duration * config.fps)) # Min frames\n",
    "    # Some T2V models also have max frame limits or work best in certain ranges\n",
    "    # num_frames = min(num_frames, 60) # Example: if model supports up to 60 frames\n",
    "\n",
    "    # ModelScope/Zeroscope might need width/height parameters\n",
    "    video_frames = t2v_pipe(\n",
    "        prompt=video_prompt, \n",
    "        num_inference_steps=25, \n",
    "        num_frames=num_frames,\n",
    "        height=config.target_resolution[1] // 2, # Example: common T2V models often prefer smaller resolutions\n",
    "        width=config.target_resolution[0] // 2\n",
    "    ).frames # For many diffusers T2V, .frames is the list of PILs\n",
    "\n",
    "    video_chunk_path = os.path.join(config.output_dir, f\"scene_{scene_idx}_chunk_{chunk_idx}_t2v.mp4\")\n",
    "    export_to_video(video_frames, video_chunk_path, fps=config.fps)\n",
    "    print(f\"T2V video chunk for scene {scene_idx}, chunk {chunk_idx} saved to {video_chunk_path}\")\n",
    "    return video_chunk_path\n",
    "\n",
    "\n",
    "# --- 5. VIDEO ASSEMBLY ---\n",
    "def assemble_scene_video_from_sub_clips(\n",
    "    sub_clip_paths: List[str], \n",
    "    target_total_duration: float, \n",
    "    config: ContentConfig, \n",
    "    scene_idx: int\n",
    ") -> str:\n",
    "    if not sub_clip_paths:\n",
    "        print(f\"Warning: No sub-clips provided for scene {scene_idx}. Cannot assemble scene video.\")\n",
    "        # Create a short black placeholder?\n",
    "        placeholder_path = os.path.join(config.output_dir, f\"scene_{scene_idx}_placeholder.mp4\")\n",
    "        # Simple way to make a black clip with moviepy if needed, but for now, just return empty string or raise error.\n",
    "        # For now, let's assume this case is handled upstream or we expect valid paths.\n",
    "        return \"\" \n",
    "\n",
    "    print(f\"Assembling video for scene {scene_idx} from {len(sub_clip_paths)} sub-clips to match duration {target_total_duration:.2f}s.\")\n",
    "    \n",
    "    clips_to_close = []\n",
    "    video_sub_clips_mvp = []\n",
    "    for path in sub_clip_paths:\n",
    "        clip = VideoFileClip(path)\n",
    "        video_sub_clips_mvp.append(clip)\n",
    "        clips_to_close.append(clip)\n",
    "\n",
    "    # Concatenate raw sub-clips first\n",
    "    concatenated_raw_video = concatenate_videoclips(video_sub_clips_mvp, method=\"compose\")\n",
    "    clips_to_close.append(concatenated_raw_video)\n",
    "    \n",
    "    # Adjust final concatenated clip to precisely match target_total_duration\n",
    "    current_duration = concatenated_raw_video.duration\n",
    "    if abs(current_duration - target_total_duration) < 0.05 : # If very close, accept it\n",
    "         final_scene_video_timed = concatenated_raw_video \n",
    "    elif current_duration > target_total_duration:\n",
    "        final_scene_video_timed = concatenated_raw_video.subclipped(0, target_total_duration)\n",
    "    else: # current_duration < target_total_duration - loop the whole concatenated clip\n",
    "        num_loops = math.ceil(target_total_duration / current_duration)\n",
    "        looped_clips = [concatenated_raw_video] * num_loops\n",
    "        temp_looped_video = concatenate_videoclips(looped_clips, method=\"compose\")\n",
    "        clips_to_close.append(temp_looped_video) # Add to close list\n",
    "        final_scene_video_timed = temp_looped_video.subclipped(0, target_total_duration)\n",
    "\n",
    "    # Add the final timed clip to close list if it's a new object (subclip creates new)\n",
    "    if final_scene_video_timed is not concatenated_raw_video and final_scene_video_timed not in clips_to_close:\n",
    "        clips_to_close.append(final_scene_video_timed)\n",
    "\n",
    "    final_scene_video_path = os.path.join(config.output_dir, f\"scene_{scene_idx}_assembled_video.mp4\")\n",
    "    try:\n",
    "        final_scene_video_timed.write_videofile(\n",
    "            final_scene_video_path, \n",
    "            fps=config.fps, \n",
    "            codec=\"libx264\", \n",
    "            audio=False, # Audio will be added in the final assembly step\n",
    "            threads=4, preset=\"medium\", logger=None # Quieter logs for sub-assemblies\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing assembled scene video for scene {scene_idx}: {e}\")\n",
    "        # Fallback or error handling\n",
    "        final_scene_video_path = \"\" # Indicate failure\n",
    "    finally:\n",
    "        for clip_obj in clips_to_close:\n",
    "            if hasattr(clip_obj, 'close') and callable(getattr(clip_obj, 'close')):\n",
    "                clip_obj.close()\n",
    "    \n",
    "    print(f\"Assembled video for scene {scene_idx} saved to {final_scene_video_path} with duration {final_scene_video_timed.duration:.2f}s.\")\n",
    "    return final_scene_video_path\n",
    "\n",
    "\n",
    "def assemble_final_reel(\n",
    "    processed_scene_assets: List[Tuple[str, str, Dict[str, Any]]],\n",
    "    config: ContentConfig,\n",
    "    output_filename: str = \"final_reel.mp4\"\n",
    ") -> Optional[str]:\n",
    "    print(\"Assembling final reel...\")\n",
    "    if not processed_scene_assets:\n",
    "        print(\"No processed scene assets to assemble. Final video cannot be created.\")\n",
    "        return None\n",
    "\n",
    "    final_scene_video_clips = [] # Renamed from final_scene_clips_for_reel for clarity\n",
    "    \n",
    "    # This list will store all clips that are loaded or created\n",
    "    # and should be closed in the finally block.\n",
    "    all_clips_to_close = []\n",
    "\n",
    "    font_path_for_textclip = \"/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf\"\n",
    "    if not os.path.exists(font_path_for_textclip):\n",
    "        print(f\"Warning: Font file not found at {font_path_for_textclip}. TextClip will use a default font.\")\n",
    "        font_path_for_textclip = \"Liberation-Sans-Bold\" # Or \"Arial\" or None\n",
    "\n",
    "    for i, (scene_video_path, scene_audio_path, narration_info) in enumerate(processed_scene_assets):\n",
    "        video_clip_for_scene = None\n",
    "        audio_clip_for_scene = None\n",
    "        text_clip_for_scene = None\n",
    "        background_clip_for_scene = None\n",
    "        \n",
    "        try:\n",
    "            narration_text = narration_info[\"text\"]\n",
    "            actual_audio_duration = narration_info[\"duration\"] # This is the target duration for this scene\n",
    "\n",
    "            if not (scene_video_path and os.path.exists(scene_video_path) and \\\n",
    "                    scene_audio_path and os.path.exists(scene_audio_path)):\n",
    "                print(f\"Skipping scene {i} due to missing media files.\")\n",
    "                continue\n",
    "\n",
    "            # Load video and audio clips\n",
    "            video_clip_for_scene = VideoFileClip(scene_video_path)\n",
    "            audio_clip_for_scene = AudioFileClip(scene_audio_path)\n",
    "            all_clips_to_close.extend([video_clip_for_scene, audio_clip_for_scene])\n",
    "\n",
    "            video_duration = video_clip_for_scene.duration\n",
    "            \n",
    "            # --- Video Duration Matching (using subclipped and concatenate_videoclips for loop) ---\n",
    "            # First, resize and crop to final shape before timing adjustments IF POSSIBLE,\n",
    "            # or do timing first. Let's stick to your old code's order:\n",
    "            # Resize, Crop, Position, THEN Time, then Audio.\n",
    "\n",
    "            # 1. Resize video to target height\n",
    "            temp_video_clip = video_clip_for_scene.resized(height=config.target_resolution[1])\n",
    "\n",
    "            # 2. Crop if wider than target width, or pad if narrower\n",
    "            if temp_video_clip.w > config.target_resolution[0]:\n",
    "                # Using .cropped() as per your working old code\n",
    "                temp_video_clip = temp_video_clip.cropped(x_center=temp_video_clip.w / 2,\n",
    "                                                          width=config.target_resolution[0])\n",
    "            elif temp_video_clip.w < config.target_resolution[0]:\n",
    "                # Pad with a background\n",
    "                background_clip_for_scene = ColorClip(size=config.target_resolution,\n",
    "                                           color=(0,0,0), # Black background\n",
    "                                           ismask=False, duration=actual_audio_duration) # Duration for background\n",
    "                all_clips_to_close.append(background_clip_for_scene)\n",
    "                # Composite video onto background\n",
    "                temp_video_clip = CompositeVideoClip([background_clip_for_scene, temp_video_clip.with_position('center')],\n",
    "                                                     size=config.target_resolution)\n",
    "            \n",
    "            # 3. Position video in center (if not already handled by padding composite)\n",
    "            # The .with_position('center') might have been applied already if padded.\n",
    "            # If not padded, apply it now.\n",
    "            if not (video_clip_for_scene.w < config.target_resolution[0] and temp_video_clip.w == config.target_resolution[0]):\n",
    "                 temp_video_clip = temp_video_clip.with_position('center')\n",
    "\n",
    "            # 4. Handle duration mismatches for the video\n",
    "            if video_duration > actual_audio_duration: # If original video was longer\n",
    "                video_clip_timed = temp_video_clip.subclipped(0, actual_audio_duration)\n",
    "            elif video_duration < actual_audio_duration: # If original video was shorter, loop it\n",
    "                # Note: we loop the `temp_video_clip` which is already resized/cropped/positioned\n",
    "                num_loops = math.ceil(actual_audio_duration / video_duration) # Loop based on original duration\n",
    "                if num_loops == 0 : num_loops = 1 # Ensure at least one instance\n",
    "                # Create a list of the clip to be looped\n",
    "                looped_video_parts = [temp_video_clip] * num_loops\n",
    "                video_clip_concatenated_for_loop = concatenate_videoclips(looped_video_parts)\n",
    "                all_clips_to_close.append(video_clip_concatenated_for_loop) # This new clip needs closing\n",
    "                video_clip_timed = video_clip_concatenated_for_loop.subclipped(0, actual_audio_duration)\n",
    "            else: # Durations match closely enough\n",
    "                video_clip_timed = temp_video_clip # temp_video_clip is already at its full duration here\n",
    "\n",
    "            # Audio duration handling is simpler because the video is now timed to actual_audio_duration\n",
    "            # We just need to ensure the audio_clip_for_scene is also exactly that duration.\n",
    "            # Your original code did padding/trimming for audio separately.\n",
    "            # Given video_clip_timed is now exactly actual_audio_duration,\n",
    "            # audio_clip_for_scene should ideally match.\n",
    "            # If audio_clip_for_scene.duration IS actual_audio_duration (from TTS generation), then no change.\n",
    "            # If TTS gave slightly different, we should use the *actual_audio_duration* from TTS.\n",
    "            # Let's assume audio_clip_for_scene.duration is the one we trust (actual_audio_duration)\n",
    "            \n",
    "            # No, your logic was: video is timed to narration[\"duration\"] which IS actual_audio_duration.\n",
    "            # Audio is also timed to actual_audio_duration.\n",
    "            # My generate_audio_for_scene returns actual_audio_duration which includes a 0.1s buffer.\n",
    "            # So, the audio_clip_for_scene should be trimmed if it's slightly longer than its intended text part.\n",
    "            # This structure is a bit confusing. Let's assume `actual_audio_duration` is the *final desired duration for this scene*.\n",
    "            \n",
    "            final_audio_for_scene = audio_clip_for_scene # Start with the loaded audio\n",
    "            if final_audio_for_scene.duration > actual_audio_duration:\n",
    "                final_audio_for_scene = final_audio_for_scene.subclipped(0, actual_audio_duration)\n",
    "            elif final_audio_for_scene.duration < actual_audio_duration:\n",
    "                silence_needed = actual_audio_duration - final_audio_for_scene.duration\n",
    "                if silence_needed > 0.01: # Only add if significant\n",
    "                    silence_clip = AudioClip(frame_function=lambda t: 0, duration=silence_needed)\n",
    "                    all_clips_to_close.append(silence_clip)\n",
    "                    final_audio_for_scene = concatenate_audioclips([final_audio_for_scene, silence_clip])\n",
    "\n",
    "\n",
    "            # 5. Combine video and audio\n",
    "            video_clip_with_audio = video_clip_timed.with_audio(final_audio_for_scene)\n",
    "\n",
    "            # 6. Add text caption\n",
    "            # Add text caption\n",
    "            text_clip_for_scene = TextClip(\n",
    "                font_path_for_textclip,\n",
    "                text=narration_text,\n",
    "                font_size=60,\n",
    "                color='white',\n",
    "                stroke_color='black',\n",
    "                stroke_width=2,\n",
    "                method='caption',\n",
    "                size=(int(config.target_resolution[0]*0.8), None)\n",
    "            )\n",
    "            source_clips_to_close.append(txt_clip_temp)\n",
    "            all_clips_to_close.append(text_clip_for_scene)\n",
    "\n",
    "            # Use with_duration as per your old code for TextClip\n",
    "            text_clip_final = text_clip_for_scene.with_position(('center', 0.8), relative=True).with_duration(actual_audio_duration)\n",
    "\n",
    "            # 7. Combine video and text into final scene composite\n",
    "            scene_composite = CompositeVideoClip(\n",
    "                [video_clip_with_audio, text_clip_final],\n",
    "                size=config.target_resolution # Ensure composite is target size\n",
    "            )\n",
    "            final_scene_video_clips.append(scene_composite)\n",
    "\n",
    "        except Exception as e_scene:\n",
    "            print(f\"Error processing scene {i}: {e_scene}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            # Any clips opened in this iteration (video_clip_for_scene, etc.) are already in all_clips_to_close\n",
    "            continue\n",
    "\n",
    "    if not final_scene_video_clips:\n",
    "        print(\"No scenes were successfully composed.\")\n",
    "        # Close any clips that might have been opened\n",
    "        for clip_obj in all_clips_to_close:\n",
    "            if hasattr(clip_obj, 'close') and callable(getattr(clip_obj, 'close')):\n",
    "                try: clip_obj.close()\n",
    "                except: pass # Ignore errors during cleanup after failure\n",
    "        return None\n",
    "\n",
    "    final_video_output_clip = None\n",
    "    final_video_path = os.path.join(config.output_dir, output_filename)\n",
    "    try:\n",
    "        final_video_output_clip = concatenate_videoclips(final_scene_video_clips, method=\"compose\")\n",
    "        all_clips_to_close.append(final_video_output_clip) # Add final concatenated clip for closing\n",
    "\n",
    "        final_video_output_clip.write_videofile(\n",
    "            final_video_path,\n",
    "            fps=config.fps,\n",
    "            codec=\"libx264\",\n",
    "            audio_codec=\"aac\",\n",
    "            threads=4,\n",
    "            preset=\"medium\", # \"ultrafast\" for speed, \"medium\" for balance\n",
    "            logger='bar'\n",
    "        )\n",
    "    except Exception as e_write:\n",
    "        print(f\"Error during final video writing: {e_write}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        final_video_path = None # Indicate failure\n",
    "    finally:\n",
    "        # Close all clips.\n",
    "        # `final_scene_video_clips` contains CompositeVideoClips that are sources for `final_video_output_clip`.\n",
    "        # Closing `final_video_output_clip` should ideally handle its sources if method='compose'.\n",
    "        # `all_clips_to_close` contains initial VideoFileClips, AudioFileClips, created ColorClips, TextClips,\n",
    "        # and potentially intermediate concatenated clips.\n",
    "        \n",
    "        # Make a set of unique clip objects to close to avoid issues with multiple references\n",
    "        # to the same underlying resources.\n",
    "        clips_to_actually_close = {id(c): c for c in all_clips_to_close if c}.values()\n",
    "        \n",
    "        for clip_obj in clips_to_actually_close:\n",
    "            if hasattr(clip_obj, 'close') and callable(getattr(clip_obj, 'close')):\n",
    "                try:\n",
    "                    clip_obj.close()\n",
    "                except Exception as e_close:\n",
    "                    # print(f\"Error closing a clip {type(clip_obj)}: {e_close}\") # Can be noisy\n",
    "                    pass\n",
    "        \n",
    "        # Also ensure the list of scene composites themselves are closed, as they are also clips\n",
    "        for scene_comp in final_scene_video_clips:\n",
    "            if hasattr(scene_comp, 'close') and callable(getattr(scene_comp, 'close')):\n",
    "                try: scene_comp.close()\n",
    "                except: pass\n",
    "\n",
    "\n",
    "    if final_video_path:\n",
    "        print(f\"Final reel saved to {final_video_path}\")\n",
    "    return final_video_path\n",
    "\n",
    "# --- MAIN WORKFLOW ---\n",
    "def main_automation_flow(topic: str, config: ContentConfig, speaker_reference_audio: Optional[str] = None):\n",
    "    final_video_path = None\n",
    "    # Initialize model placeholders\n",
    "    llm_model, llm_tokenizer = None, None\n",
    "    tts_model = None\n",
    "    t2i_pipe, refiner = None, None\n",
    "    i2v_pipe = None\n",
    "    t2v_pipe = None\n",
    "\n",
    "    try:\n",
    "        # 1. LLM for Script and Prompts\n",
    "        llm_model, llm_tokenizer = load_llm()\n",
    "        script_narration_parts, script_visual_prompts, hashtags = generate_script_from_llm(\n",
    "            topic, llm_model, llm_tokenizer, config\n",
    "        )\n",
    "        clear_vram(llm_model, llm_tokenizer) # llm_tokenizer is not a model, clear_vram handles None\n",
    "        llm_model, llm_tokenizer = None, None # Mark as cleared\n",
    "\n",
    "        # Prepare for collecting assets for each scene\n",
    "        processed_scene_assets = [] # List of (final_scene_video_path, scene_audio_path, narration_info_dict)\n",
    "\n",
    "        # 2. TTS for Narration (Load once, use for all scenes)\n",
    "        tts_model = load_tts()\n",
    "        \n",
    "        # Load video generation models (outside scene loop to avoid repeated loading/unloading if possible)\n",
    "        # This depends on VRAM. If VRAM is very limited, loading per scene might be necessary.\n",
    "        # For now, load once. clear_vram will handle them after the loop.\n",
    "        if config.use_svd_flow:\n",
    "            t2i_pipe, refiner = load_t2i_pipeline()\n",
    "            i2v_pipe = load_i2v_pipeline()\n",
    "        else:\n",
    "            t2v_pipe = load_t2v_pipeline()\n",
    "\n",
    "        # --- Loop through each scripted scene ---\n",
    "        for i, (narration_info, visual_prompt_for_scene) in enumerate(zip(script_narration_parts, script_visual_prompts)):\n",
    "            scene_start_time = time.time()\n",
    "            print(f\"\\n--- Processing Scene {i+1}/{len(script_narration_parts)} ---\")\n",
    "            narration_text = narration_info[\"text\"]\n",
    "            \n",
    "            # 2.a. Generate Audio for this narration part & Get Actual Duration\n",
    "            scene_audio_path, actual_audio_duration = generate_audio_for_scene(\n",
    "                narration_text, tts_model, config.output_dir, i,\n",
    "                speaker_wav=speaker_reference_audio\n",
    "            )\n",
    "\n",
    "            if actual_audio_duration <= 0.1: # Threshold for too short/failed audio\n",
    "                print(f\"Scene {i} has negligible audio duration ({actual_audio_duration:.2f}s). Skipping video generation for this scene.\")\n",
    "                continue\n",
    "\n",
    "            # 2.b. Determine number of video chunks needed for this audio duration\n",
    "            num_video_chunks = math.ceil(actual_audio_duration / config.model_max_video_chunk_duration)\n",
    "            if num_video_chunks == 0: num_video_chunks = 1 # Ensure at least one chunk if audio exists\n",
    "            \n",
    "            print(f\"Scene {i}: Audio duration {actual_audio_duration:.2f}s. Needs {num_video_chunks} video chunk(s).\")\n",
    "\n",
    "            video_sub_clip_paths_for_scene = []\n",
    "            current_scene_audio_covered_duration = 0.0\n",
    "\n",
    "            for chunk_idx in range(num_video_chunks):\n",
    "                print(f\"  Generating video chunk {chunk_idx+1}/{num_video_chunks} for scene {i}...\")\n",
    "                \n",
    "                # Determine duration for this specific video chunk\n",
    "                if chunk_idx < num_video_chunks - 1:\n",
    "                    current_chunk_target_duration = config.model_max_video_chunk_duration\n",
    "                else: # Last chunk takes the remainder\n",
    "                    current_chunk_target_duration = actual_audio_duration - current_scene_audio_covered_duration\n",
    "                \n",
    "                current_chunk_target_duration = max(0.5, current_chunk_target_duration) # Ensure a minimum practical duration for video models\n",
    "\n",
    "                sub_clip_visual_prompt = visual_prompt_for_scene # Use main prompt, or could vary it: f\"{visual_prompt_for_scene} (focus on part {chunk_idx+1})\"\n",
    "                \n",
    "                sub_clip_path = None\n",
    "                if config.use_svd_flow:\n",
    "                    sub_clip_path = generate_image_then_video_chunk(\n",
    "                        sub_clip_visual_prompt, i2v_pipe, t2i_pipe, refiner,\n",
    "                        i, chunk_idx, current_chunk_target_duration, config\n",
    "                    )\n",
    "                else:\n",
    "                    sub_clip_path = generate_direct_video_chunk(\n",
    "                        sub_clip_visual_prompt, t2v_pipe,\n",
    "                        i, chunk_idx, current_chunk_target_duration, config\n",
    "                    )\n",
    "                \n",
    "                if sub_clip_path and os.path.exists(sub_clip_path):\n",
    "                    video_sub_clip_paths_for_scene.append(sub_clip_path)\n",
    "                    # Get actual duration of generated sub_clip_path to update current_scene_audio_covered_duration more accurately\n",
    "                    # This is important if video model doesn't strictly adhere to target_chunk_duration\n",
    "                    with VideoFileClip(sub_clip_path) as temp_vfc:\n",
    "                        generated_chunk_duration = temp_vfc.duration\n",
    "                    current_scene_audio_covered_duration += generated_chunk_duration\n",
    "                else:\n",
    "                    print(f\"    Failed to generate video chunk {chunk_idx} for scene {i}.\")\n",
    "            \n",
    "            # 2.c. Assemble sub-clips for the current scene's video\n",
    "            if video_sub_clip_paths_for_scene:\n",
    "                final_video_for_scene_path = assemble_scene_video_from_sub_clips(\n",
    "                    video_sub_clip_paths_for_scene, actual_audio_duration, config, i\n",
    "                )\n",
    "                if final_video_for_scene_path:\n",
    "                    # Store asset paths for final assembly\n",
    "                    narration_data_for_assembly = {'text': narration_text, 'duration': actual_audio_duration}\n",
    "                    processed_scene_assets.append((final_video_for_scene_path, scene_audio_path, narration_data_for_assembly))\n",
    "                else:\n",
    "                    print(f\"Failed to assemble video for scene {i} from its sub-clips.\")\n",
    "            else:\n",
    "                print(f\"No video sub-clips generated for scene {i}. Skipping assembly for this scene.\")\n",
    "            \n",
    "            print(f\"--- Scene {i+1} processing took {time.time() - scene_start_time:.2f}s ---\")\n",
    "            # Optional: clear some VRAM if looping per scene for video models\n",
    "            # gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "        # Clear TTS and Video models from VRAM after all scenes are processed\n",
    "        clear_vram(tts_model)\n",
    "        tts_model = None\n",
    "        if config.use_svd_flow:\n",
    "            clear_vram(t2i_pipe, refiner, i2v_pipe)\n",
    "            t2i_pipe, refiner, i2v_pipe = None, None, None\n",
    "        else:\n",
    "            clear_vram(t2v_pipe)\n",
    "            t2v_pipe = None\n",
    "\n",
    "        # 3. Assemble Final Video from all processed scenes\n",
    "        if processed_scene_assets:\n",
    "            final_video_path = assemble_final_reel(\n",
    "                processed_scene_assets, config, output_filename=f\"{topic.replace(' ','_')}_final_reel.mp4\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"No scenes were successfully processed. Cannot create final video.\")\n",
    "\n",
    "        # 4. Output final info\n",
    "        if final_video_path:\n",
    "            print(\"\\n--- AUTOMATION COMPLETE ---\")\n",
    "            print(f\"Final Video: {final_video_path}\")\n",
    "            # Construct full narration text for caption\n",
    "            full_narration_text = \" \".join([asset[2][\"text\"] for asset in processed_scene_assets])\n",
    "            print(f\"Suggested Instagram Caption Text:\\n{full_narration_text}\")\n",
    "            print(f\"Suggested Hashtags: {', '.join(hashtags)}\")\n",
    "        else:\n",
    "            print(\"\\n--- AUTOMATION FAILED OR COMPLETED WITH NO OUTPUT ---\")\n",
    "            print(\"Check logs for errors or empty scene processing.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unhandled error occurred in main_automation_flow: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        print(\"Final cleanup: Ensuring all models are cleared from VRAM...\")\n",
    "        models_to_clear = [m for m in [llm_model, llm_tokenizer, tts_model, t2i_pipe, refiner, i2v_pipe, t2v_pipe] if m is not None]\n",
    "        if models_to_clear:\n",
    "            clear_vram(*models_to_clear)\n",
    "        print(\"Cleanup finished.\")\n",
    "    \n",
    "    return final_video_path\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- IMPORTANT SETUP ---\n",
    "    # (Ensure dependencies, models, CUDA, speaker WAV, and fonts are set up as per original comments)\n",
    "    \n",
    "    # Example configuration\n",
    "    content_creation_config = ContentConfig(\n",
    "        target_video_length_hint=5,          # Aim for ~20s total narration (LLM guidance)\n",
    "        model_max_video_chunk_duration=3.0,     # Video models generate chunks up to 3s\n",
    "        max_scene_narration_duration_hint=6.0,  # LLM hint: each narration part ~6s\n",
    "        min_scenes=1,\n",
    "        max_scenes=2,\n",
    "        use_svd_flow=True,                      # SDXL -> SVD\n",
    "        fps=10,                                  # Higher FPS for smoother SVD\n",
    "        output_dir=\"my_video_project\"\n",
    "    )\n",
    "\n",
    "    # For XTTSv2 voice cloning (replace with your WAV file path)\n",
    "    # Ensure it's a clean, 16-bit PCM WAV, preferably >10s.\n",
    "    speaker_audio_sample = \"record_out.wav\" # Path to your speaker reference WAV\n",
    "    if not os.path.exists(speaker_audio_sample):\n",
    "        print(f\"Warning: Speaker reference audio '{speaker_audio_sample}' not found. XTTS will use a default voice.\")\n",
    "        speaker_audio_sample = None\n",
    "\n",
    "\n",
    "    # reel_topic = \"The future of renewable energy sources\"\n",
    "    reel_topic = \"A serene moment in a Japanese Zen garden\"\n",
    "    # reel_topic = \"An 80 years old person guiding young generation.\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    generated_video = main_automation_flow(reel_topic, content_creation_config, speaker_reference_audio=speaker_audio_sample)\n",
    "    end_time = time.time()\n",
    "\n",
    "    if generated_video:\n",
    "        print(f\"Successfully generated video: {generated_video}\")\n",
    "    else:\n",
    "        print(\"Video generation failed or produced no output.\")\n",
    "    print(f\"Total execution time: {(end_time - start_time) / 60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72424212-8005-4356-b50e-3ec308c9f1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
