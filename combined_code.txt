==== app.py ====
# In app.py
import streamlit as st
import os
import time
import torch

from project_manager import ProjectManager
from ui_components import render_asset_panel, render_inspector_panel, render_timeline, render_viewer, render_project_selection_page
from system import initialize_system, go_to_step, select_item
from ui_task_executor import UITaskExecutor

st.set_page_config(page_title="Modular AI Video Studio", page_icon="üé¨", layout="wide")
torch.classes.__path__ = []

def init_session_state():
    """Initialize session state variables."""
    defaults = {
        'current_project_path': None,
        'current_step': 'project_selection',
        'selected_item_uuid': None,
        'selected_item_type': None,
    }
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value
    initialize_system()

init_session_state()

def load_project(project_name: str):
    """Sets the current project path and navigates to the editor."""
    st.session_state.current_project_path = os.path.join("modular_reels_output", project_name)
    st.session_state.current_step = 'editor'
    st.session_state.selected_item_uuid = None # Reset selection on load
    st.rerun()

def create_new_project(title: str, video_format: str):
    """Creates a new project, sets its path, and navigates to the editor."""
    name = "".join(c for c in title.lower() if c.isalnum() or c in " ").replace(" ", "_")[:50]
    output_dir = f"modular_reels_output/{name}_{int(time.time())}"
    pm = ProjectManager(output_dir)
    pm.initialize_project(title, video_format)
    st.session_state.current_project_path = output_dir
    st.session_state.current_step = 'editor'
    st.session_state.selected_item_uuid = None # Reset selection on create
    st.rerun()

def render_editor():
    """Renders the main video editor interface."""
    if not st.session_state.current_project_path:
        st.warning("No project path set. Returning to project selection.")
        go_to_step('project_selection')
        return

    pm = ProjectManager(st.session_state.current_project_path)
    if not pm.load_project():
        st.error(f"Failed to load project from: {st.session_state.current_project_path}")
        go_to_step('project_selection')
        return
        
    ui_executor = UITaskExecutor(pm)
    
    # --- START: DEFINITIVE FIX ---
    # If nothing is selected, default to selecting the project itself.
    # We no longer pass the 'rerun' argument.
    if not st.session_state.get('selected_item_uuid'):
        select_item('project', pm.state.uuid)
    # --- END: DEFINITIVE FIX ---

    if st.button(f"üé¨ Studio: {pm.state.title}", use_container_width=True):
        select_item('project', pm.state.uuid)
        st.rerun() # Explicitly rerun on this click
    
    toolbar_cols = st.columns([2, 2, 5])
    with toolbar_cols[0]:
        if st.button("‚¨ÖÔ∏è Back to Projects"):
            st.session_state.current_project_path = None
            st.session_state.selected_item_uuid = None
            st.session_state.selected_item_type = None
            go_to_step('project_selection')
    with toolbar_cols[1]:
        if st.button("üèÜ Assemble Final Video", type="primary", use_container_width=True):
            ui_executor.assemble_final_video()

    st.divider()
    left_col, center_col, right_col = st.columns([1, 2, 1.5])
    with left_col:
        render_asset_panel(pm, ui_executor)
    with center_col:
        render_viewer(pm)
        st.divider()
        render_timeline(pm, ui_executor)
    with right_col:
        st.subheader("‚öôÔ∏è Inspector")
        with st.container(border=True, height=800):
            render_inspector_panel(pm, ui_executor)

if st.session_state.current_step == 'project_selection':
    render_project_selection_page(create_new_project, load_project)
elif st.session_state.current_step == 'editor':
    render_editor()
else:
    go_to_step('project_selection')



==== base_modules.py ====
# In base_modules.py
from abc import ABC, abstractmethod
from typing import List, Tuple, Dict, Any, Optional, Union, Literal
from pydantic import BaseModel, Field

class ModuleCapabilities(BaseModel):
    title: str = Field(description="Title to show in dropdowns")
    vram_gb_min: float = Field(default=4.0)
    ram_gb_min: float = Field(default=8.0)
    supported_formats: List[Literal["Portrait", "Landscape"]] = Field(default=["Portrait", "Landscape"])
    supports_ip_adapter: bool = Field(default=False)
    supports_lora: bool = Field(default=False)
    max_subjects: int = Field(default=0)
    accepts_text_prompt: bool = Field(default=True)
    accepts_negative_prompt: bool = Field(default=True)
    supported_tts_languages: List[str] = Field(default=[])

class ContentConfig(BaseModel): pass
class ProjectState(BaseModel): pass

class BaseModuleConfig(BaseModel): model_id: str

class BaseLLM(ABC):
    def __init__(self, config: BaseModuleConfig): self.config = config; self.model = None; self.tokenizer = None
    @classmethod
    @abstractmethod
    def get_capabilities(cls) -> ModuleCapabilities: raise NotImplementedError
    @abstractmethod
    def generate_script(self, topic: str, content_config: ContentConfig) -> Dict[str, Any]: pass
    @abstractmethod
    def generate_shot_visual_prompts(self, scene_narration: str, original_scene_prompt: str, num_shots: int, content_config: ContentConfig, main_subject: str, setting: str) -> List[Tuple[str, str]]: pass
    @abstractmethod
    def clear_vram(self): pass

class BaseTTS(ABC):
    def __init__(self, config: BaseModuleConfig): self.config = config; self.model = None
    @classmethod
    @abstractmethod
    def get_capabilities(cls) -> ModuleCapabilities: raise NotImplementedError
    @abstractmethod
    def generate_audio(self, text: str, output_dir: str, scene_idx: int, language: str, speaker_wav: Optional[str] = None) -> Tuple[str, float]: pass
    @abstractmethod
    def clear_vram(self): pass

class BaseVideoGen(ABC):
    def __init__(self, config: BaseModuleConfig): self.config = config; self.pipe = None
    @classmethod
    @abstractmethod
    def get_capabilities(cls) -> ModuleCapabilities: raise NotImplementedError
    
    # --- MODIFIED: Changed to @classmethod ---
    @classmethod
    @abstractmethod
    def get_model_capabilities(cls) -> Dict[str, Any]:
        """Returns a dictionary of the model's capabilities, like resolutions and max duration."""
        raise NotImplementedError
    # --- END OF MODIFICATION ---

    def enhance_prompt(self, prompt: str, prompt_type: str = "visual") -> str: return prompt
    @abstractmethod
    def clear_vram(self): pass

class BaseT2I(BaseVideoGen):
    @abstractmethod
    def generate_image(self, prompt: str, negative_prompt: str, output_path: str, width: int, height: int, ip_adapter_image: Optional[Union[str, List[str]]] = None, seed: int = -1) -> str: pass

class BaseI2V(BaseVideoGen):
    @abstractmethod
    def generate_video_from_image(self, image_path: str, output_video_path: str, target_duration: float, content_config: ContentConfig, visual_prompt: str, motion_prompt: Optional[str], ip_adapter_image: Optional[Union[str, List[str]]] = None) -> str: pass

class BaseT2V(BaseVideoGen):
    @abstractmethod
    def generate_video_from_text(self, prompt: str, output_video_path: str, num_frames: int, fps: int, width: int, height: int, ip_adapter_image: Optional[Union[str, List[str]]] = None) -> str: pass



==== config_manager.py ====
# In config_manager.py
import os
import torch
import gc
from pydantic import BaseModel, Field
from typing import Dict, Tuple, Literal, List, Optional
from uuid import UUID, uuid4

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

class ContentConfig(BaseModel):
    fps: int = 24
    output_dir: str = "modular_reels_output"
    font_for_subtitles: str = "Arial"
    add_narration_text_to_video: bool = True
    aspect_ratio_format: Literal["Portrait", "Landscape"] = "Landscape"
    
    @property
    def final_output_resolution(self) -> Tuple[int, int]:
        return (1080, 1920) if self.aspect_ratio_format == "Portrait" else (1920, 1080)

class CharacterVersion(BaseModel):
    uuid: UUID = Field(default_factory=uuid4)
    name: str = "base"
    reference_image_path: str
    t2i_module_path: Optional[str] = None
    t2i_prompt: Optional[str] = None
    status: str = "completed"

class Character(BaseModel):
    uuid: UUID = Field(default_factory=uuid4)
    name: str
    versions: List[CharacterVersion] = Field(default_factory=list)

class Voice(BaseModel):
    uuid: UUID = Field(default_factory=uuid4)
    name: str
    tts_module_path: str
    reference_wav_path: str
    
class Narration(BaseModel):
    text: str = ""
    voice_uuid: Optional[UUID] = None
    audio_path: Optional[str] = None
    duration: float = 0.0
    status: str = "pending"

class Shot(BaseModel):
    uuid: UUID = Field(default_factory=uuid4)
    generation_flow: Literal["T2I_I2V", "T2V", "Upload_I2V"] = "T2I_I2V"
    uploaded_image_path: Optional[str] = None
    user_defined_duration: Optional[float] = None
    
    visual_prompt: str = "A cinematic shot"
    motion_prompt: str = "Subtle camera movement"
    module_selections: Dict[str, str] = Field(default_factory=dict)
    character_uuids: List[UUID] = Field(default_factory=list)
    keyframe_image_path: Optional[str] = None
    video_path: Optional[str] = None
    status: str = "pending"

class Scene(BaseModel):
    uuid: UUID = Field(default_factory=uuid4)
    title: str = "Untitled Scene"
    narration: Narration = Field(default_factory=Narration)
    shots: List[Shot] = Field(default_factory=list)
    assembled_video_path: Optional[str] = None
    status: str = "pending"

class ProjectState(BaseModel):
    uuid: UUID = Field(default_factory=uuid4)
    title: str
    video_format: Literal["Portrait", "Landscape"] = "Landscape"
    characters: List[Character] = Field(default_factory=list)
    voices: List[Voice] = Field(default_factory=list)
    scenes: List[Scene] = Field(default_factory=list)
    final_video_path: Optional[str] = None
    status: str = "in_progress"
    add_narration_text_to_video: bool = True
    
    @property
    def final_output_resolution(self) -> Tuple[int, int]:
        return (1080, 1920) if self.video_format == "Portrait" else (1920, 1080)

def clear_vram_globally(*items_to_del):
    """
    Safely clears VRAM by checking item dtypes before moving to CPU,
    then deleting references and running garbage collection.
    """
    print(f"Attempting to clear VRAM. Received {len(items_to_del)} items to delete.")
    for item in items_to_del:
        if item is None:
            continue
            
        # Only try to move items to CPU if they support it and are not float16
        # This prevents the "Pipelines loaded with dtype=torch.float16" warning.
        if hasattr(item, 'to') and hasattr(item, 'dtype') and item.dtype != torch.float16:
            try:
                item.to('cpu')
            except Exception as e:
                print(f"Could not move item of type {type(item).__name__} to CPU: {e}")

    # Delete the references to the objects passed into the function.
    # The calling function (e.g., module.clear_vram) is responsible for setting its own attributes to None.
    del items_to_del
    
    # Run garbage collection and empty the CUDA cache
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    print("VRAM clearing attempt finished.")



==== module_discovery.py ====
# In module_discovery.py

import os
import importlib
import inspect
from typing import Dict, List, Any, Type
# Correctly import from base_modules
from base_modules import BaseLLM, BaseTTS, BaseT2I, BaseI2V, BaseT2V, ModuleCapabilities

MODULE_TYPES = {
    "llm": {"base_class": BaseLLM, "path": "llm_modules"},
    "tts": {"base_class": BaseTTS, "path": "tts_modules"},
    "t2i": {"base_class": BaseT2I, "path": "t2i_modules"},
    "i2v": {"base_class": BaseI2V, "path": "i2v_modules"},
    "t2v": {"base_class": BaseT2V, "path": "t2v_modules"},
}

def discover_modules() -> Dict[str, List[Dict[str, Any]]]:
    """
    Scans module directories, imports classes, and gets their capabilities.
    """
    discovered_modules = {key: [] for key in MODULE_TYPES}
    
    for module_type, info in MODULE_TYPES.items():
        module_path = info["path"]
        base_class = info["base_class"]
        
        if not os.path.exists(module_path):
            continue
            
        for filename in os.listdir(module_path):
            if filename.endswith(".py") and not filename.startswith("__"):
                module_name = f"{module_path}.{filename[:-3]}"
                try:
                    module = importlib.import_module(module_name)
                    for attribute_name in dir(module):
                        attribute = getattr(module, attribute_name)
                        if inspect.isclass(attribute) and issubclass(attribute, base_class) and attribute is not base_class:
                            caps = attribute.get_capabilities()
                            discovered_modules[module_type].append({
                                "name": attribute.__name__,
                                "path": f"{module_name}.{attribute.__name__}",
                                "caps": caps,
                                "class": attribute
                            })
                except Exception as e:
                    print(f"Warning: Could not load module {module_name}. Error: {e}")
                    
    return discovered_modules



==== project_manager.py ====
# In project_manager.py
import os
import logging
import shutil
from typing import Optional, Any
from uuid import UUID

from config_manager import ProjectState, Character, CharacterVersion, Voice, Scene, Shot

logger = logging.getLogger(__name__)

def _safe_remove(path: Optional[str]):
    if path and os.path.exists(path):
        try: os.remove(path)
        except OSError as e: logger.error(f"Error removing file {path}: {e}")

class ProjectManager:
    # ... (__init__, _save_state, initialize_project, load_project, getters, _cleanup_shot_files, delete methods, add methods are all correct and unchanged)
    def __init__(self, output_dir: str):
        self.output_dir = output_dir; self.project_file = os.path.join(output_dir, "project_state.json"); self.state: Optional[ProjectState] = None; os.makedirs(self.output_dir, exist_ok=True)
    def _save_state(self):
        if not self.state: return
        with open(self.project_file, 'w', encoding='utf-8') as f: f.write(self.state.model_dump_json(indent=2))
    def initialize_project(self, title: str, video_format: str):
        self.state = ProjectState(title=title, video_format=video_format); self._save_state()
    def load_project(self) -> bool:
        if not os.path.exists(self.project_file): return False
        try:
            with open(self.project_file, 'r', encoding='utf-8') as f: self.state = ProjectState.model_validate_json(f.read())
            needs_save = False
            if not hasattr(self.state, 'add_narration_text_to_video'): self.state.add_narration_text_to_video = True; needs_save = True
            for i, scene in enumerate(self.state.scenes):
                if not hasattr(scene, 'title'): scene.title = f"Scene {i + 1}"; needs_save = True
                for shot in scene.shots:
                    if not hasattr(shot, 'generation_flow'): shot.generation_flow = "T2I_I2V"; needs_save = True
                    if not hasattr(shot, 'uploaded_image_path'): shot.uploaded_image_path = None; needs_save = True
                    if not hasattr(shot, 'user_defined_duration'): shot.user_defined_duration = None; needs_save = True
            if needs_save: logger.warning(f"Upgraded old project format: '{self.state.title}'"); self._save_state()
            return True
        except Exception as e: logger.error(f"Error loading or migrating project: {e}", exc_info=True); return False
    def get_scene(self, uuid: UUID) -> Optional[Scene]: return next((s for s in self.state.scenes if s.uuid == uuid), None)
    def get_shot(self, scene_uuid: UUID, shot_uuid: UUID) -> Optional[Shot]:
        scene = self.get_scene(scene_uuid); return next((s for s in scene.shots if s.uuid == shot_uuid), None) if scene else None
    def get_character(self, uuid: UUID) -> Optional[Character]: return next((c for c in self.state.characters if c.uuid == uuid), None)
    def get_voice(self, uuid: UUID) -> Optional[Voice]: return next((v for v in self.state.voices if v.uuid == uuid), None)
    def _cleanup_shot_files(self, shot: Shot):
        _safe_remove(shot.keyframe_image_path); _safe_remove(shot.video_path); _safe_remove(shot.uploaded_image_path)
        shot_dir = os.path.join(self.output_dir, "shots", str(shot.uuid))
        if os.path.isdir(shot_dir):
            try: shutil.rmtree(shot_dir)
            except OSError as e: logger.error(f"Error removing shot directory {shot_dir}: {e}")
    def delete_scene(self, scene_uuid: UUID):
        if not self.state: return
        scene_to_delete = self.get_scene(scene_uuid)
        if scene_to_delete:
            for shot in scene_to_delete.shots: self._cleanup_shot_files(shot)
            _safe_remove(scene_to_delete.assembled_video_path); _safe_remove(scene_to_delete.narration.audio_path)
        self.state.scenes = [s for s in self.state.scenes if s.uuid != scene_uuid]; self._save_state()
    def delete_shot(self, scene_uuid: UUID, shot_uuid: UUID):
        scene = self.get_scene(scene_uuid)
        if scene:
            shot_to_delete = next((s for s in scene.shots if s.uuid == shot_uuid), None)
            if shot_to_delete: self._cleanup_shot_files(shot_to_delete)
            scene.shots = [s for s in scene.shots if s.uuid != shot_uuid]; self._save_state()
    def add_character(self, name: str, base_image_path: str):
        char = Character(name=name); version = CharacterVersion(name="base", reference_image_path=base_image_path); char.versions.append(version)
        self.state.characters.append(char); self._save_state()
    def add_voice(self, name: str, module_path: str, wav_path: str):
        voice = Voice(name=name, tts_module_path=module_path, reference_wav_path=wav_path); self.state.voices.append(voice); self._save_state()
    def add_scene(self):
        if not self.state: return; self.state.scenes.append(Scene(title=f"Scene {len(self.state.scenes) + 1}")); self._save_state()
    def add_shot_to_scene(self, scene_uuid: UUID):
        scene = self.get_scene(scene_uuid)
        if scene:
            new_shot = Shot()
            if scene.shots:
                last_shot = scene.shots[-1]; new_shot.generation_flow = last_shot.generation_flow; new_shot.module_selections = last_shot.module_selections.copy()
            scene.shots.append(new_shot); self._save_state()
    def update_scene(self, scene_uuid: UUID, data: dict):
        scene = self.get_scene(scene_uuid)
        if scene:
            if 'title' in data: scene.title = data['title']
            if 'narration_text' in data: scene.narration.text = data['narration_text']; scene.narration.status = "pending"; scene.narration.audio_path = None
            if 'voice_uuid' in data: scene.narration.voice_uuid = data['voice_uuid']; scene.narration.status = "pending"
            self._save_state()

    # --- START: DEFINITIVE update_shot FIX ---
    def update_shot(self, scene_uuid: UUID, shot_uuid: UUID, data: dict):
        shot = self.get_shot(scene_uuid, shot_uuid)
        if not shot: return

        # Track what kind of change is happening
        invalidates_image = False
        invalidates_video = False

        for key, value in data.items():
            if hasattr(shot, key) and getattr(shot, key) != value:
                setattr(shot, key, value)
                # Determine the consequence of the change
                if key in ['visual_prompt', 'character_uuids'] or (key == 'module_selections' and value.get('t2i') != shot.module_selections.get('t2i')):
                    invalidates_image = True
                elif key in ['motion_prompt', 'user_defined_duration'] or (key == 'module_selections' and value.get('i2v') != shot.module_selections.get('i2v')) or (key == 'module_selections' and value.get('t2v') != shot.module_selections.get('t2v')):
                    invalidates_video = True
                elif key == 'generation_flow':
                    invalidates_image = True # Changing flow is a major change
        
        # Apply invalidations
        if invalidates_image:
            _safe_remove(shot.keyframe_image_path)
            shot.keyframe_image_path = None
            shot.status = "pending"
        
        if invalidates_image or invalidates_video:
            _safe_remove(shot.video_path)
            shot.video_path = None
            if shot.status != "pending": # Don't override a full reset
                shot.status = "image_generated" if shot.keyframe_image_path else "pending"

        self._save_state()
    # --- END: DEFINITIVE FIX ---

    def update_final_video_path(self, path: Optional[str]):
        if self.state: self.state.final_video_path = path; self.state.status = "completed" if path else "in_progress"; self._save_state()



==== system.py ====
# In system.py
import streamlit as st
import psutil
import GPUtil
from module_discovery import discover_modules
from pydantic import BaseModel

class SystemConfig(BaseModel):
    vram_gb: float
    ram_gb: float

def go_to_step(step_name: str):
    """Utility function to navigate between app steps."""
    st.session_state.current_step = step_name
    st.rerun()

# --- START: DEFINITIVE FIX for select_item ---
def select_item(item_type: str, item_uuid):
    """
    Sets the currently selected item in the inspector.
    It does NOT rerun the app. The caller is responsible for the rerun.
    """
    st.session_state.selected_item_type = item_type
    st.session_state.selected_item_uuid = item_uuid
# --- END: DEFINITIVE FIX ---

@st.cache_resource
def get_discovered_modules():
    print("--- Discovering all available modules... ---")
    return discover_modules()

@st.cache_resource
def get_system_config() -> SystemConfig:
    print("--- Detecting system specifications... ---")
    try:
        gpus = GPUtil.getGPUs()
        vram = gpus[0].memoryTotal / 1024 if gpus else 0.0
    except Exception:
        vram = 0.0
    ram = psutil.virtual_memory().total / (1024**3)
    return SystemConfig(vram_gb=round(vram, 1), ram_gb=round(ram, 1))

def initialize_system():
    if 'system_config' not in st.session_state:
        st.session_state.system_config = get_system_config()
    if 'discovered_modules' not in st.session_state:
        st.session_state.discovered_modules = get_discovered_modules()



==== task_executor.py ====
# In task_executor.py
import logging
import os
from uuid import UUID
from importlib import import_module
from typing import List
from project_manager import ProjectManager
from video_assembly import assemble_final_reel, assemble_scene_video_from_sub_clips
from config_manager import Shot, Scene, ContentConfig

logger = logging.getLogger(__name__)

def _import_class(module_path_str: str):
    if not module_path_str: return None
    try:
        module_path, class_name = module_path_str.rsplit('.', 1)
        module = import_module(module_path)
        return getattr(module, class_name)
    except Exception as e:
        print(f"Warning: Could not import module: {module_path_str}. Error: {e}")
        return None

class TaskExecutor:
    def __init__(self, project_manager: ProjectManager):
        self.pm = project_manager

    def get_module_max_duration(self, shot: Shot) -> float:
        flow = shot.generation_flow
        if flow in ("T2I_I2V", "Upload_I2V"): module_path = shot.module_selections.get('i2v')
        elif flow == "T2V": module_path = shot.module_selections.get('t2v')
        else: return 0.0
        if not module_path: return 0.0
        ModuleClass = _import_class(module_path)
        if not ModuleClass: return 0.0
        try: return ModuleClass.get_model_capabilities().get("max_shot_duration", 0.0)
        except Exception: return 0.0

    def calculate_shot_durations_for_scene(self, scene: Scene) -> List[float]:
        if not scene.narration.duration > 0 or not scene.shots:
            return [0.0] * len(scene.shots)
        remaining_time = scene.narration.duration
        calculated_durations = []
        for shot in scene.shots:
            if remaining_time <= 0:
                calculated_durations.append(0.0)
                continue
            module_capacity = self.get_module_max_duration(shot)
            if shot.user_defined_duration is not None and shot.user_defined_duration > 0:
                contribution = min(shot.user_defined_duration, module_capacity, remaining_time)
            else:
                contribution = min(module_capacity, remaining_time)
            calculated_durations.append(contribution)
            remaining_time -= contribution
        return calculated_durations

    def execute_task(self, task: str, task_data: dict) -> bool:
        task_map = {
            "generate_audio": self._execute_generate_audio,
            "generate_shot_image": self._execute_generate_shot_image,
            "generate_shot_video": self._execute_generate_shot_video,
            "generate_shot_t2v": self._execute_generate_shot_t2v,
            "assemble_final_video": self._execute_assemble_final,
        }
        try:
            if task in task_map: return task_map[task](**task_data)
            logger.error(f"Unknown task: {task}"); return False
        except Exception as e:
            logger.error(f"Error executing task {task}: {e}", exc_info=True); return False

    def _execute_generate_audio(self, scene_uuid: UUID) -> bool:
        scene = self.pm.get_scene(scene_uuid)
        if not scene or not scene.narration.voice_uuid: return False
        voice = self.pm.get_voice(scene.narration.voice_uuid)
        if not voice: return False
        try: scene_idx = self.pm.state.scenes.index(scene)
        except ValueError: logger.error(f"Could not find scene with UUID {scene_uuid} in project state."); return False
        TtsClass = _import_class(voice.tts_module_path); tts_module = TtsClass(TtsClass.Config())
        path, duration = tts_module.generate_audio(text=scene.narration.text, output_dir=self.pm.output_dir, scene_idx=scene_idx, language="en", speaker_wav=voice.reference_wav_path)
        tts_module.clear_vram()
        if path and os.path.exists(path):
            scene.narration.audio_path = path; scene.narration.duration = duration; scene.narration.status = "generated"; self.pm._save_state(); return True
        return False

    def _execute_generate_shot_image(self, scene_uuid: UUID, shot_uuid: UUID) -> bool:
        shot = self.pm.get_shot(scene_uuid, shot_uuid);
        if not shot: return False
        t2i_path = shot.module_selections.get('t2i');
        if not t2i_path: logger.error("T2I module not selected for T2I_I2V flow."); return False
        T2iClass = _import_class(t2i_path); t2i_module = T2iClass(T2iClass.Config())
        model_caps = t2i_module.get_model_capabilities(); w, h = model_caps["resolutions"][self.pm.state.video_format]
        path = os.path.join(self.pm.output_dir, "shots", f"shot_{shot.uuid}_keyframe.png"); os.makedirs(os.path.dirname(path), exist_ok=True)
        ip_adapter_paths = [self.pm.get_character(uid).versions[0].reference_image_path for uid in shot.character_uuids]
        img_path = t2i_module.generate_image(prompt=shot.visual_prompt, negative_prompt="bad quality", output_path=path, width=w, height=h, ip_adapter_image=ip_adapter_paths); t2i_module.clear_vram()
        if img_path: shot.keyframe_image_path = img_path; shot.status = "image_generated"; self.pm._save_state(); return True
        return False

    # --- START: DEFINITIVE FIX ---
    def _execute_generate_shot_video(self, scene_uuid: UUID, shot_uuid: UUID) -> bool:
        shot = self.pm.get_shot(scene_uuid, shot_uuid)
        if not shot: return False
        
        scene = self.pm.get_scene(scene_uuid)
        try: shot_index = scene.shots.index(shot)
        except ValueError: logger.error(f"Shot {shot_uuid} not found in scene {scene_uuid}"); return False
        
        all_durations = self.calculate_shot_durations_for_scene(scene)
        target_duration = all_durations[shot_index]
        
        if target_duration <= 0:
            logger.warning(f"Target duration for shot {shot.uuid} is 0. Skipping generation.")
            return True
        
        # --- UNCHANGED but now executed ---
        if shot.generation_flow == "Upload_I2V": shot.keyframe_image_path = shot.uploaded_image_path; self.pm._save_state()
        i2v_path = shot.module_selections.get('i2v')
        if not i2v_path: logger.error("I2V module not selected."); return False
        if not shot.keyframe_image_path or not os.path.exists(shot.keyframe_image_path): logger.error("Cannot generate video, keyframe image missing."); return False
        
        I2vClass = _import_class(i2v_path); i2v_module = I2vClass(I2vClass.Config())
        temp_config = ContentConfig(output_dir=self.pm.output_dir, aspect_ratio_format=self.pm.state.video_format)
        vid_path = os.path.join(self.pm.output_dir, "shots", f"shot_{shot.uuid}_video.mp4")
        
        result_path = i2v_module.generate_video_from_image(image_path=shot.keyframe_image_path, output_video_path=vid_path, target_duration=target_duration, content_config=temp_config, visual_prompt=shot.visual_prompt, motion_prompt=shot.motion_prompt); i2v_module.clear_vram()
        
        if result_path: shot.video_path = result_path; shot.status = "video_generated"; self.pm._save_state(); return True
        return False

    def _execute_generate_shot_t2v(self, scene_uuid: UUID, shot_uuid: UUID) -> bool:
        shot = self.pm.get_shot(scene_uuid, shot_uuid)
        if not shot: return False

        scene = self.pm.get_scene(scene_uuid)
        try: shot_index = scene.shots.index(shot)
        except ValueError: logger.error(f"Shot {shot_uuid} not found in scene {scene_uuid}"); return False

        all_durations = self.calculate_shot_durations_for_scene(scene)
        target_duration = all_durations[shot_index]

        if target_duration <= 0:
            logger.warning(f"Target duration for T2V shot {shot.uuid} is 0. Skipping generation.")
            return True
        
        # --- UNCHANGED but now executed ---
        t2v_path = shot.module_selections.get('t2v')
        if not t2v_path: logger.error("T2V module not selected for T2V flow."); return False
        
        T2vClass = _import_class(t2v_path); t2v_module = T2vClass(T2vClass.Config())
        model_caps = t2v_module.get_model_capabilities(); w, h = model_caps["resolutions"][self.pm.state.video_format]
        vid_path = os.path.join(self.pm.output_dir, "shots", f"shot_{shot.uuid}_video.mp4"); os.makedirs(os.path.dirname(vid_path), exist_ok=True)
        fps = 24; num_frames = int(target_duration * fps)
        
        ip_adapter_paths = [self.pm.get_character(uid).versions[0].reference_image_path for uid in shot.character_uuids]
        result_path = t2v_module.generate_video_from_text(prompt=shot.visual_prompt, output_video_path=vid_path, num_frames=num_frames, fps=fps, width=w, height=h, ip_adapter_image=ip_adapter_paths); t2v_module.clear_vram()
        
        if result_path: shot.video_path = result_path; shot.keyframe_image_path = None; shot.status = "video_generated"; self.pm._save_state(); return True
        return False
    # --- END: DEFINITIVE FIX ---
        
    def _execute_assemble_final(self, **kwargs) -> bool:
        assets = []
        for i, scene in enumerate(self.pm.state.scenes):
            shot_videos = [s.video_path for s in scene.shots if s.video_path and os.path.exists(s.video_path)]
            if not shot_videos or not scene.narration.audio_path or not os.path.exists(scene.narration.audio_path) or scene.narration.duration <= 0: continue
            
            scene_cfg = ContentConfig(output_dir=self.pm.output_dir)
            scene_video_path = assemble_scene_video_from_sub_clips(sub_clip_paths=shot_videos, target_total_duration=scene.narration.duration, config=scene_cfg, scene_idx=i)
            if not scene_video_path: continue
            scene.assembled_video_path = scene_video_path
            assets.append((scene.assembled_video_path, scene.narration.audio_path, {"text": scene.narration.text, "duration": scene.narration.duration}))
        
        if not assets: logger.error("No completed assets found to assemble."); return False
        self.pm._save_state()
        final_cfg = ContentConfig(output_dir=self.pm.output_dir, aspect_ratio_format=self.pm.state.video_format, add_narration_text_to_video=self.pm.state.add_narration_text_to_video)
        final_path = assemble_final_reel(assets, final_cfg)
        if final_path: self.pm.update_final_video_path(final_path); return True
        return False



==== ui_components.py ====
# In ui_components.py
import streamlit as st
from system import get_discovered_modules, select_item
from utils import list_projects
import os

def render_project_selection_page(create_callback, load_callback):
    # This function doesn't need the project manager, so it's correct
    st.title("üé• Modular AI Video Studio"); c1, c2 = st.columns([1, 2]);
    with c1:
        st.subheader("üöÄ Create New Project")
        with st.form("new_project_form"):
            title = st.text_input("Project Title", "My Awesome Video"); video_format = st.selectbox("Format", ("Landscape", "Portrait"), index=0)
            if st.form_submit_button("Create Project", type="primary"):
                if title: create_callback(title, video_format)
    with c2:
        st.subheader("üìÇ Load Existing Project"); projects = list_projects();
        if not projects: st.info("No projects found.")
        else:
            for p in projects:
                with st.container(border=True):
                    cols = st.columns([4, 1]); cols[0].markdown(f"**{p['title']}**")
                    if cols[1].button("Load", key=f"load_{p['name']}", use_container_width=True):
                        load_callback(p['name'])

# --- START: DEFINITIVE FIX ---
def render_asset_panel(pm, ui_executor):
    st.subheader("üé≠ Casting");
    with st.expander("Characters", expanded=True):
        if not pm.state.characters: st.caption("No characters created yet.")
        for char in pm.state.characters:
            cols = st.columns([1, 3, 1])
            if char.versions and char.versions[0].reference_image_path and os.path.exists(char.versions[0].reference_image_path):
                cols[0].image(char.versions[0].reference_image_path, width=50)
            cols[1].write(char.name)
            if cols[2].button("üëÅÔ∏è", key=f"select_char_{char.uuid}", help="Inspect Character"): 
                select_item('character', char.uuid)
                st.rerun()
        with st.popover("‚ûï Add New Character", use_container_width=True):
            with st.form("add_char_form", clear_on_submit=True):
                name = st.text_input("Character Name"); image = st.file_uploader("Upload Base Image", type=['png', 'jpg', 'jpeg'])
                if st.form_submit_button("Create Character", type="primary"):
                    if name and image: ui_executor.create_character(name, image)
    with st.expander("Voices", expanded=True):
        if not pm.state.voices: st.caption("No voices created yet.")
        for voice in pm.state.voices:
            cols = st.columns([4, 1]); cols[0].write(voice.name)
            if cols[1].button("üëÅÔ∏è", key=f"select_voice_{voice.uuid}", help="Inspect Voice"):
                select_item('voice', voice.uuid)
                st.rerun()
        with st.popover("‚ûï Add New Voice", use_container_width=True):
            with st.form("add_voice_form", clear_on_submit=True):
                name = st.text_input("Voice Name"); tts_modules = get_discovered_modules().get('tts', []); tts_options = {mod['caps'].title: mod['path'] for mod in tts_modules}
                selected_title = st.selectbox("TTS Model", options=list(tts_options.keys())); speaker_wav = st.file_uploader("Reference Speaker Audio (.wav)", type=['wav'])
                if st.form_submit_button("Create Voice", type="primary"):
                    if name and selected_title and speaker_wav: ui_executor.create_voice(name, tts_options[selected_title], speaker_wav)

def render_viewer(pm):
    st.subheader("üì∫ Viewer")
    selected_type = st.session_state.get('selected_item_type'); selected_uuid = st.session_state.get('selected_item_uuid')
    with st.container(border=True, height=450):
        item_to_show, video_path, image_path = None, None, None
        if selected_type == 'shot': item_to_show = pm.get_shot(*selected_uuid)
        elif selected_type == 'scene': item_to_show = pm.get_scene(selected_uuid)
        if item_to_show:
            video_path = getattr(item_to_show, 'video_path', None) or getattr(item_to_show, 'assembled_video_path', None)
            image_path = getattr(item_to_show, 'keyframe_image_path', None) or getattr(item_to_show, 'uploaded_image_path', None)
        if video_path and os.path.exists(video_path): st.video(video_path)
        elif image_path and os.path.exists(image_path): st.image(image_path, use_container_width=True)
        elif pm.state.final_video_path and os.path.exists(pm.state.final_video_path): st.video(pm.state.final_video_path)
        else: st.info("The final video or selected asset preview will appear here.")

def render_timeline(pm, ui_executor):
    st.subheader("üóìÔ∏è Timeline")
    with st.container(border=True, height=350):
        if not pm.state.scenes: st.caption("No scenes yet. Use the Inspector to add a scene.")
        for i, scene in enumerate(pm.state.scenes):
            narration_duration = scene.narration.duration; all_shot_durations = ui_executor.task_executor.calculate_shot_durations_for_scene(scene)
            total_shot_contribution = sum(all_shot_durations)
            if narration_duration == 0: summary_icon = "‚è≥"
            elif total_shot_contribution >= narration_duration - 0.1: summary_icon = "‚úÖ"
            else: summary_icon = "‚ùå"
            expander_title = f"{summary_icon} {scene.title} ({narration_duration:.1f}s)"
            with st.expander(expander_title, expanded=True):
                is_selected_scene = st.session_state.selected_item_type == 'scene' and st.session_state.selected_item_uuid == scene.uuid
                coverage_ratio = (total_shot_contribution / narration_duration) if narration_duration > 0 else 1.0
                if narration_duration == 0: coverage_color, coverage_text = "grey", "Generate audio to see coverage"
                elif abs(total_shot_contribution - narration_duration) < 0.1: coverage_color, coverage_text = "green", f"Coverage: {total_shot_contribution:.1f}s / {narration_duration:.1f}s"
                else: coverage_color, coverage_text = "red", f"Deficit: {(narration_duration - total_shot_contribution):.1f}s"
                st.progress(min(coverage_ratio, 1.0), text=coverage_text)
                if st.button("Inspect Scene", key=f"select_scene_{scene.uuid}", type="primary" if is_selected_scene else "secondary", use_container_width=True):
                    select_item('scene', scene.uuid)
                    st.rerun()
                st.markdown("---")
                if not scene.shots:
                    st.caption("No shots in this scene. Use the Inspector to add one.")
                else:
                    shot_cols = st.columns(len(scene.shots))
                    for j, shot in enumerate(scene.shots):
                        with shot_cols[j]:
                            with st.container(border=True):
                                module_capacity = ui_executor.task_executor.get_module_max_duration(shot); actual_contribution = all_shot_durations[j]
                                if module_capacity >= actual_contribution and actual_contribution > 0: status_icon, color = "‚úÖ", "green"
                                else: status_icon, color = "‚ùå", "red"
                                status_icon_btn = "üé¨" if shot.status == 'video_generated' else "üñºÔ∏è" if shot.status in ['image_generated', 'upload_complete'] else "‚è≥"
                                st.markdown(f"**{status_icon_btn} Shot {j+1}**")
                                st.markdown(f"<small style='color:{color};'>{status_icon} {actual_contribution:.2f}s / {module_capacity:.2f}s</small>", unsafe_allow_html=True)
                                is_selected_shot = st.session_state.selected_item_type == 'shot' and st.session_state.selected_item_uuid[1] == shot.uuid
                                if st.button("Inspect", key=f"select_shot_{shot.uuid}", use_container_width=True, type="primary" if is_selected_shot else "secondary"):
                                    select_item('shot', (scene.uuid, shot.uuid))
                                    st.rerun()

def render_inspector_panel(pm, ui_executor):
    selected_type = st.session_state.get('selected_item_type'); selected_uuid = st.session_state.get('selected_item_uuid')
    if not selected_uuid: st.info("Select an item to inspect its properties."); return
    if selected_type == 'project':
        st.markdown(f"**Project: {pm.state.title}**"); st.write(f"Format: {pm.state.video_format}")
        st.checkbox("Add Narration Text to Final Video", value=pm.state.add_narration_text_to_video, key="project_add_text", on_change=lambda: setattr(pm.state, 'add_narration_text_to_video', st.session_state.project_add_text) or pm._save_state())
        st.divider(); st.subheader("Timeline Management"); 
        if st.button("‚ûï Add New Scene", use_container_width=True): ui_executor.add_scene()
    elif selected_type == 'scene':
        scene = pm.get_scene(selected_uuid); st.markdown(f"**üé¨ Scene Inspector**")
        st.text_input("Scene Title", value=scene.title, key=f"title_{scene.uuid}", on_change=ui_executor.update_scene_title, args=(scene.uuid,))
        st.subheader("Narration"); st.text_area("Script", value=scene.narration.text, key=f"narration_{scene.uuid}", height=120, on_change=ui_executor.update_scene_narration, args=(scene.uuid,))
        voice_options = {v.name: v.uuid for v in pm.state.voices}
        if not voice_options: st.warning("No voices created!")
        else:
            voice_names = list(voice_options.keys()); current_voice = pm.get_voice(scene.narration.voice_uuid);
            if not current_voice and voice_names: current_voice = pm.get_voice(voice_options[voice_names[0]]); ui_executor.update_scene_voice(scene.uuid)
            idx = voice_names.index(current_voice.name) if current_voice else 0
            st.selectbox("Voice", options=voice_names, index=idx, key=f"voice_select_{scene.uuid}", on_change=ui_executor.update_scene_voice, args=(scene.uuid,))
        can_gen_audio = bool(voice_options and st.session_state.get(f"narration_{scene.uuid}", scene.narration.text) and scene.narration.voice_uuid)
        if scene.narration.audio_path and os.path.exists(scene.narration.audio_path):
            st.audio(scene.narration.audio_path)
            if st.button("Regen Audio", use_container_width=True, disabled=not can_gen_audio): ui_executor.generate_scene_audio(scene.uuid)
        else:
            if st.button("Gen Audio", use_container_width=True, type="primary", disabled=not can_gen_audio): ui_executor.generate_scene_audio(scene.uuid)
        st.divider(); st.subheader("Management")
        if st.button("‚ûï Add Shot", use_container_width=True): ui_executor.add_shot_to_scene(scene.uuid)
        if st.button("üóëÔ∏è Delete Scene", type="secondary", use_container_width=True): ui_executor.delete_scene(scene.uuid)
    elif selected_type == 'shot':
        # This section is correct and unchanged
        scene_uuid, shot_uuid = selected_uuid; scene = pm.get_scene(scene_uuid); shot = pm.get_shot(scene_uuid, shot_uuid); shot_idx = scene.shots.index(shot)
        st.markdown(f"**üéûÔ∏è Shot {shot_idx + 1} Inspector**")
        with st.form(key=f"shot_form_{shot.uuid}"):
            st.subheader("Generation Flow"); flow_options = {"T2I ‚û°Ô∏è I2V": "T2I_I2V", "T2V": "T2V", "Upload ‚û°Ô∏è I2V": "Upload_I2V"}; flow_captions = list(flow_options.keys())
            current_flow_idx = flow_captions.index(next((k for k, v in flow_options.items() if v == shot.generation_flow), "T2I ‚û°Ô∏è I2V"))
            flow_selection = st.radio("Method", options=flow_captions, index=current_flow_idx, horizontal=True)
            visual = st.text_area("Visual Prompt", shot.visual_prompt, height=100)
            motion = st.text_area("Motion Prompt", shot.motion_prompt, height=68, help="For I2V models, describe desired motion.")
            char_options = {c.name: c.uuid for c in pm.state.characters}; default_chars = [pm.get_character(uid).name for uid in shot.character_uuids if pm.get_character(uid)]
            assigned_chars = st.multiselect("Characters", options=list(char_options.keys()), default=default_chars)
            st.subheader("Technical Details"); modules = get_discovered_modules(); uploaded_file = None
            t2i_options = {m['caps'].title: m['path'] for m in modules.get('t2i', [])}; i2v_options = {m['caps'].title: m['path'] for m in modules.get('i2v', [])}; t2v_options = {m['caps'].title: m['path'] for m in modules.get('t2v', [])}
            t2i_paths_rev = {v: k for k, v in t2i_options.items()}; i2v_paths_rev = {v: k for k, v in i2v_options.items()}; t2v_paths_rev = {v: k for k, v in t2v_options.items()}
            new_t2i_title, new_i2v_title, new_t2v_title = None, None, None
            selected_flow_value = flow_options[flow_selection]
            if selected_flow_value == "T2I_I2V":
                if t2i_options: current_t2i_title = t2i_paths_rev.get(shot.module_selections.get('t2i'), list(t2i_options.keys())[0]); new_t2i_title = st.selectbox("Image Model", options=t2i_options.keys(), index=list(t2i_options.keys()).index(current_t2i_title))
                if i2v_options: current_i2v_title = i2v_paths_rev.get(shot.module_selections.get('i2v'), list(i2v_options.keys())[0]); new_i2v_title = st.selectbox("Video Model", options=i2v_options.keys(), index=list(i2v_options.keys()).index(current_i2v_title))
            elif selected_flow_value == "T2V":
                if t2v_options: current_t2v_title = t2v_paths_rev.get(shot.module_selections.get('t2v'), list(t2v_options.keys())[0]); new_t2v_title = st.selectbox("T2V Model", options=t2v_options.keys(), index=list(t2v_options.keys()).index(current_t2v_title))
            elif selected_flow_value == "Upload_I2V":
                uploaded_file = st.file_uploader("Upload Image", type=['png', 'jpg', 'jpeg'])
                if i2v_options: current_i2v_title = i2v_paths_rev.get(shot.module_selections.get('i2v'), list(i2v_options.keys())[0]); new_i2v_title = st.selectbox("Video Model", options=i2v_options.keys(), index=list(i2v_options.keys()).index(current_i2v_title))
            st.subheader("Duration Control"); is_auto_duration = shot.user_defined_duration is None
            auto_duration_toggle = st.checkbox("Auto-set Duration (Greedy)", value=is_auto_duration)
            module_capacity = ui_executor.task_executor.get_module_max_duration(shot); user_duration = shot.user_defined_duration
            if not auto_duration_toggle:
                user_duration = st.slider("Manual Duration (s)", min_value=0.1, max_value=module_capacity or 10.0, value=shot.user_defined_duration or module_capacity, step=0.1)
            if st.form_submit_button("üíæ Update Shot Details"):
                if uploaded_file: ui_executor.handle_shot_image_upload(scene_uuid, shot_uuid, uploaded_file)
                final_module_selections = shot.module_selections.copy()
                if new_t2i_title: final_module_selections['t2i'] = t2i_options.get(new_t2i_title)
                if new_i2v_title: final_module_selections['i2v'] = i2v_options.get(new_i2v_title)
                if new_t2v_title: final_module_selections['t2v'] = t2v_options.get(new_t2v_title)
                new_char_uuids = [char_options[name] for name in assigned_chars]
                final_user_duration = None if auto_duration_toggle else user_duration
                ui_executor.pm.update_shot(scene_uuid, shot_uuid, {"generation_flow": selected_flow_value, "visual_prompt": visual, "motion_prompt": motion, "module_selections": final_module_selections, "character_uuids": new_char_uuids, "user_defined_duration": final_user_duration})
                st.toast("Shot details updated!"); st.rerun()
        st.subheader("Generation")
        if shot.generation_flow == "T2I_I2V":
            cols = st.columns(2)
            with cols[0]:
                if st.button("üñºÔ∏è Generate Image", use_container_width=True, type="primary" if not shot.keyframe_image_path else "secondary"): ui_executor.generate_shot_image(scene_uuid, shot_uuid)
            with cols[1]:
                if st.button("üìπ Generate Video", use_container_width=True, type="primary" if not shot.video_path else "secondary", disabled=not shot.keyframe_image_path): ui_executor.generate_shot_video(scene_uuid, shot_uuid)
        elif shot.generation_flow == "T2V":
            if st.button("üìπ Generate Video", use_container_width=True, type="primary"): ui_executor.generate_shot_t2v(scene_uuid, shot_uuid)
        elif shot.generation_flow == "Upload_I2V":
            if st.button("üìπ Generate Video from Upload", use_container_width=True, type="primary", disabled=not shot.uploaded_image_path): ui_executor.generate_shot_video(scene_uuid, shot_uuid)
        st.divider()
        if st.button("üóëÔ∏è Delete Shot", type="secondary", use_container_width=True): ui_executor.delete_shot(scene_uuid, shot.uuid)
    elif selected_type in ['character', 'voice']:
        asset = pm.get_character(selected_uuid) if selected_type == 'character' else pm.get_voice(selected_uuid)
        st.markdown(f"**{selected_type.title()} Inspector: {asset.name}**")
        if selected_type == 'voice':
            st.write(f"**TTS Model:** `{asset.tts_module_path.split('.')[-1]}`");
            if os.path.exists(asset.reference_wav_path): st.audio(asset.reference_wav_path)
        if st.button(f"üóëÔ∏è Delete {selected_type.title()}", type="secondary", use_container_width=True):
            if selected_type == 'character': ui_executor.delete_character(asset.uuid)
            else: ui_executor.delete_voice(asset.uuid)
    with st.expander("üêû Debug: Show Full Project State"):
        st.json(pm.state.model_dump_json(indent=2))



==== ui_task_executor.py ====
# In ui_task_executor.py
import streamlit as st
import os
from uuid import UUID
from project_manager import ProjectManager
from task_executor import TaskExecutor
from utils import load_and_correct_image_orientation
from config_manager import Shot
from importlib import import_module

def _import_class(module_path_str: str):
    if not module_path_str: return None
    try:
        module_path, class_name = module_path_str.rsplit('.', 1); module = import_module(module_path); return getattr(module, class_name)
    except Exception as e:
        print(f"Warning: Could not import module: {module_path_str}. Error: {e}"); return None

class UITaskExecutor:
    def __init__(self, project_manager: ProjectManager):
        self.pm = project_manager; self._task_executor = None
    @property
    def task_executor(self) -> TaskExecutor:
        if self._task_executor is None: self._task_executor = TaskExecutor(self.pm)
        return self._task_executor
    
    def get_shot_duration(self, shot: Shot) -> float:
        scene = next((s for s in self.pm.state.scenes if shot in s.shots), None)
        if not scene or not scene.narration.duration > 0 or not scene.shots: return 0.0
        target_shot_duration = scene.narration.duration / len(scene.shots); flow = shot.generation_flow
        if flow in ("T2I_I2V", "Upload_I2V"): module_path = shot.module_selections.get('i2v')
        elif flow == "T2V": module_path = shot.module_selections.get('t2v')
        else: return 0.0
        if not module_path: return 0.0
        ModuleClass = _import_class(module_path)
        if not ModuleClass: return 0.0
        try:
            model_caps = ModuleClass.get_model_capabilities(); module_max_duration = model_caps.get("max_shot_duration", 0.0)
        except Exception: module_max_duration = 0.0
        return min(target_shot_duration, module_max_duration)

    def add_scene(self):
        from system import select_item
        if not self.pm or not self.pm.state: return
        self.pm.add_scene()
        # The new scene is the last one in the list
        new_scene = self.pm.state.scenes[-1]
        select_item('scene', new_scene.uuid, rerun=True) # Rerun is now handled by select_item

    def update_scene_title(self, scene_uuid: UUID):
        key = f"title_{scene_uuid}"; new_title = st.session_state.get(key)
        if new_title: self.pm.update_scene(scene_uuid, {"title": new_title})
        
    def update_scene_narration(self, scene_uuid: UUID):
        key = f"narration_{scene_uuid}"; new_text = st.session_state.get(key)
        if new_text is not None: self.pm.update_scene(scene_uuid, {"narration_text": new_text})

    def update_scene_voice(self, scene_uuid: UUID):
        voice_key = f"voice_select_{scene_uuid}"; selected_voice_name = st.session_state.get(voice_key)
        voice_options = {v.name: v.uuid for v in self.pm.state.voices}
        if selected_voice_name and selected_voice_name in voice_options:
            new_voice_uuid = voice_options[selected_voice_name]
            scene = self.pm.get_scene(scene_uuid)
            if scene and scene.narration.voice_uuid != new_voice_uuid:
                self.pm.update_scene(scene_uuid, {"voice_uuid": new_voice_uuid}); st.toast("Voice updated.", icon="üó£Ô∏è")
    
    def _execute_and_reload(self, task, data, success_msg, error_msg):
        with st.spinner(f"{error_msg.replace('failed', 'in progress')}..."): success = self.task_executor.execute_task(task, data)
        if success: st.toast(success_msg, icon="‚ú®")
        else: st.error(error_msg)
        st.rerun()
        
    def create_character(self, name: str, image_file):
        char_dir = os.path.join(self.pm.output_dir, "characters", name.replace(" ", "_")); os.makedirs(char_dir, exist_ok=True)
        img_path = os.path.join(char_dir, "base_reference.png"); corrected_image = load_and_correct_image_orientation(image_file)
        if corrected_image: corrected_image.save(img_path, "PNG"); self.pm.add_character(name, img_path); st.toast(f"Character '{name}' created!", icon="üë§"); st.rerun()
        else: st.error("Could not process image.")
        
    def create_voice(self, name: str, module_path: str, wav_file):
        voice_dir = os.path.join(self.pm.output_dir, "voices", name.replace(" ", "_")); os.makedirs(voice_dir, exist_ok=True)
        wav_path = os.path.join(voice_dir, "reference.wav");
        with open(wav_path, "wb") as f: f.write(wav_file.getbuffer())
        self.pm.add_voice(name, module_path, wav_path); st.toast(f"Voice '{name}' created!", icon="üó£Ô∏è"); st.rerun()
    
    def delete_scene(self, scene_uuid: UUID):
        self.pm.delete_scene(scene_uuid); st.toast("Scene deleted.", icon="üóëÔ∏è"); from system import select_item; select_item('project', self.pm.state.uuid, rerun=True)
    
    def add_shot_to_scene(self, scene_uuid: UUID):
        self.pm.add_shot_to_scene(scene_uuid); st.toast("New shot added.", icon="üéûÔ∏è"); st.rerun()
    
    def delete_shot(self, scene_uuid: UUID, shot_uuid: UUID):
        self.pm.delete_shot(scene_uuid, shot_uuid); st.toast("Shot deleted.", icon="üóëÔ∏è"); from system import select_item; select_item('scene', scene_uuid, rerun=True)
    
    def handle_shot_image_upload(self, scene_uuid: UUID, shot_uuid: UUID, image_file):
        shot_dir = os.path.join(self.pm.output_dir, "shots", str(shot_uuid)); os.makedirs(shot_dir, exist_ok=True)
        img_path = os.path.join(shot_dir, "uploaded_reference.png"); corrected_image = load_and_correct_image_orientation(image_file)
        if corrected_image:
            corrected_image.save(img_path, "PNG"); self.pm.update_shot(scene_uuid, shot_uuid, {"uploaded_image_path": img_path, "status": "upload_complete"}); st.toast("Uploaded image saved.", icon="üñºÔ∏è"); st.rerun()
        else: st.error("Could not process uploaded image.")
        
    def generate_shot_image(self, scene_uuid: UUID, shot_uuid: UUID): self._execute_and_reload("generate_shot_image", {"scene_uuid": scene_uuid, "shot_uuid": shot_uuid}, "Image generated!", "Image generation failed.")
    def generate_shot_video(self, scene_uuid: UUID, shot_uuid: UUID): self._execute_and_reload("generate_shot_video", {"scene_uuid": scene_uuid, "shot_uuid": shot_uuid}, "Video generated!", "Video generation failed.")
    def generate_shot_t2v(self, scene_uuid: UUID, shot_uuid: UUID): self._execute_and_reload("generate_shot_t2v", {"scene_uuid": scene_uuid, "shot_uuid": shot_uuid}, "T2V Video generated!", "T2V generation failed.")
    def generate_scene_audio(self, scene_uuid: UUID): self._execute_and_reload("generate_audio", {"scene_uuid": scene_uuid}, "Audio generated!", "Audio generation failed.")
    def assemble_final_video(self): self._execute_and_reload("assemble_final_video", {}, "Final video assembled!", "Final assembly failed.")
    def delete_character(self, uuid: UUID): self.pm.delete_character(uuid); st.toast("Character deleted.", icon="üóëÔ∏è"); from system import select_item; select_item('project', self.pm.state.uuid, rerun=True)
    def delete_voice(self, uuid: UUID): self.pm.delete_voice(uuid); st.toast("Voice deleted.", icon="üóëÔ∏è"); from system import select_item; select_item('project', self.pm.state.uuid, rerun=True)



==== utils.py ====
# In utils.py
import datetime
import json
import os
from PIL import Image, ImageOps
import streamlit as st

def load_and_correct_image_orientation(image_source):
    """Loads an image and corrects its orientation based on EXIF data."""
    try:
        image = Image.open(image_source)
        corrected_image = ImageOps.exif_transpose(image)
        return corrected_image
    except Exception as e:
        st.error(f"Could not load or correct image: {e}")
        return None

# --- REWRITTEN list_projects function ---
def list_projects():
    """
    Lists all projects from the output directory based on the NEW project structure.
    It looks for 'project_state.json'.
    """
    projects = []
    base_dir = "modular_reels_output"
    if not os.path.exists(base_dir):
        return []

    for project_dir in os.listdir(base_dir):
        project_path = os.path.join(base_dir, project_dir)
        if os.path.isdir(project_path):
            # Look for the new project state file
            project_file = os.path.join(project_path, "project_state.json")
            if os.path.exists(project_file):
                try:
                    with open(project_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # Extract info directly from the new ProjectState model structure
                    projects.append({
                        'name': project_dir,  # The directory name is the unique ID
                        'title': data.get('title', 'Untitled Project'),
                    })
                except json.JSONDecodeError:
                    print(f"Warning: Could not parse project_state.json in {project_dir}")
                except Exception as e:
                    print(f"Error loading project {project_dir}: {e}")
                    
    # Sort by directory name (which includes timestamp) for chronological order
    return sorted(projects, key=lambda p: p['name'], reverse=True)



==== video_assembly.py ====
import math # For math.ceil
import os

from typing import List, Optional, Tuple, Dict, Any
from moviepy import VideoFileClip, AudioFileClip, concatenate_videoclips, TextClip, CompositeVideoClip
from moviepy.audio.AudioClip import concatenate_audioclips, AudioClip
from moviepy.video.VideoClip import ColorClip

from config_manager import ContentConfig 


# --- 5. VIDEO ASSEMBLY ---

def assemble_scene_video_from_sub_clips(
    sub_clip_paths: List[str], 
    target_total_duration: float, 
    config: ContentConfig, 
    scene_idx: int
) -> str:
    """Assembles multiple video sub-clips into a single scene video with precise duration control.
    
    This function takes multiple video sub-clips and combines them into a single scene video
    that matches the target duration. If the combined duration is shorter than the target,
    the video will be looped. If longer, it will be trimmed.
    
    Args:
        sub_clip_paths (List[str]): List of paths to video sub-clips to be combined
        target_total_duration (float): Desired duration for the final scene video in seconds
        config (ContentConfig): Configuration object containing video settings
        scene_idx (int): Index of the scene being assembled
        
    Returns:
        str: Path to the assembled scene video file. Returns empty string if assembly fails.
        
    Note:
        - Handles resource cleanup properly
        - Supports video concatenation and duration adjustment
        - Creates output in the directory specified by config.output_dir
    """
    if not sub_clip_paths:
        print(f"Warning: No sub-clips provided for scene {scene_idx}. Cannot assemble scene video.")
        # Create a short black placeholder?
        placeholder_path = os.path.join(config.output_dir, f"scene_{scene_idx}_placeholder.mp4")
        # Simple way to make a black clip with moviepy if needed, but for now, just return empty string or raise error.
        # For now, let's assume this case is handled upstream or we expect valid paths.
        return "" 

    print(f"Assembling video for scene {scene_idx} from {len(sub_clip_paths)} sub-clips to match duration {target_total_duration:.2f}s.")
    
    clips_to_close = []
    video_sub_clips_mvp = []
    for path in sub_clip_paths:
        clip = VideoFileClip(path)
        video_sub_clips_mvp.append(clip)
        clips_to_close.append(clip)

    # Concatenate raw sub-clips first
    concatenated_raw_video = concatenate_videoclips(video_sub_clips_mvp, method="compose")
    clips_to_close.append(concatenated_raw_video)
    
    # Adjust final concatenated clip to precisely match target_total_duration
    current_duration = concatenated_raw_video.duration
    if abs(current_duration - target_total_duration) < 0.05 : # If very close, accept it
         final_scene_video_timed = concatenated_raw_video 
    elif current_duration > target_total_duration:
        final_scene_video_timed = concatenated_raw_video.subclipped(0, target_total_duration)
    else: # current_duration < target_total_duration - loop the whole concatenated clip
        num_loops = math.ceil(target_total_duration / current_duration)
        looped_clips = [concatenated_raw_video] * num_loops
        temp_looped_video = concatenate_videoclips(looped_clips, method="compose")
        clips_to_close.append(temp_looped_video) # Add to close list
        final_scene_video_timed = temp_looped_video.subclipped(0, target_total_duration)

    # Add the final timed clip to close list if it's a new object (subclip creates new)
    if final_scene_video_timed is not concatenated_raw_video and final_scene_video_timed not in clips_to_close:
        clips_to_close.append(final_scene_video_timed)

    final_scene_video_path = os.path.join(config.output_dir, f"scene_{scene_idx}_assembled_video.mp4")
    try:
        final_scene_video_timed.write_videofile(
            final_scene_video_path, 
            fps=config.fps, 
            codec="libx264", 
            audio=False, # Audio will be added in the final assembly step
            threads=4, preset="medium", logger=None # Quieter logs for sub-assemblies
        )
    except Exception as e:
        print(f"Error writing assembled scene video for scene {scene_idx}: {e}")
        # Fallback or error handling
        final_scene_video_path = "" # Indicate failure
    finally:
        for clip_obj in clips_to_close:
            if hasattr(clip_obj, 'close') and callable(getattr(clip_obj, 'close')):
                clip_obj.close()
    
    print(f"Assembled video for scene {scene_idx} saved to {final_scene_video_path} with duration {final_scene_video_timed.duration:.2f}s.")
    return final_scene_video_path


# In video_assembly.py

def assemble_final_reel(
    processed_scene_assets: List[Tuple[str, str, Dict[str, Any]]],
    config: ContentConfig,
    output_filename: str = "final_reel.mp4"
) -> Optional[str]:
    """Creates the final video reel by combining multiple scene videos with audio and text overlays.
    
    This function takes processed scene assets (video, audio, and narration info) and combines them
    into a final video reel. It handles video resizing, cropping, audio synchronization, and text
    overlay placement. The function ensures proper resource management and cleanup.
    
    Args:
        processed_scene_assets (List[Tuple[str, str, Dict[str, Any]]]): List of tuples containing:
            - scene_video_path: Path to the scene video file
            - scene_audio_path: Path to the scene audio file
            - narration_info: Dictionary containing narration text and duration
        config (ContentConfig): Configuration object containing video settings
        output_filename (str, optional): Name for the final output file. Defaults to "final_reel.mp4"
        
    Returns:
        Optional[str]: Path to the final assembled video file. Returns None if assembly fails.
        
    Features:
        - Combines video, audio, and text captions for each scene
        - Handles video resizing and cropping to target resolution
        - Manages audio synchronization
        - Adds text overlays with proper positioning
        - Implements comprehensive resource cleanup
        
    Note:
        - Requires proper font file for text overlays
        - Handles memory efficiently through proper resource cleanup
        - Provides error handling and fallback mechanisms
    """
    print("Assembling final reel...")
    if not processed_scene_assets:
        print("No processed scene assets to assemble. Final video cannot be created.")
        return None

    print(f"config.add_narration_text_to_video: {config.add_narration_text_to_video}")
    
    final_scene_video_clips = [] # Renamed from final_scene_clips_for_reel for clarity
    
    # This list will store all clips that are loaded or created
    # and should be closed in the finally block.
    all_clips_to_close = []

    font_path_for_textclip = "/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf"
    if not os.path.exists(font_path_for_textclip):
        print(f"Warning: Font file not found at {font_path_for_textclip}. TextClip will use a default font.")
        font_path_for_textclip = "Liberation-Sans-Bold" # Or "Arial" or None

    for i, (scene_video_path, scene_audio_path, narration_info) in enumerate(processed_scene_assets):
        video_clip_for_scene = None
        audio_clip_for_scene = None
        text_clip_for_scene = None
        background_clip_for_scene = None
        
        try:
            narration_text = narration_info["text"]
            actual_audio_duration = narration_info["duration"] # This is the target duration for this scene

            if not (scene_video_path and os.path.exists(scene_video_path) and \
                    scene_audio_path and os.path.exists(scene_audio_path)):
                print(f"Skipping scene {i} due to missing media files.")
                continue

            # Load video and audio clips
            video_clip_for_scene = VideoFileClip(scene_video_path)
            audio_clip_for_scene = AudioFileClip(scene_audio_path)
            all_clips_to_close.extend([video_clip_for_scene, audio_clip_for_scene])

            video_duration = video_clip_for_scene.duration
            
            # --- Video Duration Matching (using subclipped and concatenate_videoclips for loop) ---
            # First, resize and crop to final shape before timing adjustments IF POSSIBLE,
            # or do timing first. Let's stick to your old code's order:
            # Resize, Crop, Position, THEN Time, then Audio.

            # 1. Resize video to target height
            temp_video_clip = video_clip_for_scene.resized(height=config.final_output_resolution[1])

            # 2. Crop if wider than target width, or pad if narrower
            if temp_video_clip.w > config.final_output_resolution[0]:
                # Using .cropped() as per your working old code
                temp_video_clip = temp_video_clip.cropped(x_center=temp_video_clip.w / 2,
                                                          width=config.final_output_resolution[0])
            elif temp_video_clip.w < config.final_output_resolution[0]:
                # Pad with a background
                background_clip_for_scene = ColorClip(size=config.final_output_resolution,
                                           color=(0,0,0), # Black background
                                           duration=actual_audio_duration) # Duration for background
                all_clips_to_close.append(background_clip_for_scene)
                # Composite video onto background
                temp_video_clip = CompositeVideoClip([background_clip_for_scene, temp_video_clip.with_position('center')],
                                                     size=config.final_output_resolution)
            
            # 3. Position video in center (if not already handled by padding composite)
            # The .with_position('center') might have been applied already if padded.
            # If not padded, apply it now.
            if not (video_clip_for_scene.w < config.final_output_resolution[0] and temp_video_clip.w == config.final_output_resolution[0]):
                 temp_video_clip = temp_video_clip.with_position('center')

            # 4. Handle duration mismatches for the video
            if video_duration > actual_audio_duration: # If original video was longer
                video_clip_timed = temp_video_clip.subclipped(0, actual_audio_duration)
            elif video_duration < actual_audio_duration: # If original video was shorter, loop it
                # Note: we loop the `temp_video_clip` which is already resized/cropped/positioned
                num_loops = math.ceil(actual_audio_duration / video_duration) # Loop based on original duration
                if num_loops == 0 : num_loops = 1 # Ensure at least one instance
                # Create a list of the clip to be looped
                looped_video_parts = [temp_video_clip] * num_loops
                video_clip_concatenated_for_loop = concatenate_videoclips(looped_video_parts)
                all_clips_to_close.append(video_clip_concatenated_for_loop) # This new clip needs closing
                video_clip_timed = video_clip_concatenated_for_loop.subclipped(0, actual_audio_duration)
            else: # Durations match closely enough
                video_clip_timed = temp_video_clip # temp_video_clip is already at its full duration here

            final_audio_for_scene = audio_clip_for_scene # Start with the loaded audio
            if final_audio_for_scene.duration > actual_audio_duration:
                final_audio_for_scene = final_audio_for_scene.subclipped(0, actual_audio_duration)
            elif final_audio_for_scene.duration < actual_audio_duration:
                silence_needed = actual_audio_duration - final_audio_for_scene.duration
                if silence_needed > 0.01: # Only add if significant
                    silence_clip = AudioClip(frame_function=lambda t: 0, duration=silence_needed)
                    all_clips_to_close.append(silence_clip)
                    final_audio_for_scene = concatenate_audioclips([final_audio_for_scene, silence_clip])


            # 5. Combine video and audio
            video_clip_with_audio = video_clip_timed.with_audio(final_audio_for_scene)
            
            # This list will hold the video clip, and conditionally, the text clip.
            clips_for_composition = [video_clip_with_audio]

            # 6. Add text caption (if enabled in config)
            if config.add_narration_text_to_video:
                print(f"Adding narration text for scene {i}...")
                # Calculate font size based on video height (e.g., 5% of height)
                base_font_size = int(config.final_output_resolution[1] * 0.05)  # 5% of height
                font_size = max(40, min(base_font_size, 60))  # Between 40 and 60

                text_width = int(config.final_output_resolution[0] * 0.8)
                aspect_ratio = config.final_output_resolution[0] / config.final_output_resolution[1]
                vertical_position = 0.7 if aspect_ratio < 1 else 0.75

                # --- THIS IS THE FIX: Reverted to the original working syntax ---
                text_clip_for_scene = TextClip(
                    font_path_for_textclip,
                    text=narration_text,
                    font_size=font_size,
                    color='white',
                    stroke_color='black',
                    stroke_width=2,
                    method='caption',
                    size=(text_width, None)
                )
                all_clips_to_close.append(text_clip_for_scene)

                text_clip_final = text_clip_for_scene.with_position(('center', vertical_position), relative=True).with_duration(actual_audio_duration)
                
                clips_for_composition.append(text_clip_final)
            else:
                print(f"Skipping narration text for scene {i} as per config.")


            # 7. Combine video and (optional) text into final scene composite
            scene_composite = CompositeVideoClip(
                clips_for_composition,
                size=config.final_output_resolution # Ensure composite is target size
            )
            final_scene_video_clips.append(scene_composite)

        except Exception as e_scene:
            print(f"Error processing scene {i}: {e_scene}")
            import traceback
            traceback.print_exc()
            # Any clips opened in this iteration (video_clip_for_scene, etc.) are already in all_clips_to_close
            continue

    if not final_scene_video_clips:
        print("No scenes were successfully composed.")
        # Close any clips that might have been opened
        for clip_obj in all_clips_to_close:
            if hasattr(clip_obj, 'close') and callable(getattr(clip_obj, 'close')):
                try: clip_obj.close()
                except: pass # Ignore errors during cleanup after failure
        return None

    final_video_output_clip = None
    final_video_path = os.path.join(config.output_dir, output_filename)
    try:
        final_video_output_clip = concatenate_videoclips(final_scene_video_clips, method="compose")
        all_clips_to_close.append(final_video_output_clip) # Add final concatenated clip for closing

        final_video_output_clip.write_videofile(
            final_video_path,
            fps=config.fps,
            codec="libx264",
            audio_codec="aac",
            threads=4,
            preset="medium", # "ultrafast" for speed, "medium" for balance
            logger='bar'
        )
    except Exception as e_write:
        print(f"Error during final video writing: {e_write}")
        import traceback
        traceback.print_exc()
        final_video_path = None # Indicate failure
    finally:
        # Close all clips.
        # `final_scene_video_clips` contains CompositeVideoClips that are sources for `final_video_output_clip`.
        # Closing `final_video_output_clip` should ideally handle its sources if method='compose'.
        # `all_clips_to_close` contains initial VideoFileClips, AudioFileClips, created ColorClips, TextClips,
        # and potentially intermediate concatenated clips.
        
        # Make a set of unique clip objects to close to avoid issues with multiple references
        # to the same underlying resources.
        clips_to_actually_close = {id(c): c for c in all_clips_to_close if c}.values()
        
        for clip_obj in clips_to_actually_close:
            if hasattr(clip_obj, 'close') and callable(getattr(clip_obj, 'close')):
                try:
                    clip_obj.close()
                except Exception as e_close:
                    # print(f"Error closing a clip {type(clip_obj)}: {e_close}") # Can be noisy
                    pass
        
        # Also ensure the list of scene composites themselves are closed, as they are also clips
        for scene_comp in final_scene_video_clips:
            if hasattr(scene_comp, 'close') and callable(getattr(scene_comp, 'close')):
                try: scene_comp.close()
                except: pass


    if final_video_path:
        print(f"Final reel saved to {final_video_path}")
    return final_video_path



==== llm_modules/__init__.py ====
from .llm_zephyr import ZephyrLLM



==== llm_modules/llm_zephyr.py ====
# llm_modules/llm_zephyr.py
import torch
import json
import re
from typing import List, Optional, Tuple, Dict, Any
from transformers import AutoModelForCausalLM, AutoTokenizer

from base_modules import BaseLLM, BaseModuleConfig, ModuleCapabilities
from config_manager import ContentConfig, DEVICE, clear_vram_globally

class ZephyrLLMConfig(BaseModuleConfig):
    model_id: str = "HuggingFaceH4/zephyr-7b-beta"
    max_new_tokens_script: int = 2048 # Increased for new fields
    max_new_tokens_shot_prompt: int = 256
    temperature: float = 0.7
    top_k: int = 50
    top_p: float = 0.95

class ZephyrLLM(BaseLLM):
    Config = ZephyrLLMConfig

    @classmethod
    def get_capabilities(cls) -> ModuleCapabilities:
        return ModuleCapabilities(
            title="Zephyr 7B",
            vram_gb_min=8.0,
            ram_gb_min=16.0,
            # LLM-specific capabilities are not the main focus, so we use defaults.
        )
    
    def _load_model_and_tokenizer(self):
        if self.model is None or self.tokenizer is None:
            print(f"Loading LLM: {self.config.model_id}...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_id)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            try:
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.config.model_id, torch_dtype=torch.float16
                ).to(DEVICE)
            except Exception as e:
                print(f"Failed to load LLM with device_map='auto' ({e}), trying with explicit device: {DEVICE}")
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.config.model_id, torch_dtype=torch.float16
                ).to(DEVICE)
            print("LLM loaded.")

    def clear_vram(self):
        print("Clearing LLM VRAM...")
        models_to_clear = [m for m in [self.model] if m is not None]
        if models_to_clear: clear_vram_globally(*models_to_clear)
        self.model, self.tokenizer = None, None
        print("LLM VRAM cleared.")

    def _parse_llm_json_response(self, decoded_output: str, context: str = "script") -> Optional[Dict]:
        match = re.search(r'\{[\s\S]*\}', decoded_output)
        json_text = match.group(0) if match else decoded_output
        try:
            return json.loads(re.sub(r',(\s*[}\]])', r'\1', json_text))
        except json.JSONDecodeError as e:
            print(f"Error parsing LLM JSON for {context}: {e}. Raw output:\n{decoded_output}")
            return None

    def generate_script(self, topic: str, content_config: ContentConfig) -> Dict[str, Any]:
        self._load_model_and_tokenizer()
        print(f"Generating script for topic: '{topic}' in language: {content_config.language}")
        
        # --- MODIFICATION START ---
        # Map language code to full name for better prompting
        language_map = {
            'en': 'English', 'es': 'Spanish', 'fr': 'French',
            'de': 'German', 'it': 'Italian', 'pt': 'Portuguese',
            'pl': 'Polish', 'tr': 'Turkish', 'ru': 'Russian',
            'nl': 'Dutch', 'cs': 'Czech', 'ar': 'Arabic',
            'zh-cn': 'Chinese (Simplified)', 'ja': 'Japanese',
            'hu': 'Hungarian', 'ko': 'Korean', 'hi': 'Hindi'
        }
        target_language = language_map.get(content_config.language, 'English')

        system_prompt = (
            "You are a multilingual AI assistant creating content for a short video. "
            "You will be asked to write the narration in a specific language, but all other content (visual prompts, descriptions, hashtags) must be in English for the video generation models. "
            "Your response must be a single, valid JSON object with these exact keys: "
            "\"main_subject_description\", \"setting_description\", \"narration\", \"visuals\", \"hashtags\"."
        )
        
        user_prompt = f"""
        **IMPORTANT INSTRUCTIONS:**
        1.  The **"narration"** text MUST be written in **{target_language}**. Use the native script if applicable (e.g., Devanagari for Hindi).
        2.  Use proper punctuation (like commas and periods) in the narration for a natural-sounding voiceover.
        3.  All other fields ("main_subject_description", "setting_description", "visuals", "hashtags") MUST remain in **English**.

        ---
        Create content for a short video about "{topic}".
        The total narration should be ~{content_config.target_video_length_hint}s, with {content_config.min_scenes} to {content_config.max_scenes} scenes.
        Each scene's narration should be ~{content_config.max_scene_narration_duration_hint}s.
        
        Return your response in this exact JSON format:
        {{
            "main_subject_description": "A detailed, consistent description of the main character or subject (e.g., 'Fluffy, a chubby but cute orange tabby cat with green eyes'). MUST BE IN ENGLISH.",
            "setting_description": "A description of the primary environment (e.g., 'a cozy, sunlit living room with plush furniture'). MUST BE IN ENGLISH.",
            "narration": [
                {{"scene": 1, "text": "First scene narration text, written in {target_language}.", "duration_estimate": {content_config.max_scene_narration_duration_hint}}}
            ],
            "visuals": [
                {{"scene": 1, "prompt": "Detailed visual prompt for scene 1. MUST BE IN ENGLISH."}}
            ],
            "hashtags": ["relevantTag1", "relevantTag2"]
        }}
        """
        # --- MODIFICATION END ---
        
        messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]

        for attempt in range(3):
            print(f"Attempt {attempt + 1} of 3 to generate valid script JSON...")
            
            tokenized_chat = self.tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt").to(self.model.device)
            outputs = self.model.generate(
                input_ids=tokenized_chat, max_new_tokens=self.config.max_new_tokens_script, 
                do_sample=True, top_k=self.config.top_k, top_p=self.config.top_p, 
                temperature=self.config.temperature, pad_token_id=self.tokenizer.eos_token_id
            )
            decoded_output = self.tokenizer.decode(outputs[0][tokenized_chat.shape[-1]:], skip_special_tokens=True)
            response_data = self._parse_llm_json_response(decoded_output, "script")

            if response_data and all(k in response_data for k in ["narration", "visuals", "main_subject_description"]):
                print("Successfully generated and parsed valid script JSON.")
                return {
                    "main_subject_description": response_data.get("main_subject_description"),
                    "setting_description": response_data.get("setting_description"),
                    "narration": sorted(response_data.get("narration", []), key=lambda x: x["scene"]),
                    "visuals": [p["prompt"] for p in sorted(response_data.get("visuals", []), key=lambda x: x["scene"])],
                    "hashtags": response_data.get("hashtags", [])
                }
            else:
                print(f"Attempt {attempt + 1} failed. The response was not a valid JSON or was missing required keys.")
                if attempt < 2:
                    print("Retrying...")
        
        print("LLM script generation failed after 3 attempts. Using fallback.")
        # Fallback remains in English as a safe default
        return {
            "main_subject_description": topic, "setting_description": "a simple background",
            "narration": [{"text": f"An intro to {topic}.", "duration_estimate": 5.0}],
            "visuals": [f"Cinematic overview of {topic}."], "hashtags": [f"#{topic.replace(' ', '')}"]
        }

    def generate_shot_visual_prompts(self, scene_narration: str, original_scene_prompt: str, num_shots: int, content_config: ContentConfig, main_subject: str, setting: str) -> List[Tuple[str, str]]:
        self._load_model_and_tokenizer()
        shot_prompts = []
        
        # Define the prompts, which are the same for each shot generation call
        system_prompt = (
            "You are an Movie director. Your task is to generate a 'visual_prompt' and a 'motion_prompt' for a short video shot "
            "The prompts MUST incorporate the provided main subject and setting. Do NOT change the subject. "
            "Respond in this exact JSON format: {\"visual_prompt\": \"...\", \"motion_prompt\": \"...\"}"
        )

        for shot_idx in range(num_shots):
            print(f"--- Generating prompts for Shot {shot_idx + 1}/{num_shots} ---")
            
            # --- NEW: Defensive check to prevent intermittent crashes ---
            # This handles rare cases where the model/tokenizer might be cleared from memory
            # between calls within the same task execution.
            if self.model is None or self.tokenizer is None:
                print("WARNING: LLM was unloaded unexpectedly. Forcing a reload before generating shot prompt.")
                self._load_model_and_tokenizer()

            user_prompt = f"""
            **Main Subject (MUST BE INCLUDED):** {main_subject}
            **Setting (MUST BE INCLUDED):** {setting}
            
            ---
            **Original Scene Goal:** "{original_scene_prompt}"
            **This Shot's Narration:** "{scene_narration}"
            
            Based on ALL the information above, create a visual and motion prompt for shot {shot_idx + 1}/{num_shots}.
            The visual prompt should be a specific, detailed moment consistent with the subject and setting.
            try to describe the visual prompt in minimum words but in very specific details what a director would want  the image to look like.
            Descrive character, subject and envrionment in words, only chose important words no need to make complete sentances.
            try to describe the visual prompt in minimum words but in very specific details what a director would want  the image to look like.
            Descrive character, subject and envrionment in words, only chose important words no need to make complete sentances.
            Also descirbe camera mm, shot type, location, lighting, color, mood, etc.
            Do not include any other text or comments other then given json format.
            """
            messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
            
            visual_prompt, motion_prompt = None, None

            # --- MODIFICATION START: Add retry loop for each shot ---
            for attempt in range(3):
                print(f"Attempt {attempt + 1} of 3 to generate valid prompt JSON for shot {shot_idx + 1}...")
                
                tokenized_chat = self.tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt").to(self.model.device)
                outputs = self.model.generate(
                    input_ids=tokenized_chat, max_new_tokens=self.config.max_new_tokens_shot_prompt, 
                    do_sample=True, temperature=self.config.temperature, pad_token_id=self.tokenizer.eos_token_id
                )
                decoded_output = self.tokenizer.decode(outputs[0][tokenized_chat.shape[-1]:], skip_special_tokens=True)
                response_data = self._parse_llm_json_response(decoded_output, f"shot {shot_idx+1} prompt")

                # Check for a dictionary with both required string keys
                if (isinstance(response_data, dict) and 
                    isinstance(response_data.get("visual_prompt"), str) and 
                    isinstance(response_data.get("motion_prompt"), str)):
                    
                    visual_prompt = response_data["visual_prompt"]
                    motion_prompt = response_data["motion_prompt"]
                    print(f"Successfully generated and parsed prompts for shot {shot_idx + 1}.")
                    break  # Exit the retry loop on success
                else:
                    print(f"Attempt {attempt + 1} failed for shot {shot_idx + 1}. Invalid JSON or missing keys.")
            # --- MODIFICATION END ---

            # If after 3 attempts, we still don't have prompts, use the fallback
            if not visual_prompt or not motion_prompt:
                print(f"All attempts failed for shot {shot_idx + 1}. Using fallback prompts.")
                visual_prompt = f"{main_subject} in {setting}, {original_scene_prompt}"
                motion_prompt = "gentle camera movement"

            shot_prompts.append((visual_prompt, motion_prompt))
            print(f"  > Shot {shot_idx+1} Visual: \"{visual_prompt[:80]}...\"")
            print(f"  > Shot {shot_idx+1} Motion: \"{motion_prompt[:80]}...\"")
        
        return shot_prompts



==== tts_modules/__init__.py ====
from .tts_coqui import CoquiTTSModule



==== tts_modules/tts_coqui.py ====
# tts_modules/tts_coqui.py
import os
import torch
import numpy as np
from typing import Tuple, Optional
from TTS.api import TTS as CoquiTTS
from moviepy import AudioFileClip
from scipy.io import wavfile

from base_modules import BaseTTS, BaseModuleConfig, ModuleCapabilities
from config_manager import DEVICE, clear_vram_globally

class CoquiTTSConfig(BaseModuleConfig):
    model_id: str = "tts_models/multilingual/multi-dataset/xtts_v2"

class CoquiTTSModule(BaseTTS):
    Config = CoquiTTSConfig

    @classmethod
    def get_capabilities(cls) -> ModuleCapabilities:
        return ModuleCapabilities(
            title="XTTS, Multi-Language, Documentary Style",
            vram_gb_min=2.0, # XTTS is relatively lightweight
            ram_gb_min=8.0,
            supported_tts_languages=["en", "es", "fr", "de", "it", "pt", "pl", "tr", "ru", "nl", "cs", "ar", "zh-cn", "ja", "hu", "ko", "hi"]
        )

    def _load_model(self):
        if self.model is None:
            print(f"Loading TTS model: {self.config.model_id}...")
            self.model = CoquiTTS(model_name=self.config.model_id, progress_bar=True).to(DEVICE)
            print("TTS model loaded.")
    
    def clear_vram(self):
        print("Clearing TTS VRAM...")
        if self.model is not None:
            clear_vram_globally(self.model)
        self.model = None
        print("TTS VRAM cleared.")

    def generate_audio(
        self, text: str, output_dir: str, scene_idx: int, language: str, speaker_wav: Optional[str] = None
    ) -> Tuple[str, float]:
        self._load_model()
        
        print(f"Generating audio in {language} for scene {scene_idx}: \"{text[:50]}...\"")
        output_path = os.path.join(output_dir, f"scene_{scene_idx}_audio.wav")
        
        tts_kwargs = {"language": language, "file_path": output_path}
        
        if "xtts" in self.config.model_id.lower():
            if speaker_wav and os.path.exists(speaker_wav):
                tts_kwargs["speaker_wav"] = speaker_wav
            else:
                if speaker_wav: print(f"Warning: Speaker WAV {speaker_wav} not found. XTTS using default voice.")
        
        self.model.tts_to_file(text, **tts_kwargs)
        
        duration = 0.0
        try:
            if os.path.exists(output_path) and os.path.getsize(output_path) > 0:
                with AudioFileClip(output_path) as audio_clip:
                    duration = audio_clip.duration + 0.1 # Small buffer
            else: raise ValueError("Audio file not generated or is empty.")
        except Exception as e:
            print(f"Error getting duration for {output_path}: {e}. Creating fallback.")
            samplerate = 22050 
            wavfile.write(output_path, samplerate, np.zeros(int(0.1 * samplerate), dtype=np.int16))
            duration = 0.1

        print(f"Actual audio duration for scene {scene_idx}: {duration:.2f}s")
        return output_path, duration



==== t2i_modules/__init__.py ====
from .t2i_juggernaut import JuggernautT2I
from .t2i_sdxl import SdxlT2I



==== t2i_modules/t2i_juggernaut.py ====
# In t2i_modules/t2i_juggernaut.py
import torch
from typing import List, Optional, Dict, Any, Union
from diffusers import StableDiffusionXLPipeline, DiffusionPipeline
from diffusers.utils import load_image
from transformers import BitsAndBytesConfig
from diffusers import DPMSolverMultistepScheduler as JuggernautScheduler

from base_modules import BaseT2I, BaseModuleConfig, ModuleCapabilities
from config_manager import DEVICE, clear_vram_globally

class JuggernautT2IConfig(BaseModuleConfig):
    model_id: str = "RunDiffusion/Juggernaut-XL-v9"
    refiner_id: Optional[str] = None
    # --- NEW: Flag to control memory-saving quantization ---
    use_8bit_quantization: bool = True
    num_inference_steps: int = 35
    guidance_scale: float = 6.0 
    ip_adapter_repo: str = "h94/IP-Adapter"
    ip_adapter_subfolder: str = "sdxl_models"
    ip_adapter_weight_name: str = "ip-adapter_sdxl.bin"


class JuggernautT2I(BaseT2I):
    Config = JuggernautT2IConfig

    def __init__(self, config: JuggernautT2IConfig):
        super().__init__(config)
        self.refiner_pipe = None
        self._loaded_ip_adapter_count = 0
    
    @classmethod
    def get_capabilities(cls) -> ModuleCapabilities:
        return ModuleCapabilities(
            title="Juggernaut XL v9 (Quality), 2 Subjects considered",
            vram_gb_min=8.0,
            ram_gb_min=12.0,
            supported_formats=["Portrait", "Landscape"],
            supports_ip_adapter=True,
            supports_lora=True,
            max_subjects=2,
            accepts_text_prompt=True,
            accepts_negative_prompt=True
        )

    @classmethod
    def get_model_capabilities(self) -> Dict[str, Any]:
        return {
            "resolutions": {"Portrait": (832, 1216), "Landscape": (1216, 832)},
            "max_shot_duration": 3.0 
        }

    def enhance_prompt(self, prompt: str, prompt_type: str = "visual") -> str:
        quality_keywords = "cinematic photography, hyperdetailed, (skin details:1.1), 8k, professional lighting"
        if prompt.strip().endswith(','):
            return f"{prompt} {quality_keywords}"
        else:
            return f"{prompt}, {quality_keywords}"

    def _load_pipeline(self):
        if self.pipe is None:
            if self.config.use_8bit_quantization:
                print("Loading T2I pipeline (Juggernaut) with 8-bit quantization to save VRAM...")
                bnb_config = BitsAndBytesConfig(
                    load_in_8bit=True,
                )
                # --- START OF FIX: Remove device_map and use .to(DEVICE) instead ---
                # This prevents the accelerate hook conflict when loading IP-Adapters later.
                self.pipe = StableDiffusionXLPipeline.from_pretrained(
                    self.config.model_id,
                    quantization_config=bnb_config,
                    torch_dtype=torch.float16,
                    variant="fp16",
                    use_safetensors=True,
                ).to(DEVICE)
                # --- END OF FIX ---
            else:
                print(f"Loading T2I pipeline (Juggernaut) in full precision to {DEVICE}...")
                self.pipe = StableDiffusionXLPipeline.from_pretrained(
                    self.config.model_id, torch_dtype=torch.float16, variant="fp16", use_safetensors=True
                ).to(DEVICE)
            
            self.pipe.scheduler = JuggernautScheduler.from_config(self.pipe.scheduler.config, use_karras_sigmas=True)
            print(f"Juggernaut pipeline configured with {JuggernautScheduler.__name__} sampler.")

            if self.config.refiner_id:
                print(f"Refiner specified but not typically used with Juggernaut, skipping load.")

    def clear_vram(self):
        print("Clearing T2I (Juggernaut) VRAM...")
        models = [m for m in [self.pipe, self.refiner_pipe] if m is not None]
        if models: clear_vram_globally(*models)
        self.pipe, self.refiner_pipe = None, None
        self._loaded_ip_adapter_count = 0
        print("T2I (Juggernaut) VRAM cleared.")

    def generate_image(self, prompt: str, negative_prompt: str, output_path: str, width: int, height: int, ip_adapter_image: Optional[Union[str, List[str]]] = None, seed: int = -1) -> str:
        self._load_pipeline()
        
        generator = None
        if seed != -1:
            print(f"Using fixed seed for generation: {seed}")
            generator = torch.Generator(device=self.pipe.device).manual_seed(seed)
        else:
            print("Using random seed for generation.")

        pipeline_kwargs = {"generator": generator} if generator else {}
        ip_images_to_load = []

        if ip_adapter_image:
            if isinstance(ip_adapter_image, str):
                ip_images_to_load = [ip_adapter_image]
            else:
                ip_images_to_load = ip_adapter_image
        
        num_ip_images = len(ip_images_to_load)

        if num_ip_images > 0:
            print(f"Juggernaut T2I: Activating IP-Adapter with {num_ip_images} character image(s).")
            if self._loaded_ip_adapter_count != num_ip_images:
                print(f"Loading {num_ip_images} IP-Adapter(s) for the pipeline...")
                if hasattr(self.pipe, "unload_ip_adapter"): self.pipe.unload_ip_adapter()
                adapter_weights = [self.config.ip_adapter_weight_name] * num_ip_images
                self.pipe.load_ip_adapter(
                    self.config.ip_adapter_repo, 
                    subfolder=self.config.ip_adapter_subfolder, 
                    weight_name=adapter_weights
                )
                self._loaded_ip_adapter_count = num_ip_images
                print(f"Successfully loaded {self._loaded_ip_adapter_count} adapters.")
            
            scales = [0.6] * num_ip_images
            self.pipe.set_ip_adapter_scale(scales) 
            ip_images = [load_image(p) for p in ip_images_to_load]
            pipeline_kwargs["ip_adapter_image"] = ip_images
        else:
            print("Juggernaut T2I: No IP-Adapter image provided.")
            if self._loaded_ip_adapter_count > 0:
                 if hasattr(self.pipe, "unload_ip_adapter"): self.pipe.unload_ip_adapter()
                 self._loaded_ip_adapter_count = 0

        enhanced_prompt = self.enhance_prompt(prompt)
        print(f"Juggernaut generating image with resolution: {width}x{height}")
        print(f"  - Prompt: '{enhanced_prompt}'")
        print(f"  - Negative: '{negative_prompt}'")

        image = self.pipe(
            prompt=enhanced_prompt,
            negative_prompt=negative_prompt,
            width=width,
            height=height,
            num_inference_steps=self.config.num_inference_steps,
            guidance_scale=self.config.guidance_scale,
            **pipeline_kwargs
        ).images[0]
        
        image.save(output_path)
        print(f"Image saved to {output_path}")
        return output_path



==== t2i_modules/t2i_sdxl.py ====
# t2i_modules/t2i_sdxl.py
import torch
from typing import List, Optional, Dict, Any, Union
from diffusers import StableDiffusionXLPipeline, DiffusionPipeline
from diffusers.utils import load_image

from base_modules import BaseT2I, BaseModuleConfig, ModuleCapabilities
from config_manager import DEVICE, clear_vram_globally

class SdxlT2IConfig(BaseModuleConfig):
    model_id: str = "stabilityai/stable-diffusion-xl-base-1.0"
    refiner_id: Optional[str] = "stabilityai/stable-diffusion-xl-refiner-1.0"
    num_inference_steps: int = 30
    guidance_scale: float = 7.5
    base_denoising_end: float = 0.8
    refiner_denoising_start: float = 0.8

class SdxlT2I(BaseT2I):
    Config = SdxlT2IConfig
    
    def __init__(self, config: SdxlT2IConfig):
        super().__init__(config)
        self.refiner_pipe = None

    @classmethod
    def get_capabilities(cls) -> ModuleCapabilities:
        return ModuleCapabilities(
            title="SDXL + Refiner (High VRAM): No Subjects considered",
            vram_gb_min=10.0, # SDXL with refiner is heavy
            ram_gb_min=16.0,
            supported_formats=["Portrait", "Landscape"],
            # Even if we don't implement IP-Adapter here, we declare support
            # because the pipeline is capable. A more advanced version could add it.
            supports_ip_adapter=True,
            supports_lora=True,
            max_subjects=2,
            accepts_text_prompt=True,
            accepts_negative_prompt=True
        )

    @classmethod
    def get_model_capabilities(self) -> Dict[str, Any]:
        return {
            "resolutions": {"Portrait": (896, 1152), "Landscape": (1344, 768)},
            "max_shot_duration": 3.0
        }

    def _load_pipeline(self):
        if self.pipe is None:
            print(f"Loading T2I pipeline (SDXL): {self.config.model_id}...")
            self.pipe = StableDiffusionXLPipeline.from_pretrained(
                self.config.model_id, torch_dtype=torch.float16, variant="fp16", use_safetensors=True
            ).to(DEVICE)
            print("SDXL Base pipeline loaded.")
            if self.config.refiner_id:
                print(f"Loading T2I Refiner pipeline: {self.config.refiner_id}...")
                self.refiner_pipe = DiffusionPipeline.from_pretrained(
                    self.config.refiner_id, text_encoder_2=self.pipe.text_encoder_2,
                    vae=self.pipe.vae, torch_dtype=torch.float16,
                    use_safetensors=True, variant="fp16"
                ).to(DEVICE)
                print("SDXL Refiner pipeline loaded.")

    def clear_vram(self):
        print("Clearing T2I (SDXL) VRAM...")
        models = [m for m in [self.pipe, self.refiner_pipe] if m is not None]
        if models: clear_vram_globally(*models)
        self.pipe, self.refiner_pipe = None, None
        print("T2I (SDXL) VRAM cleared.")

    # --- START OF FIX: Updated method signature and implementation ---
    def generate_image(self, prompt: str, negative_prompt: str, output_path: str, width: int, height: int, ip_adapter_image: Optional[Union[str, List[str]]] = None, seed: int = -1) -> str:
        self._load_pipeline()

        if ip_adapter_image:
            print("Warning: SDXLT2I module received IP-Adapter image but does not currently implement its use.")
        
        generator = None
        if seed != -1:
            print(f"Using fixed seed for generation: {seed}")
            # Ensure the generator is on the same device as the pipeline
            generator = torch.Generator(device=self.pipe.device).manual_seed(seed)
        else:
            print("Using random seed for generation.")

        kwargs = {
            "prompt": prompt,
            "negative_prompt": negative_prompt, # Now passing this argument
            "width": width, "height": height,
            "num_inference_steps": self.config.num_inference_steps,
            "guidance_scale": self.config.guidance_scale,
            "generator": generator # Now passing the generator
        }
        if self.refiner_pipe:
            kwargs["output_type"] = "latent"
            kwargs["denoising_end"] = self.config.base_denoising_end

        image = self.pipe(**kwargs).images[0]
        
        if self.refiner_pipe:
            print("Refining image...")
            refiner_kwargs = {
                "prompt": prompt,
                "negative_prompt": negative_prompt,
                "image": image,
                "denoising_start": self.config.refiner_denoising_start,
                "num_inference_steps": self.config.num_inference_steps,
                "generator": generator
            }
            image = self.refiner_pipe(**refiner_kwargs).images[0]
        
        image.save(output_path)
        print(f"Image saved to {output_path}")
        return output_path
    # --- END OF FIX ---



==== i2v_modules/i2v_ltx.py ====
# i2v_modules/i2v_ltx.py
import torch
from typing import Dict, Any, List, Optional, Union
from diffusers import LTXImageToVideoPipeline
from diffusers.utils import export_to_video, load_image
from PIL import Image

from base_modules import BaseI2V, BaseModuleConfig, ModuleCapabilities
from config_manager import DEVICE, clear_vram_globally, ContentConfig

class LtxI2VConfig(BaseModuleConfig):
    model_id: str = "Lightricks/LTX-Video"
    num_inference_steps: int = 50
    guidance_scale: float = 7.5

class LtxI2V(BaseI2V):
    Config = LtxI2VConfig

    @classmethod
    def get_capabilities(cls) -> ModuleCapabilities:
        return ModuleCapabilities(
            title="LTX, 8bit Load, Port/LandScape, 2 Sub, Take +/- Prompts, max 4 sec",
            vram_gb_min=8.0,
            ram_gb_min=12.0,
            supported_formats=["Portrait", "Landscape"],
            supports_ip_adapter=True,
            supports_lora=True, # Juggernaut is a fine-tune, can easily use LoRAs
            max_subjects=2, # Can handle one or two IP adapter images
            accepts_text_prompt=True,
            accepts_negative_prompt=True
        )

    @classmethod
    def get_model_capabilities(self) -> Dict[str, Any]:
        return {
            "resolutions": {"Portrait": (480, 704), "Landscape": (704, 480)},
            "max_shot_duration": 4 
        }
    
    def enhance_prompt(self, prompt: str, prompt_type: str = "visual") -> str:
        # SVD doesn't use text prompts, but this shows how you could add model-specific keywords.
        # For example, for a different model you might do:
        if prompt_type == "visual":
           return f"{prompt}, 8k, photorealistic, cinematic lighting"
        return prompt # Return original for SVD

    def _load_pipeline(self):
        if self.pipe is None:
            print(f"Loading I2V pipeline (LTX): {self.config.model_id}...")
            self.pipe = LTXImageToVideoPipeline.from_pretrained(self.config.model_id, torch_dtype=torch.bfloat16)
            self.pipe.enable_model_cpu_offload()
            print("I2V (LTX) pipeline loaded.")

    def clear_vram(self):
        print("Clearing I2V (LTX) VRAM...")
        if self.pipe is not None: clear_vram_globally(self.pipe)
        self.pipe = None
        print("I2V (LTX) VRAM cleared.")

    def _resize_and_pad(self, image: Image.Image, target_width: int, target_height: int) -> Image.Image:
        original_aspect = image.width / image.height; target_aspect = target_width / target_height
        if original_aspect > target_aspect: new_width, new_height = target_width, int(target_width / original_aspect)
        else: new_height, new_width = target_height, int(target_height * original_aspect)
        resized_image = image.resize((new_width, new_height), Image.LANCZOS)
        background = Image.new('RGB', (target_width, target_height), (0, 0, 0))
        background.paste(resized_image, ((target_width - new_width) // 2, (target_height - new_height) // 2))
        return background

    def generate_video_from_image(self, image_path: str, output_video_path: str, target_duration: float, content_config: ContentConfig, visual_prompt: str, motion_prompt: Optional[str], ip_adapter_image: Optional[Union[str, List[str]]] = None) -> str:
        self._load_pipeline()
        
        input_image = load_image(image_path)
        target_res = self.get_model_capabilities()["resolutions"]
        aspect_ratio = "Landscape" if input_image.width > input_image.height else "Portrait"
        target_width, target_height = target_res[aspect_ratio]
        prepared_image = self._resize_and_pad(input_image, target_width, target_height)

        num_frames = max(16, int(target_duration * content_config.fps))
        full_prompt = f"{visual_prompt}, {motion_prompt}" if motion_prompt else visual_prompt

        # --- NEW LOGIC TO HANDLE ip_adapter_image ---
        # While LTX doesn't have a formal IP-Adapter, we can use the character
        # reference to guide the style by adding it to the prompt.
        if ip_adapter_image:
            print("LTX I2V: Using character reference to guide prompt style.")
            # For simplicity, we add a generic phrase. A more complex system could use an image-to-text model.
            full_prompt = f"in the style of the reference character, {full_prompt}"
            
        print(f"LTX I2V using prompt: {full_prompt}")
        
        video = self.pipe(
            prompt=full_prompt, image=prepared_image, width=target_width, height=target_height,
            num_frames=num_frames, num_inference_steps=self.config.num_inference_steps,
            guidance_scale=self.config.guidance_scale,
            negative_prompt="worst quality, inconsistent motion, blurry"
        ).frames[0]
        
        export_to_video(video, output_video_path, fps=content_config.fps)
        print(f"LTX video shot saved to {output_video_path}")
        return output_video_path



==== i2v_modules/i2v_slideshow.py ====
# In i2v_modules/i2v_slideshow.py
from typing import Dict, Any, List, Optional, Union
# --- THIS IS THE FIX: Importing ImageClip directly, matching the project's pattern ---
from moviepy.video.VideoClip import ImageClip

from base_modules import BaseI2V, BaseModuleConfig, ModuleCapabilities
from config_manager import ContentConfig

class SlideshowI2VConfig(BaseModuleConfig):
    # This module doesn't load a model, but the config is part of the contract.
    model_id: str = "moviepy_image_clip"

class SlideshowI2V(BaseI2V):
    Config = SlideshowI2VConfig

    @classmethod
    def get_capabilities(cls) -> ModuleCapabilities:
        """
        Defines the capabilities of this simple, non-AI module.
        It uses minimal resources and doesn't support AI-specific features.
        """
        return ModuleCapabilities(
            title="Slideshow (Static Image)",
            vram_gb_min=0.1,  # Uses virtually no VRAM
            ram_gb_min=1.0,   # Uses very little RAM
            supported_formats=["Portrait", "Landscape"],
            supports_ip_adapter=False, # Not an AI model
            supports_lora=False,       # Not an AI model
            max_subjects=0,
            accepts_text_prompt=False, # Ignores prompts
            accepts_negative_prompt=False
        )

    @classmethod
    def get_model_capabilities(self) -> Dict[str, Any]:
        """
        This module has no native resolution and can handle long durations.
        """
        return {
            # It can handle any resolution, as it just wraps the image.
            "resolutions": {"Portrait": (1080, 1920), "Landscape": (1920, 1080)},
            "max_shot_duration": 60.0 # Can be very long
        }

    def _load_pipeline(self):
        """No pipeline to load for this module."""
        print("SlideshowI2V: No pipeline to load.")
        pass

    def clear_vram(self):
        """No VRAM to clear for this module."""
        print("SlideshowI2V: No VRAM to clear.")
        pass

    def enhance_prompt(self, prompt: str, prompt_type: str = "visual") -> str:
        """This module ignores prompts, so no enhancement is needed."""
        return prompt

    def generate_video_from_image(self, image_path: str, output_video_path: str, target_duration: float, content_config: ContentConfig, visual_prompt: str, motion_prompt: Optional[str], ip_adapter_image: Optional[Union[str, List[str]]] = None) -> str:
        """
        Creates a video by holding a static image for the target duration.
        """
        print(f"SlideshowI2V: Creating static video for {target_duration:.2f}s from {image_path}")
        
        video_clip = None
        try:
            # Create a video clip from the static image and set its duration.
            video_clip = ImageClip(image_path).with_duration(target_duration)
            
            # Use the correct syntax for write_videofile, matching video_assembly.py
            video_clip.write_videofile(
                output_video_path, 
                fps=content_config.fps,
                codec="libx264", 
                audio=False, # This is a visual-only shot
                threads=4, 
                preset="medium",
                logger=None # Suppress verbose moviepy logs
            )
            
            print(f"Slideshow video shot saved to {output_video_path}")
            return output_video_path

        except Exception as e:
            print(f"Error creating slideshow video: {e}")
            return "" # Return empty string on failure
        finally:
            # Ensure the clip resources are released
            if video_clip:
                video_clip.close()



==== i2v_modules/i2v_svd.py ====
# i2v_modules/i2v_svd.py
import torch
from typing import Dict, Any, List, Optional, Union
from diffusers import StableVideoDiffusionPipeline
from diffusers.utils import load_image, export_to_video
from PIL import Image

from base_modules import BaseI2V, BaseModuleConfig, ModuleCapabilities
from config_manager import DEVICE, clear_vram_globally, ContentConfig

class SvdI2VConfig(BaseModuleConfig):
    model_id: str = "stabilityai/stable-video-diffusion-img2vid-xt"
    decode_chunk_size: int = 8
    motion_bucket_id: int = 127
    noise_aug_strength: float = 0.02
    model_native_frames: int = 25

class SvdI2V(BaseI2V):
    Config = SvdI2VConfig

    @classmethod
    def get_capabilities(cls) -> ModuleCapabilities:
        return ModuleCapabilities(
            title="SVD, Float16, Port/Landscape, No Prompt just image, Max 2 Sec",
            vram_gb_min=8.0,
            ram_gb_min=12.0,
            supported_formats=["Portrait", "Landscape"],
            supports_ip_adapter=True,
            supports_lora=True, # Juggernaut is a fine-tune, can easily use LoRAs
            max_subjects=2, # Can handle one or two IP adapter images
            accepts_text_prompt=False,
            accepts_negative_prompt=True
        )

    @classmethod
    def get_model_capabilities(self) -> Dict[str, Any]:
        return {
            "resolutions": {"Portrait": (576, 1024), "Landscape": (1024, 576)},
            "max_shot_duration": 2.0 
        }
        
    def enhance_prompt(self, prompt: str, prompt_type: str = "visual") -> str:
        # SVD doesn't use text prompts, but this shows how you could add model-specific keywords.
        # For example, for a different model you might do:
        # if prompt_type == "visual":
        #    return f"{prompt}, 8k, photorealistic, cinematic lighting"
        return prompt # Return original for SVD

    def _load_pipeline(self):
        if self.pipe is None:
            print(f"Loading I2V pipeline (SVD): {self.config.model_id}...")
            self.pipe = StableVideoDiffusionPipeline.from_pretrained(
                self.config.model_id, torch_dtype=torch.float16
            )
            self.pipe.enable_model_cpu_offload()
            print("I2V (SVD) pipeline loaded.")

    def clear_vram(self):
        print("Clearing I2V (SVD) VRAM...")
        if self.pipe is not None: clear_vram_globally(self.pipe)
        self.pipe = None
        print("I2V (SVD) VRAM cleared.")

    def _resize_and_pad(self, image: Image.Image, target_width: int, target_height: int) -> Image.Image:
        original_aspect = image.width / image.height; target_aspect = target_width / target_height
        if original_aspect > target_aspect: new_width, new_height = target_width, int(target_width / original_aspect)
        else: new_height, new_width = target_height, int(target_height * original_aspect)
        resized_image = image.resize((new_width, new_height), Image.LANCZOS)
        background = Image.new('RGB', (target_width, target_height), (0, 0, 0))
        background.paste(resized_image, ((target_width - new_width) // 2, (target_height - new_height) // 2))
        return background

    def generate_video_from_image(self, image_path: str, output_video_path: str, target_duration: float, content_config: ContentConfig, visual_prompt: str, motion_prompt: Optional[str], ip_adapter_image: Optional[Union[str, List[str]]] = None) -> str:
        self._load_pipeline()

        if ip_adapter_image:
            print("Warning: SvdI2V module received IP-Adapter image but does not currently implement its use.")

        input_image = load_image(image_path)
        svd_target_res = self.get_model_capabilities()["resolutions"]
        aspect_ratio = "Landscape" if input_image.width > input_image.height else "Portrait"
        svd_target_width, svd_target_height = svd_target_res[aspect_ratio]
        prepared_image = self._resize_and_pad(input_image, svd_target_width, svd_target_height)

        calculated_fps = max(1, round(self.config.model_native_frames / target_duration)) if target_duration > 0 else 8
        motion_bucket_id = self.config.motion_bucket_id
        if motion_prompt:
            motion_prompt_lower = motion_prompt.lower()
            if any(w in motion_prompt_lower for w in ['fast', 'quick', 'rapid', 'zoom in', 'pan right']): motion_bucket_id = min(255, motion_bucket_id + 50)
            elif any(w in motion_prompt_lower for w in ['slow', 'gentle', 'subtle', 'still']): motion_bucket_id = max(0, motion_bucket_id - 50)
            print(f"Adjusted motion_bucket_id to {motion_bucket_id} based on prompt: '{motion_prompt}'")

        video_frames = self.pipe(
            image=prepared_image, height=svd_target_height, width=svd_target_width,
            decode_chunk_size=self.config.decode_chunk_size, num_frames=self.config.model_native_frames,
            motion_bucket_id=motion_bucket_id, noise_aug_strength=self.config.noise_aug_strength,
        ).frames[0]

        export_to_video(video_frames, output_video_path, fps=calculated_fps)
        print(f"SVD video shot saved to {output_video_path}")
        return output_video_path



==== i2v_modules/i2v_wan.py ====
# In i2v_modules/i2v_wan.py
import torch
import numpy as np
from typing import Dict, Any, List, Optional, Union
from PIL import Image

# Import the necessary components
from diffusers import WanImageToVideoPipeline, AutoencoderKLWan
from diffusers.utils import export_to_video, load_image
from transformers import CLIPVisionModel, UMT5EncoderModel, T5Tokenizer, CLIPImageProcessor

from base_modules import BaseI2V, BaseModuleConfig, ModuleCapabilities
from config_manager import DEVICE, clear_vram_globally, ContentConfig

class WanI2VConfig(BaseModuleConfig):
    """Configuration for the Wan 2.1 I2V model."""
    model_id: str = "Wan-AI/Wan2.1-I2V-14B-480P-Diffusers"
    
    num_inference_steps: int = 30
    guidance_scale: float = 5.0

class WanI2V(BaseI2V):
    """
    Image-to-Video module using the Wan 2.1 14B pipeline.
    """
    Config = WanI2VConfig

    @classmethod
    def get_capabilities(cls) -> ModuleCapabilities:
        """Declare the capabilities of the Wan 2.1 I2V model."""
        return ModuleCapabilities(
            title="Wan 2.1 I2V (14B)",
            vram_gb_min=40.0,
            ram_gb_min=24.0,
            supported_formats=["Portrait", "Landscape"],
            supports_ip_adapter=False,
            supports_lora=False,
            max_subjects=0,
            accepts_text_prompt=True,
            accepts_negative_prompt=True
        )

    @classmethod
    def get_model_capabilities(self) -> Dict[str, Any]:
        """Return the specific resolutions and max duration for this model."""
        return {
            "resolutions": {"base_pixel_area": 399360},  # 480P model base area
            "max_shot_duration": 4.0
        }

    def _load_pipeline(self):
        """
        Loads the WanImageToVideoPipeline following the official documentation example.
        """
        if self.pipe is not None: return

        print(f"Loading I2V pipeline ({self.config.model_id})...")

        # 1. Load individual components with appropriate dtypes
        image_encoder = CLIPVisionModel.from_pretrained(
            self.config.model_id, 
            subfolder="image_encoder", 
            torch_dtype=torch.float32
        )

        vae = AutoencoderKLWan.from_pretrained(
            self.config.model_id, 
            subfolder="vae", 
            torch_dtype=torch.float32
        )

        # 2. Create the pipeline with the components
        self.pipe = WanImageToVideoPipeline.from_pretrained(
            self.config.model_id,
            vae=vae,
            image_encoder=image_encoder,
            torch_dtype=torch.bfloat16
        )

        # 3. Enable model CPU offload for memory efficienc                                                                                                                                                                                                                                                                                                  y
        self.pipe.enable_model_cpu_offload()
        
        print("I2V (Wan 14B) pipeline loaded successfully.")

    def clear_vram(self):
        """Clears the VRAM used by all loaded components."""
        print(f"Clearing I2V (Wan 14B) VRAM...")
        if self.pipe is not None:
            clear_vram_globally(self.pipe)
        self.pipe = None
        print("I2V (Wan 14B) VRAM cleared.")

    def generate_video_from_image(
        self, image_path: str, output_video_path: str, target_duration: float, 
        content_config: ContentConfig, visual_prompt: str, motion_prompt: Optional[str], 
        ip_adapter_image: Optional[Union[str, List[str]]] = None
    ) -> str:
        """Generates a video by animating a source image using the 14B model."""
        self._load_pipeline()

        input_image = load_image(image_path)
        
        model_caps = self.get_model_capabilities()
        max_area = model_caps["resolutions"]["base_pixel_area"]
        aspect_ratio = input_image.height / input_image.width
        
        # Calculate dimensions using the correct scale factors
        mod_value = self.pipe.vae_scale_factor_spatial * self.pipe.transformer.config.patch_size[1]
        h = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value
        w = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value
        prepared_image = input_image.resize((w, h))

        num_frames = int(target_duration * content_config.fps)
        full_prompt = f"{visual_prompt}, {motion_prompt}" if motion_prompt else visual_prompt
        negative_prompt = "Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards"
        
        print(f"Generating Wan I2V ({w}x{h}) from image: {image_path}")
        print(f"  - Prompt: \"{full_prompt[:70]}...\"")

        video_frames = self.pipe(
            image=prepared_image,
            prompt=full_prompt,
            negative_prompt=negative_prompt,
            height=h,
            width=w,
            num_frames=num_frames,
            guidance_scale=self.config.guidance_scale,
            num_inference_steps=self.config.num_inference_steps,
        ).frames[0]
        
        export_to_video(video_frames, output_video_path, fps=content_config.fps)
        
        print(f"Wan I2V 14B video shot saved to {output_video_path}")
        return output_video_path



==== i2v_modules/__init__.py ====
from .i2v_ltx import LtxI2V
from .i2v_svd import SvdI2V
from .i2v_slideshow import SlideshowI2V
from .i2v_wan import WanI2V



==== t2v_modules/__init__.py ====
from .t2v_zeroscope import ZeroscopeT2V
from .t2v_wan import WanT2V
from .t2v_ltx import LtxT2V



==== t2v_modules/t2v_ltx.py ====
# In t2v_modules/t2v_ltx.py
import torch
from typing import Dict, Any, List, Optional, Union
import os

# --- Import the necessary pipelines and configs ---
from diffusers import LTXPipeline, LTXVideoTransformer3DModel
from diffusers.utils import export_to_video
from transformers import T5EncoderModel, BitsAndBytesConfig as TransformersBitsAndBytesConfig
from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig

from base_modules import BaseT2V, BaseModuleConfig, ModuleCapabilities
from config_manager import DEVICE, clear_vram_globally

class LtxT2VConfig(BaseModuleConfig):
    model_id: str = "Lightricks/LTX-Video" 
    use_8bit_quantization: bool = True
    num_inference_steps: int = 50
    guidance_scale: float = 7.5
    decode_timestep: float = 0.03
    decode_noise_scale: float = 0.025
    # No IP-Adapter configs needed as this pipeline doesn't support them

class LtxT2V(BaseT2V):
    Config = LtxT2VConfig

    # No __init__ needed if we just have the default behavior

    @classmethod
    def get_capabilities(cls) -> ModuleCapabilities:
        """This module is for pure T2V and does NOT support IP-Adapters."""
        return ModuleCapabilities(
            title="LTX, Port/Landscape, No Subject, 5 sec",
            vram_gb_min=8.0,
            ram_gb_min=12.0,
            supported_formats=["Portrait", "Landscape"],
            # --- THE CRITICAL CHANGE: Be honest about capabilities ---
            supports_ip_adapter=False,
            supports_lora=False, # This pipeline doesn't have a LoRA loader either
            max_subjects=0, 
            accepts_text_prompt=True,
            accepts_negative_prompt=True
        )
    
    @classmethod
    def get_model_capabilities(self) -> Dict[str, Any]:
        return {"resolutions": {"Portrait": (512, 768), "Landscape": (768, 512)}, "max_shot_duration": 5.0}

    def _load_pipeline(self):
        if self.pipe is not None: return

        if self.config.use_8bit_quantization:
            print(f"Loading T2V pipeline ({self.config.model_id}) with 8-bit quantization...")
            text_encoder_quant_config = TransformersBitsAndBytesConfig(load_in_8bit=True)
            text_encoder_8bit = T5EncoderModel.from_pretrained(self.config.model_id, subfolder="text_encoder", quantization_config=text_encoder_quant_config, torch_dtype=torch.float16)
            transformer_quant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True)
            transformer_8bit = LTXVideoTransformer3DModel.from_pretrained(self.config.model_id, subfolder="transformer", quantization_config=transformer_quant_config, torch_dtype=torch.float16)
            
            # Note: We are no longer passing the `image_encoder` as it was being ignored.
            self.pipe = LTXPipeline.from_pretrained(
                self.config.model_id,
                text_encoder=text_encoder_8bit,
                transformer=transformer_8bit,
                torch_dtype=torch.float16,
                device_map="balanced",
            )
            print("Quantized T2V pipeline loaded successfully.")
        else:
            print(f"Loading T2V pipeline ({self.config.model_id}) in full precision...")
            self.pipe = LTXPipeline.from_pretrained(
                self.config.model_id,
                torch_dtype=torch.bfloat16
            )
            self.pipe.enable_model_cpu_offload()

        self.pipe.vae.enable_tiling()
        print("VAE tiling enabled for memory efficiency.")

    def clear_vram(self):
        print(f"Clearing T2V (LTX) VRAM...")
        if self.pipe is not None:
            clear_vram_globally(self.pipe)
        self.pipe = None
        print("T2V (LTX) VRAM cleared.")

    def generate_video_from_text(
        self, prompt: str, output_video_path: str, num_frames: int, fps: int, width: int, height: int, ip_adapter_image: Optional[Union[str, List[str]]] = None
    ) -> str:
        self._load_pipeline()

        # --- THE GRACEFUL HANDLING ---
        # If character images are passed, inform the user they are being ignored.
        if ip_adapter_image:
            print("="*50)
            print("WARNING: The LtxT2V module does not support IP-Adapters for character consistency.")
            print("The provided character images will be ignored for this T2V generation.")
            print("="*50)
        
        # All IP-Adapter logic is removed. We just call the pipeline.
        pipeline_kwargs = {}
        
        negative_prompt = "worst quality, inconsistent motion, blurry, jittery, distorted, text, watermark, bad anatomy"
        print(f"Generating LTX T2V ({width}x{height}) for prompt: \"{prompt[:50]}...\"")
        
        video_frames = self.pipe(
            prompt=prompt,
            negative_prompt=negative_prompt,
            width=width,
            height=height,
            num_frames=num_frames,
            num_inference_steps=self.config.num_inference_steps,
            guidance_scale=self.config.guidance_scale,
            decode_timestep=self.config.decode_timestep,
            decode_noise_scale=self.config.decode_noise_scale,
            **pipeline_kwargs
        ).frames[0]
        
        export_to_video(video_frames, output_video_path, fps=fps)
        
        print(f"LTX T2V video shot saved to {output_video_path}")
        return output_video_path



==== t2v_modules/t2v_wan.py ====
# In t2v_modules/t2v_wan.py
import torch
from typing import Dict, Any, List, Optional, Union

# --- Important: Import the specific classes for this model ---
from diffusers import WanPipeline, AutoencoderKLWan
from diffusers.utils import export_to_video

from base_modules import BaseT2V, BaseModuleConfig, ModuleCapabilities
from config_manager import DEVICE, clear_vram_globally

class WanT2VConfig(BaseModuleConfig):
    """Configuration for the Wan 2.1 T2V model."""
    model_id: str = "Wan-AI/Wan2.1-T2V-1.3B-Diffusers"
    # Parameters from the model card example
    num_inference_steps: int = 30 
    guidance_scale: float = 5.0

class WanT2V(BaseT2V):
    """
    Text-to-Video module using Wan 2.1 T2V 1.3B model.
    This model is efficient and produces high-quality video but does not support
    character consistency (IP-Adapter).
    """
    Config = WanT2VConfig

    @classmethod
    def get_capabilities(cls) -> ModuleCapabilities:
        """Declare the capabilities of the Wan 2.1 model."""
        return ModuleCapabilities(
            title="Wan 2.1 (1.3B, Fast, 5s Shots)",
            vram_gb_min=15.0, # Based on the 8.19 GB requirement from the model card
            ram_gb_min=12.0,
            supported_formats=["Portrait", "Landscape"],
            # This model does not support IP-Adapter, so we are honest here.
            supports_ip_adapter=False, 
            supports_lora=False, # The pipeline does not have a LoRA loader
            max_subjects=0,
            accepts_text_prompt=True,
            accepts_negative_prompt=True
        )

    @classmethod
    def get_model_capabilities(self) -> Dict[str, Any]:
        """Return the specific resolutions and max duration for this model."""
        return {
            # Based on the example: width=832, height=480
            "resolutions": {"Portrait": (480, 832), "Landscape": (832, 480)},
            # Based on the example: "generate a 5-second 480P video"
            "max_shot_duration": 5.0 
        }

    def _load_pipeline(self):
        """Loads the custom WanPipeline and its required VAE."""
        if self.pipe is not None:
            return

        print(f"Loading T2V pipeline ({self.config.model_id})...")
        
        # This model requires loading the VAE separately first
        vae = AutoencoderKLWan.from_pretrained(
            self.config.model_id, 
            subfolder="vae", 
            torch_dtype=torch.float32 # VAE often works better in float32
        )

        # Then, load the main pipeline, passing the VAE to it
        self.pipe = WanPipeline.from_pretrained(
            self.config.model_id, 
            vae=vae, 
            torch_dtype=torch.bfloat16 # bfloat16 is recommended in the example
        )

        self.pipe.enable_model_cpu_offload()

        print(f"T2V ({self.config.model_id}) pipeline loaded to {DEVICE}.")

    def clear_vram(self):
        """Clears the VRAM used by the pipeline."""
        print(f"Clearing T2V (Wan) VRAM...")
        if self.pipe is not None:
            clear_vram_globally(self.pipe)
        self.pipe = None
        print("T2V (Wan) VRAM cleared.")

    def generate_video_from_text(
        self, prompt: str, output_video_path: str, num_frames: int, fps: int, width: int, height: int, ip_adapter_image: Optional[Union[str, List[str]]] = None
    ) -> str:
        """Generates a video shot using the Wan T2V pipeline."""
        self._load_pipeline()
        
        # Gracefully handle the case where character images are passed to a non-supporting model.
        if ip_adapter_image:
            print("="*50)
            print("WARNING: The WanT2V module does not support IP-Adapters for character consistency.")
            print("The provided character images will be ignored for this T2V generation.")
            print("="*50)

        # Use the detailed negative prompt from the model card for best results
        negative_prompt = "Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards"
        
        print(f"Generating Wan T2V ({width}x{height}) for prompt: \"{prompt[:70]}...\"")
        
        video_frames = self.pipe(
            prompt=prompt,
            negative_prompt=negative_prompt,
            height=height,
            width=width,
            num_frames=num_frames,
            guidance_scale=self.config.guidance_scale,
            num_inference_steps=self.config.num_inference_steps
        ).frames[0]
        
        # The system's config determines the final FPS, not the model's example
        export_to_video(video_frames, output_video_path, fps=fps)
        
        print(f"Wan T2V video shot saved to {output_video_path}")
        return output_video_path



==== t2v_modules/t2v_zeroscope.py ====
# In t2v_modules/t2v_zeroscope.py
import torch
from typing import Dict, Any, List, Optional, Union
from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler
from diffusers.utils import export_to_video

from base_modules import BaseT2V, BaseModuleConfig, ModuleCapabilities
from config_manager import DEVICE, clear_vram_globally

class ZeroscopeT2VConfig(BaseModuleConfig):
    model_id: str = "cerspense/zeroscope_v2_576w"
    upscaler_model_id: str = "cerspense/zeroscope_v2_xl"
    
    num_inference_steps: int = 30
    guidance_scale: float = 9.0
    # --- START OF FIX: Add strength for the upscaling process ---
    upscaler_strength: float = 0.7 
    # --- END OF FIX ---

class ZeroscopeT2V(BaseT2V):
    Config = ZeroscopeT2VConfig

    @classmethod
    def get_capabilities(cls) -> ModuleCapabilities:
        return ModuleCapabilities(
            title="Zeroscope, Port/Landscape, No Subject, 2 sec",
            vram_gb_min=8.0,
            ram_gb_min=12.0,
            supported_formats=["Portrait", "Landscape"],
            supports_ip_adapter=False, # Zeroscope does not support IP-Adapter
            supports_lora=False, # Zeroscope does not support LoRA loading
            max_subjects=0,
            accepts_text_prompt=True,
            accepts_negative_prompt=True
        )

    
    def __init__(self, config: ZeroscopeT2VConfig):
        super().__init__(config)
        self.upscaler_pipe = None

    @classmethod
    def get_model_capabilities(self) -> Dict[str, Any]:
        # Zeroscope has a fixed native resolution that is then upscaled
        base_resolution = (576, 320)
        return {
            "resolutions": {"Portrait": base_resolution, "Landscape": base_resolution},
            "max_shot_duration": 2.0 
        }

    def _load_pipeline(self):
        if self.pipe is None:
            print(f"Loading T2V pipeline ({self.config.model_id})...")
            self.pipe = DiffusionPipeline.from_pretrained(self.config.model_id, torch_dtype=torch.float16)
            self.pipe.scheduler = DPMSolverMultistepScheduler.from_config(self.pipe.scheduler.config)
            self.pipe.enable_model_cpu_offload()
            print(f"T2V ({self.config.model_id}) pipeline loaded.")
            
        if self.upscaler_pipe is None:
            print(f"Loading T2V Upscaler pipeline ({self.config.upscaler_model_id})...")
            self.upscaler_pipe = DiffusionPipeline.from_pretrained(self.config.upscaler_model_id, torch_dtype=torch.float16)
            self.upscaler_pipe.scheduler = DPMSolverMultistepScheduler.from_config(self.upscaler_pipe.scheduler.config)
            self.upscaler_pipe.enable_model_cpu_offload()
            print(f"T2V Upscaler ({self.config.upscaler_model_id}) pipeline loaded.")

    def clear_vram(self):
        print(f"Clearing T2V VRAM...")
        models_to_clear = [m for m in [self.pipe, self.upscaler_pipe] if m is not None]
        if models_to_clear: clear_vram_globally(*models_to_clear)
        self.pipe, self.upscaler_pipe = None, None
        print("T2V VRAM cleared.")

    def generate_video_from_text(
        self, prompt: str, output_video_path: str, num_frames: int, fps: int, width: int, height: int, ip_adapter_image: Optional[Union[str, List[str]]] = None
    ) -> str:
        self._load_pipeline()
        
        if ip_adapter_image:
            print("Warning: ZeroscopeT2V module received IP-Adapter image but does not currently implement its use.")

        negative_prompt = "blurry, low quality, watermark, bad anatomy, text, letters, distorted"
        
        # Note: Zeroscope generates at a fixed resolution, so we use its capabilities directly
        model_res = self.get_model_capabilities()["resolutions"]["Landscape"]
        
        print(f"Stage 1: Generating T2V ({model_res[0]}x{model_res[1]}) for prompt: \"{prompt[:70]}...\"")
        
        video_frames_tensor = self.pipe(
            prompt=prompt, negative_prompt=negative_prompt,
            num_inference_steps=self.config.num_inference_steps,
            height=model_res[1], width=model_res[0], num_frames=num_frames,
            guidance_scale=self.config.guidance_scale, output_type="pt"
        ).frames
        
        print("Stage 2: Upscaling video to HD...")
        
        # --- START OF FIX ---
        upscaled_video_frames = self.upscaler_pipe(
            prompt=prompt,
            negative_prompt=negative_prompt,
            video=video_frames_tensor, # The argument is 'video', not 'image'.
            strength=self.config.upscaler_strength, # Add the strength parameter
            num_inference_steps=self.config.num_inference_steps,
            guidance_scale=self.config.guidance_scale,
        ).frames[0]
        # --- END OF FIX ---

        export_to_video(upscaled_video_frames, output_video_path, fps=fps)
        
        print(f"High-quality T2V video shot saved to {output_video_path}")
        return output_video_path



