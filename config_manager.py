# config_manager.py
import os
import torch # For DEVICE check
import gc
from dataclasses import dataclass, field
from typing import Tuple, Optional, Literal

@dataclass
class ContentConfig:
    """Configuration for overall content generation parameters."""
    # Video settings
    target_video_length_hint: float = 20.0  # Target total video length for LLM guidance (seconds)
    
    # Max duration a single video chunk generated by the model can be (seconds)
    model_max_video_chunk_duration: float = 3.0 
    
    # Final output resolution (e.g., 1080x1920 for 9:16, 1920x1080 for 16:9)
    final_output_resolution: Tuple[int, int] = (1280, 720) # (Width, Height)
    
    # FPS settings for generated videos and final output
    fps: int = 10 # Frames per second
    
    # Scene settings (for LLM guidance)
    min_scenes: int = 2  # Min narration/visual scene pairs from LLM
    max_scenes: int = 5  # Max narration/visual scene pairs from LLM
    max_scene_narration_duration_hint: float = 6.0 # LLM hint for ideal narration part length (seconds)
    
    # Model pipeline selection
    use_svd_flow: bool = True  # True for T2I -> I2V (e.g., SDXL -> SVD)
                               # False for direct T2V (e.g., Zeroscope)
    
    # Output settings
    output_dir: str = "modular_reels_output"
    font_for_subtitles: str = "Arial" # Font name or path to .ttf file for subtitles

    # Generation resolution (dimensions for the AI models to generate at)
    # This can be different from final_output_resolution, often smaller for speed.
    # It will be upscaled/resized during video assembly.
    # If None, defaults to a fraction of final_output_resolution.
    _generation_resolution: Optional[Tuple[int, int]] = None 
    
    # Minimum dimensions for generation (to ensure quality if auto-calculated)
    min_generation_width: int = 256
    min_generation_height: int = 256

    def __init__(self, **kwargs):
        # Handle generation_resolution specially
        if 'generation_resolution' in kwargs:
            self._generation_resolution = kwargs.pop('generation_resolution')
        
        # Initialize other fields
        for key, value in kwargs.items():
            setattr(self, key, value)
        
        # Call post_init for validation
        self.__post_init__()

    @property
    def generation_resolution(self) -> Tuple[int, int]:
        if self._generation_resolution:
            width, height = self._generation_resolution
        else:
            # Default to half of final output resolution if not explicitly set
            width = self.final_output_resolution[0] // 2
            height = self.final_output_resolution[1] // 2

        # Ensure dimensions are at least the minimum
        width = max(width, self.min_generation_width)
        height = max(height, self.min_generation_height)

        # Ensure dimensions are divisible by 8 (common requirement for diffusion models)
        width = (width // 8) * 8
        height = (height // 8) * 8
        
        # If SVD flow is used, SVD-XT is trained on 1024x576 or 576x1024.
        # We should try to match one of its supported aspect ratios if possible,
        # or be aware that it might crop/resize internally.
        # For now, this just ensures divisibility by 8.
        # Further model-specific adjustments might be needed in the model modules.
        # For Zeroscope_v2_576w, width is 576, height is 320.
        if not self.use_svd_flow: # Example: if using Zeroscope 576w
            if "zeroscope_v2_576w" in ModuleSelectorConfig().t2v_config.model_id: # Check default
                 # Override for Zeroscope if it's the selected T2V model
                # This is a bit of a hack here; ideally, model modules would report preferred sizes.
                print("Adjusting generation_resolution for Zeroscope_v2_576w (576x320)")
                return (576, 320)

        return (width, height)

    @generation_resolution.setter
    def generation_resolution(self, value: Optional[Tuple[int, int]]):
        self._generation_resolution = value

    def __post_init__(self):
        os.makedirs(self.output_dir, exist_ok=True)
        
        # Validate aspect ratio of final output
        if self.final_output_resolution[0] <= 0 or self.final_output_resolution[1] <= 0:
            raise ValueError("Final output resolution dimensions must be positive.")
        
        aspect_ratio = self.final_output_resolution[0] / self.final_output_resolution[1]
        # Allowing a wider range, e.g., from 1:4 (0.25) to 4:1 (4.0) might be useful
        # Instagram Reels are 9:16 (0.5625), Shorts same, TikTok same. YouTube 16:9 (1.77).
        if not (0.25 <= aspect_ratio <= 4.0):
            print(f"Warning: Final output aspect ratio {aspect_ratio:.2f} is unusual. Common ratios are 9:16, 16:9, 1:1, 4:5.")
        
        # Validate generation dimensions (property will handle divisibility by 8)
        gen_width, gen_height = self.generation_resolution # Access property to trigger calculation
        if gen_width < self.min_generation_width or gen_height < self.min_generation_height:
            # This check is somewhat redundant if min_generation_width/height are enforced in the property,
            # but good for explicit user-set _generation_resolution.
            raise ValueError(
                f"Calculated or set generation resolution {gen_width}x{gen_height} "
                f"is below minimum {self.min_generation_width}x{self.min_generation_height}. "
                "Adjust min_generation_width/height or final_output_resolution."
            )
        print(f"Using Generation Resolution: {gen_width}x{gen_height}")
        print(f"Using Final Output Resolution: {self.final_output_resolution[0]}x{self.final_output_resolution[1]}")

@dataclass
class LLMConfig:
    model_id: str = "HuggingFaceH4/zephyr-7b-beta"
    max_new_tokens_script: int = 1536
    max_new_tokens_chunk_prompt: int = 256 # Shorter for chunk prompts
    temperature: float = 0.7
    top_k: int = 50
    top_p: float = 0.95

@dataclass
class TTSConfig:
    model_id: str = "tts_models/multilingual/multi-dataset/xtts_v2"
    speaker_language: str = "en"

@dataclass
class T2IConfig: # For SDXL or similar
    model_id: str = "stabilityai/stable-diffusion-xl-base-1.0"
    refiner_id: Optional[str] = None # e.g., "stabilityai/stable-diffusion-xl-refiner-1.0"
    num_inference_steps: int = 30
    guidance_scale: float = 7.5
    # If using refiner, these control the split
    base_denoising_end: float = 0.8 
    refiner_denoising_start: float = 0.8


@dataclass
class I2VConfig: # For SVD or similar
    model_id: str = "stabilityai/stable-video-diffusion-img2vid-xt" # SVD-XT
    decode_chunk_size: int = 8 # SVD specific, can be 2, 4, 8. Higher might be faster but more VRAM.
    motion_bucket_id: int = 127 
    noise_aug_strength: float = 0.02
    # SVD models have fixed output frames, e.g., SVD-XT is 25, base SVD is 14.
    # The module will use this as the target and clip/loop if necessary.
    # This 'num_frames' is what the model *generates*. The main script calculates desired frames based on duration.
    model_native_frames: int = 25 # For SVD-XT. For base SVD, it's 14.
    # Min frames for generation request to SVD, even if target_chunk_duration is very short
    min_request_frames: int = 8 # SVD might have issues with too few frames requested.
    svd_min_frames: int = 8 # Minimum frames to request from SVD model


@dataclass
class T2VConfig: # For Zeroscope, ModelScope T2V or similar
    model_id: str = "cerspense/zeroscope_v2_576w"
    num_inference_steps: int = 25
    # num_frames for T2V models is often more flexible than SVD

@dataclass
class ModuleSelectorConfig:
    """Selects which implementation module to use for each step."""
    llm_module: str = "llm_modules.llm_zephyr"
    tts_module: str = "tts_modules.tts_coqui"
    t2i_module: str = "t2i_modules.t2i_sdxl"
    i2v_module: str = "i2v_modules.i2v_svd"
    t2v_module: str = "t2v_modules.t2v_zeroscope"

    # Configurations for each selected module type
    llm_config: LLMConfig = field(default_factory=LLMConfig)
    tts_config: TTSConfig = field(default_factory=TTSConfig)
    t2i_config: T2IConfig = field(default_factory=T2IConfig)
    i2v_config: I2VConfig = field(default_factory=I2VConfig)
    t2v_config: T2VConfig = field(default_factory=T2VConfig)

# Global device setting
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
if DEVICE == "cuda":
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True" # Good for PyTorch

def clear_vram_globally(*models_or_pipelines_to_del):
    """Aggressively tries to clear VRAM by deleting objects and calling gc and torch.cuda.empty_cache."""
    print(f"Attempting to clear VRAM. Received {len(models_or_pipelines_to_del)} items to delete.")
    
    for item_idx, item in enumerate(models_or_pipelines_to_del):
        if item is None:
            continue
        
        item_name = type(item).__name__
        # Attempt to move to CPU if applicable (common for Hugging Face pipelines/models)
        if hasattr(item, 'cpu') and callable(item.cpu):
            try:
                item.cpu()
                print(f"Moved {item_name} (item {item_idx}) to CPU.")
            except Exception as e:
                print(f"Warning: Could not move {item_name} (item {item_idx}) to CPU: {e}")
        
        # Specifically for HF pipelines that have a .model attribute
        if hasattr(item, 'model') and hasattr(item.model, 'cpu') and callable(item.model.cpu):
            try:
                item.model.cpu()
                print(f"Moved model of {item_name} (item {item_idx}) to CPU.")
            except Exception as e:
                print(f"Warning: Could not move model of {item_name} (item {item_idx}) to CPU: {e}")
        
        # Delete the item from the list of arguments (local scope)
        # This doesn't guarantee deletion if other references exist, but it's a start.
    
    # Delete references passed to this function
    # This only affects the local copies 'models_or_pipelines_to_del'
    # The caller needs to ensure their references are also set to None or del'd.
    del models_or_pipelines_to_del 

    # Python's garbage collection
    collected_count = gc.collect()
    print(f"Garbage collector collected {collected_count} objects.")

    # PyTorch CUDA cache clearing
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        # torch.cuda.synchronize() # Ensures all CUDA ops are done before cache clear takes full effect.
                                 # Can be time-consuming, use if strictly needed.
        print("PyTorch CUDA cache emptied.")
    
    print("VRAM clearing attempt finished.")