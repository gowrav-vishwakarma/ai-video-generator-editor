{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b63501f-2941-4ba6-b1a0-36dcb3a4ae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_script.py\n",
    "import os\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from diffusers import StableDiffusionXLPipeline, StableVideoDiffusionPipeline, DiffusionPipeline\n",
    "from diffusers.utils import load_image, export_to_video\n",
    "from TTS.api import TTS # Coqui TTS\n",
    "from moviepy import VideoFileClip, AudioFileClip, concatenate_videoclips, TextClip, CompositeVideoClip\n",
    "import gc # Garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "677f6184-dfb2-46fb-8317-302b682dcaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0. CONFIGURATION ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "OUTPUT_DIR = \"instagram_content\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# LLM paths (example, download these first)\n",
    "LLM_MODEL_ID = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "# TTS model (XTTSv2 example)\n",
    "TTS_MODEL_ID = \"tts_models/multilingual/multi-dataset/xtts_v2\" # or \"tts_models/en/ljspeech/tacotron2-DDC\"\n",
    "# T2I model\n",
    "T2I_MODEL_ID = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "T2I_REFINER_ID = \"stabilityai/stable-diffusion-xl-refiner-1.0\"\n",
    "# I2V model (SVD)\n",
    "I2V_MODEL_ID = \"stabilityai/stable-video-diffusion-img2vid-xt\"\n",
    "# T2V model (alternative)\n",
    "T2V_MODEL_ID = \"damo-vilab/text-to-video-ms-1.7b\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76458e3a-6d90-42d6-a511-b188c5eeccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. INITIALIZE MODELS (Load only when needed or keep loaded if VRAM allows) ---\n",
    "def load_llm():\n",
    "    print(\"Loading LLM...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_ID)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        LLM_MODEL_ID,\n",
    "        torch_dtype=torch.float16, # Use float16 for 4090\n",
    "        device_map=\"auto\", # Automatically uses GPU\n",
    "        # load_in_8bit=True, # Optional: if VRAM is an issue with other models loaded\n",
    "        # load_in_4bit=True, # Optional: for even smaller footprint\n",
    "    )\n",
    "    # Using pipeline for simplicity\n",
    "    return pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\", max_new_tokens=512)\n",
    "\n",
    "def load_tts():\n",
    "    print(\"Loading TTS model...\")\n",
    "    # Make sure you have the model downloaded or it will download on first run\n",
    "    # For XTTSv2, you might need to specify a speaker_wav for voice cloning\n",
    "    return TTS(model_name=TTS_MODEL_ID, progress_bar=True).to(DEVICE)\n",
    "\n",
    "def load_t2i_pipeline():\n",
    "    print(\"Loading T2I pipeline (SDXL)...\")\n",
    "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "        T2I_MODEL_ID, torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    "    ).to(DEVICE)\n",
    "    # Optional: Load refiner if you want to use it\n",
    "    # refiner = DiffusionPipeline.from_pretrained(\n",
    "    #     T2I_REFINER_ID, text_encoder_2=pipe.text_encoder_2, vae=pipe.vae,\n",
    "    #     torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\"\n",
    "    # ).to(DEVICE)\n",
    "    # return pipe, refiner\n",
    "    return pipe, None # Simpler for now\n",
    "\n",
    "def load_i2v_pipeline(): # SVD\n",
    "    print(\"Loading I2V pipeline (SVD)...\")\n",
    "    pipe = StableVideoDiffusionPipeline.from_pretrained(\n",
    "        I2V_MODEL_ID, torch_dtype=torch.float16, variant=\"fp16\"\n",
    "    ).to(DEVICE)\n",
    "    pipe.enable_model_cpu_offload() # If VRAM is tight with other models\n",
    "    return pipe\n",
    "\n",
    "def load_t2v_pipeline(): # ModelScope Text-to-Video\n",
    "    print(\"Loading T2V pipeline (ModelScope)...\")\n",
    "    # Use DiffusionPipeline as per the documentation for damo-vilab/text-to-video-ms-1.7b\n",
    "    pipe = DiffusionPipeline.from_pretrained(\n",
    "        T2V_MODEL_ID, torch_dtype=torch.float16, variant=\"fp16\"\n",
    "    ).to(DEVICE)\n",
    "    # The .enable_model_cpu_offload() should still work if the loaded pipe supports it\n",
    "    # For ModelScope, this is standard.\n",
    "    pipe.enable_model_cpu_offload()\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99a88876-14fc-4191-ba60-6f96b7989c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- UTILITY TO CLEAR VRAM ---\n",
    "def clear_vram(*models_or_pipelines):\n",
    "    for item in models_or_pipelines:\n",
    "        if hasattr(item, 'cpu') and callable(getattr(item, 'cpu')):\n",
    "            item.cpu() # If it's a pipeline/model with a .cpu() method\n",
    "        elif hasattr(item, 'model') and hasattr(item.model, 'cpu') and callable(getattr(item.model, 'cpu')):\n",
    "            item.model.cpu()\n",
    "    del models_or_pipelines\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"VRAM cleared and memory collected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97df1a34-eeb1-4847-939f-b1f922f20e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. TEXT GENERATION ---\n",
    "def generate_script_and_prompts(topic, llm_pipeline):\n",
    "    print(f\"Generating script and prompts for topic: {topic}\")\n",
    "    # Improved prompt for structured output\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant creating content for an Instagram Reel about \"{topic}\".\n",
    "    The Reel should be engaging and around 15-30 seconds long.\n",
    "    Provide the following:\n",
    "    1. Narration Script: A short, punchy narration. Break it into 2-3 short sentences or scenes.\n",
    "    2. Visual Prompts: For each sentence/scene in the narration, provide a detailed visual prompt suitable for an image or video generation model. Describe the scene, style, colors, and mood.\n",
    "    3. Hashtags: 3-5 relevant hashtags.\n",
    "\n",
    "    Format your response clearly, for example:\n",
    "\n",
    "    Narration Script:\n",
    "    Scene 1: [Sentence 1 for narration]\n",
    "    Scene 2: [Sentence 2 for narration]\n",
    "\n",
    "    Visual Prompts:\n",
    "    Scene 1: [Visual prompt for sentence 1, e.g., \"A vibrant, futuristic cityscape at sunset, Blade Runner style, cinematic lighting, detailed.\"]\n",
    "    Scene 2: [Visual prompt for sentence 2, e.g., \"Close up on a curious robot eye, glowing blue, intricate details, macro shot.\"]\n",
    "\n",
    "    Hashtags:\n",
    "    #tag1 #tag2 #tag3\n",
    "    \"\"\"\n",
    "    response = llm_pipeline(prompt, num_return_sequences=1)[0]['generated_text']\n",
    "    print(\"LLM Response:\\n\", response)\n",
    "\n",
    "    # Basic parsing (this needs to be robust)\n",
    "    narration_parts = []\n",
    "    visual_prompts_parts = []\n",
    "    hashtags_str = \"\"\n",
    "\n",
    "    in_narration = False\n",
    "    in_visuals = False\n",
    "    in_hashtags = False\n",
    "\n",
    "    current_scene_narration = []\n",
    "    current_scene_visuals = []\n",
    "\n",
    "    for line in response.splitlines():\n",
    "        line_lower = line.lower()\n",
    "        if \"narration script:\" in line_lower:\n",
    "            in_narration = True\n",
    "            in_visuals = False\n",
    "            in_hashtags = False\n",
    "            continue\n",
    "        elif \"visual prompts:\" in line_lower:\n",
    "            in_narration = False\n",
    "            in_visuals = True\n",
    "            in_hashtags = False\n",
    "            continue\n",
    "        elif \"hashtags:\" in line_lower:\n",
    "            in_narration = False\n",
    "            in_visuals = False\n",
    "            in_hashtags = True\n",
    "            continue\n",
    "\n",
    "        if in_narration and line.strip() and not line_lower.startswith(\"scene\"):\n",
    "            if line_lower.startswith(\"scene\"): # Handles cases where scene numbering is on the same line\n",
    "                 narration_parts.append(line.split(\":\",1)[1].strip())\n",
    "            else:\n",
    "                narration_parts.append(line.strip())\n",
    "        elif in_visuals and line.strip() and not line_lower.startswith(\"scene\"):\n",
    "            if line_lower.startswith(\"scene\"):\n",
    "                visual_prompts_parts.append(line.split(\":\",1)[1].strip())\n",
    "            else:\n",
    "                visual_prompts_parts.append(line.strip())\n",
    "        elif in_hashtags and line.strip():\n",
    "            hashtags_str += line.strip() + \" \"\n",
    "\n",
    "    # A bit of cleanup for parsing, assuming one line per scene now\n",
    "    # This parsing is VERY basic and likely needs improvement based on LLM output format\n",
    "    parsed_narration = [s.split(\":\", 1)[1].strip() for s in response.split(\"Narration Script:\")[1].split(\"Visual Prompts:\")[0].splitlines() if \"Scene\" in s]\n",
    "    parsed_visuals = [s.split(\":\", 1)[1].strip() for s in response.split(\"Visual Prompts:\")[1].split(\"Hashtags:\")[0].splitlines() if \"Scene\" in s]\n",
    "    parsed_hashtags = response.split(\"Hashtags:\")[1].strip() if \"Hashtags:\" in response else \"#GeneratedContent\"\n",
    "\n",
    "\n",
    "    if not parsed_narration or not parsed_visuals:\n",
    "        print(\"Error: Could not parse LLM response properly. Using fallback.\")\n",
    "        # Fallback if parsing fails (common issue with LLM outputs)\n",
    "        parsed_narration = [f\"A short segment about {topic}.\", \"Concluding thoughts on the future.\"]\n",
    "        parsed_visuals = [\n",
    "            f\"Cinematic shot of {topic}, vibrant colors, high detail, trending on artstation.\",\n",
    "            f\"Abstract representation of {topic}, thought-provoking, professional.\"\n",
    "        ]\n",
    "        parsed_hashtags = f\"#{topic.replace(' ', '')} #AI #GeneratedContent\"\n",
    "\n",
    "    return parsed_narration, parsed_visuals, parsed_hashtags.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35141253-5f6e-45d4-a3d6-983b40e68a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. AUDIO GENERATION ---\n",
    "def generate_audio(text, tts_model, output_path, speaker_wav=None): # speaker_wav for XTTS\n",
    "    print(f\"Generating audio for: {text}\")\n",
    "    if \"xtts\" in TTS_MODEL_ID.lower() and speaker_wav:\n",
    "        tts_model.tts_to_file(text, speaker_wav=speaker_wav, language=\"en\", file_path=output_path)\n",
    "    else: # For other models like Tacotron\n",
    "        tts_model.tts_to_file(text, file_path=output_path)\n",
    "    print(f\"Audio saved to {output_path}\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "942e3080-9cb6-420c-852b-d40de3451b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. VISUAL GENERATION ---\n",
    "# Option A: Image (SDXL) then Video (SVD) - Recommended for \"Subject to Video\"\n",
    "def generate_image_then_video(image_prompt, i2v_pipe, t2i_pipe, refiner_pipe, scene_idx):\n",
    "    # Generate Image (Keyframe)\n",
    "    print(f\"Generating keyframe image for: {image_prompt}\")\n",
    "    # For SDXL, you can add negative prompts, specify num_inference_steps, guidance_scale etc.\n",
    "    # Using SDXL\n",
    "    image = t2i_pipe(\n",
    "        prompt=image_prompt,\n",
    "        # negative_prompt=\"low quality, blurry, watermark\", # Example\n",
    "        num_inference_steps=30, # SDXL typically needs fewer steps\n",
    "        guidance_scale=7.5,\n",
    "        # If using refiner:\n",
    "        # output_type=\"latent\" if refiner_pipe else \"pil\",\n",
    "    ).images[0]\n",
    "    # if refiner_pipe:\n",
    "    #     image = refiner_pipe(prompt=image_prompt, image=image[None, :]).images[0]\n",
    "\n",
    "    image_path = os.path.join(OUTPUT_DIR, f\"scene_{scene_idx}_keyframe.png\")\n",
    "    image.save(image_path)\n",
    "    print(f\"Keyframe image saved to {image_path}\")\n",
    "\n",
    "    # Generate Video from Image (SVD)\n",
    "    print(f\"Generating video from image using SVD...\")\n",
    "    # SVD parameters\n",
    "    video_frames = i2v_pipe(\n",
    "        image,\n",
    "        decode_chunk_size=8, # Lower if OOM, max typically 8 or 4\n",
    "        num_frames=25, # SVD XT default, ~4 seconds at low FPS\n",
    "        motion_bucket_id=127, # Adjust for more/less motion\n",
    "        fps=7, # Low FPS for SVD output, will be re-timed in MoviePy\n",
    "        noise_aug_strength=0.02 # Default\n",
    "    ).frames[0]\n",
    "\n",
    "    video_clip_path = os.path.join(OUTPUT_DIR, f\"scene_{scene_idx}_svd.mp4\")\n",
    "    export_to_video(video_frames, video_clip_path, fps=7) # Save with SVD's native FPS\n",
    "    print(f\"SVD video clip saved to {video_clip_path}\")\n",
    "    return video_clip_path\n",
    "\n",
    "# Option B: Direct Text-to-Video (ModelScope)\n",
    "def generate_direct_video(video_prompt, t2v_pipe, scene_idx):\n",
    "    print(f\"Generating direct video for: {video_prompt}\")\n",
    "    # ModelScope parameters\n",
    "    video_frames = t2v_pipe(video_prompt, num_inference_steps=25, num_frames=20).frames # ~2-3 sec\n",
    "    video_clip_path = os.path.join(OUTPUT_DIR, f\"scene_{scene_idx}_t2v.mp4\")\n",
    "    export_to_video(video_frames, video_clip_path, fps=8) # ModelScope native FPS\n",
    "    print(f\"T2V video clip saved to {video_clip_path}\")\n",
    "    return video_clip_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a163c04b-457f-4c56-9bc3-04eac46119a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. VIDEO ASSEMBLY ---\n",
    "def assemble_final_video(video_clip_paths, audio_clip_paths, narration_parts, output_filename=\"final_reel.mp4\"):\n",
    "    print(\"Assembling final video...\")\n",
    "    final_clips = []\n",
    "    target_resolution = (1080, 1920) # Instagram Reel 9:16\n",
    "    source_clips_to_close = []\n",
    "\n",
    "    # Use a specific, existing font path\n",
    "    font_path_for_textclip = \"/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf\"\n",
    "    # Fallback if the above is not found, try DejaVu:\n",
    "    # if not os.path.exists(font_path_for_textclip):\n",
    "    #     font_path_for_textclip = \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\"\n",
    "    # Fallback to Arial Bold from msttcorefonts if others aren't there:\n",
    "    # if not os.path.exists(font_path_for_textclip):\n",
    "    #     font_path_for_textclip = \"/usr/share/fonts/truetype/msttcorefonts/arialbd.ttf\" # or Arial_Bold.ttf\n",
    "\n",
    "    # IMPORTANT: Check if the chosen font_path_for_textclip actually exists before proceeding\n",
    "    if not os.path.exists(font_path_for_textclip):\n",
    "        print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(f\"ERROR: Font file NOT FOUND at: {font_path_for_textclip}\")\n",
    "        print(f\"Please verify the font path and filename.\")\n",
    "        print(f\"Available font directories you listed: dejavu, droid, liberation, msttcorefonts, noto, ubuntu\")\n",
    "        print(f\"Example: Check 'ls /usr/share/fonts/truetype/liberation/' to find the exact .ttf file name.\")\n",
    "        print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        # You might want to raise an error here or use a very generic font name that ImageMagick might find\n",
    "        # For now, let it proceed, but Pillow will likely fail again if the path is wrong.\n",
    "        # raise FileNotFoundError(f\"Font file not found: {font_path_for_textclip}\")\n",
    "\n",
    "\n",
    "    for i, (video_path, audio_path, text_caption) in enumerate(zip(video_clip_paths, audio_clip_paths, narration_parts)):\n",
    "        video_clip_temp = VideoFileClip(video_path)\n",
    "        source_clips_to_close.append(video_clip_temp)\n",
    "\n",
    "        video_clip_resized = video_clip_temp.resized(height=target_resolution[1])\n",
    "\n",
    "        if video_clip_resized.w > target_resolution[0]:\n",
    "            video_clip_final_shape = video_clip_resized.cropped(x_center=video_clip_resized.w/2, width=target_resolution[0])\n",
    "        else:\n",
    "            video_clip_final_shape = video_clip_resized\n",
    "\n",
    "        video_clip_positioned = video_clip_final_shape.with_position('center')\n",
    "\n",
    "        audio_clip_temp = AudioFileClip(audio_path)\n",
    "        source_clips_to_close.append(audio_clip_temp)\n",
    "\n",
    "        video_clip_with_audio = video_clip_positioned.with_audio(audio_clip_temp)\n",
    "        video_clip_timed = video_clip_with_audio.with_duration(audio_clip_temp.duration)\n",
    "\n",
    "        # Corrected TextClip call:\n",
    "        txt_clip_temp = TextClip(\n",
    "            font_path_for_textclip,    # First argument is font\n",
    "            text=text_caption,         # 'text' is a keyword argument\n",
    "            font_size=60,              # CORRECTED: font_size (with underscore)\n",
    "            color='white',\n",
    "            stroke_color='black',\n",
    "            stroke_width=2,\n",
    "            method='caption',\n",
    "            size=(int(target_resolution[0]*0.8), None)\n",
    "        )\n",
    "        source_clips_to_close.append(txt_clip_temp)\n",
    "\n",
    "        txt_clip_final = txt_clip_temp.with_position(('center', 0.8), relative=True).with_duration(video_clip_timed.duration)\n",
    "\n",
    "        scene_composite = CompositeVideoClip([video_clip_timed, txt_clip_final], size=target_resolution)\n",
    "        final_clips.append(scene_composite)\n",
    "\n",
    "    if not final_clips:\n",
    "        print(\"No clips to assemble!\")\n",
    "        for clip_to_close in source_clips_to_close:\n",
    "            if hasattr(clip_to_close, 'close') and callable(clip_to_close.close):\n",
    "                clip_to_close.close()\n",
    "        return None\n",
    "\n",
    "    final_video = concatenate_videoclips(final_clips, method=\"compose\")\n",
    "    final_video_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "    \n",
    "    try:\n",
    "        final_video.write_videofile(final_video_path, \n",
    "                                    fps=24, \n",
    "                                    codec=\"libx264\", \n",
    "                                    audio_codec=\"aac\",\n",
    "                                    threads=4, \n",
    "                                    preset=\"medium\",\n",
    "                                    logger='bar'\n",
    "                                   ) \n",
    "    except Exception as e:\n",
    "        print(f\"Error during video writing: {e}\")\n",
    "        print(\"Make sure ffmpeg is correctly installed and accessible by MoviePy.\")\n",
    "        print(f\"Ensure the font '{font_path_for_textclip}' is available or try a system default font.\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    finally:\n",
    "        for clip_to_close in source_clips_to_close:\n",
    "            if hasattr(clip_to_close, 'close') and callable(clip_to_close.close):\n",
    "                try:\n",
    "                    clip_to_close.close()\n",
    "                except Exception as e_close:\n",
    "                    print(f\"Error closing clip {type(clip_to_close)}: {e_close}\")\n",
    "        if hasattr(final_video, 'close') and callable(final_video.close):\n",
    "            try:\n",
    "                final_video.close()\n",
    "            except Exception as e_close:\n",
    "                print(f\"Error closing final_video: {e_close}\")\n",
    "\n",
    "    print(f\"Final video saved to {final_video_path}\")\n",
    "    return final_video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe8174ec-88c8-4631-b668-c3fa110aa0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MAIN WORKFLOW ---\n",
    "def main_automation_flow(topic, use_svd_flow=True):\n",
    "    # --- Load models ---\n",
    "    # Manage VRAM: Load one by one or use .cpu_offload()\n",
    "    llm = None\n",
    "    tts_model = None\n",
    "    t2i_pipe, refiner = None, None\n",
    "    i2v_pipe = None\n",
    "    t2v_pipe = None\n",
    "    final_video_path = None\n",
    "    speaker_reference_audio = \"record_out.wav\" # REQUIRED for XTTS voice cloning\n",
    "    # Make sure speaker_reference_audio exists or provide a default way to handle TTS without it.\n",
    "    # e.g. if not os.path.exists(speaker_reference_audio): speaker_reference_audio = None\n",
    "\n",
    "    try:\n",
    "        # 1. LLM for Script and Prompts\n",
    "        llm = load_llm()\n",
    "        narration_scenes, visual_prompts_scenes, hashtags = generate_script_and_prompts(topic, llm)\n",
    "        clear_vram(llm) # Unload LLM to free VRAM for vision models\n",
    "\n",
    "        # 2. TTS for Narration\n",
    "        tts_model = load_tts()\n",
    "        audio_paths = []\n",
    "        for i, scene_text in enumerate(narration_scenes):\n",
    "            audio_file = os.path.join(OUTPUT_DIR, f\"scene_{i}_audio.wav\")\n",
    "            # For XTTSv2, provide speaker_wav for voice cloning.\n",
    "            # Ensure you have a reference audio file for XTTS if using it.\n",
    "            # If speaker_reference_audio is not set, XTTS might use a default voice or fail.\n",
    "            # For non-XTTS models, speaker_wav can be None.\n",
    "            generate_audio(scene_text, tts_model, audio_file, speaker_wav=speaker_reference_audio if \"xtts\" in TTS_MODEL_ID.lower() else None)\n",
    "            audio_paths.append(audio_file)\n",
    "        clear_vram(tts_model)\n",
    "\n",
    "        # 3. Visual Generation\n",
    "        video_clip_paths = []\n",
    "        if use_svd_flow: # Image (SDXL) -> Video (SVD)\n",
    "            t2i_pipe, refiner = load_t2i_pipeline() # SDXL\n",
    "            i2v_pipe = load_i2v_pipeline()         # SVD\n",
    "            for i, visual_prompt in enumerate(visual_prompts_scenes):\n",
    "                clip_path = generate_image_then_video(visual_prompt, i2v_pipe, t2i_pipe, refiner, i)\n",
    "                video_clip_paths.append(clip_path)\n",
    "            clear_vram(t2i_pipe, refiner, i2v_pipe)\n",
    "        else: # Direct Text-to-Video (ModelScope)\n",
    "            t2v_pipe = load_t2v_pipeline()\n",
    "            for i, visual_prompt in enumerate(visual_prompts_scenes):\n",
    "                clip_path = generate_direct_video(visual_prompt, t2v_pipe, i)\n",
    "                video_clip_paths.append(clip_path)\n",
    "            clear_vram(t2v_pipe)\n",
    "\n",
    "        # 4. Video Assembly\n",
    "        if video_clip_paths and audio_paths:\n",
    "            final_video_path = assemble_final_video(video_clip_paths, audio_paths, narration_scenes)\n",
    "        else:\n",
    "            print(\"Not enough assets generated to assemble video.\")\n",
    "\n",
    "        # 5. Output final info\n",
    "        if final_video_path:\n",
    "            print(\"\\n--- AUTOMATION COMPLETE ---\")\n",
    "            print(f\"Final Video: {final_video_path}\")\n",
    "            print(f\"Suggested Instagram Caption Text:\\n{' '.join(narration_scenes)}\")\n",
    "            print(f\"Suggested Hashtags: {hashtags}\")\n",
    "        else:\n",
    "            print(\"\\n--- AUTOMATION FAILED ---\")\n",
    "            print(\"Check logs for errors.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        # Ensure all models are cleared from VRAM if they were loaded\n",
    "        print(\"Cleaning up any remaining models from VRAM...\")\n",
    "        models_to_clear = [m for m in [llm, tts_model, t2i_pipe, refiner, i2v_pipe, t2v_pipe] if m is not None]\n",
    "        if models_to_clear:\n",
    "            clear_vram(*models_to_clear)\n",
    "        print(\"Cleanup finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17e056ba-c77b-4b51-9e47-16e4177d3429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7248b680250944568a43e3d588160bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating script and prompts for topic: Exploring ancient underwater cities\n",
      "LLM Response:\n",
      " \n",
      "    You are an AI assistant creating content for an Instagram Reel about \"Exploring ancient underwater cities\".\n",
      "    The Reel should be engaging and around 15-30 seconds long.\n",
      "    Provide the following:\n",
      "    1. Narration Script: A short, punchy narration. Break it into 2-3 short sentences or scenes.\n",
      "    2. Visual Prompts: For each sentence/scene in the narration, provide a detailed visual prompt suitable for an image or video generation model. Describe the scene, style, colors, and mood.\n",
      "    3. Hashtags: 3-5 relevant hashtags.\n",
      "\n",
      "    Format your response clearly, for example:\n",
      "\n",
      "    Narration Script:\n",
      "    Scene 1: [Sentence 1 for narration]\n",
      "    Scene 2: [Sentence 2 for narration]\n",
      "\n",
      "    Visual Prompts:\n",
      "    Scene 1: [Visual prompt for sentence 1, e.g., \"A vibrant, futuristic cityscape at sunset, Blade Runner style, cinematic lighting, detailed.\"]\n",
      "    Scene 2: [Visual prompt for sentence 2, e.g., \"Close up on a curious robot eye, glowing blue, intricate details, macro shot.\"]\n",
      "\n",
      "    Hashtags:\n",
      "    #tag1 #tag2 #tag3\n",
      "    \n",
      "    Example:\n",
      "    Narration Script:\n",
      "    Scene 1: \"Dive into the lost city of Atlantis.\"\n",
      "    Scene 2: \"Discover ancient ruins hidden beneath the waves.\"\n",
      "\n",
      "    Visual Prompts:\n",
      "    Scene 1: \"A stunning underwater landscape, with crystal-clear water, vibrant coral reefs, and a mysterious city in the distance, lit by the sun's rays. The camera zooms in on the city, revealing intricate details and ancient architecture.\"\n",
      "    Scene 2: \"A close-up shot of a stone pillar, covered in seaweed and barnacles, with a small school of fish swimming around it. The camera pans out to reveal a vast underwater ruin, with crumbling walls and overgrown vegetation.\"\n",
      "\n",
      "    Hashtags:\n",
      "    #underwatercities #ancientruins #marinearchaeology\n",
      "Error: Could not parse LLM response properly. Using fallback.\n",
      "VRAM cleared and memory collected.\n",
      "Loading TTS model...\n",
      "Generating audio for: A short segment about Exploring ancient underwater cities.\n",
      "Audio saved to instagram_content/scene_0_audio.wav\n",
      "Generating audio for: Concluding thoughts on the future.\n",
      "Audio saved to instagram_content/scene_1_audio.wav\n",
      "VRAM cleared and memory collected.\n",
      "Loading T2I pipeline (SDXL)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f4f12c67d248ac88a504bc87407837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading I2V pipeline (SVD)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e753d8e0ca0423e83bbc8ac2535ea1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating keyframe image for: Cinematic shot of Exploring ancient underwater cities, vibrant colors, high detail, trending on artstation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b68fc230e44c59b042ded82c6746c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyframe image saved to instagram_content/scene_0_keyframe.png\n",
      "Generating video from image using SVD...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223e7dad644b4333b48837668ced4f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD video clip saved to instagram_content/scene_0_svd.mp4\n",
      "Generating keyframe image for: Abstract representation of Exploring ancient underwater cities, thought-provoking, professional.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae391e72da5b4bf0ac648c83115cbf94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyframe image saved to instagram_content/scene_1_keyframe.png\n",
      "Generating video from image using SVD...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d55206e436444d0a46368a065736454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD video clip saved to instagram_content/scene_1_svd.mp4\n",
      "VRAM cleared and memory collected.\n",
      "Assembling final video...\n",
      "{'video_found': True, 'audio_found': False, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf61.1.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1024, 576], 'bitrate': 1130, 'fps': 7.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]', 'encoder': 'Lavc61.3.100 libx264'}}], 'input_number': 0}], 'duration': 3.57, 'bitrate': 1132, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1024, 576], 'video_bitrate': 1130, 'video_fps': 7.0, 'video_duration': 3.57, 'video_n_frames': 24}\n",
      "/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i instagram_content/scene_0_svd.mp4 -loglevel error -f image2pipe -vf scale=1024:576 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "{'video_found': True, 'audio_found': False, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf61.1.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1024, 576], 'bitrate': 1135, 'fps': 7.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]', 'encoder': 'Lavc61.3.100 libx264'}}], 'input_number': 0}], 'duration': 3.57, 'bitrate': 1137, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1024, 576], 'video_bitrate': 1135, 'video_fps': 7.0, 'video_duration': 3.57, 'video_n_frames': 24}\n",
      "/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i instagram_content/scene_1_svd.mp4 -loglevel error -f image2pipe -vf scale=1024:576 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "MoviePy - Building video instagram_content/final_reel.mp4.\n",
      "MoviePy - Writing audio in final_reelTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video instagram_content/final_reel.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame_index:  43%|████████▏          | 87/202 [00:11<00:15,  7.37it/s, now=None]/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/moviepy/video/io/ffmpeg_reader.py:178: UserWarning: In file instagram_content/scene_0_svd.mp4, 1769472 bytes wanted but 0 bytes read at frame index 25 (out of a total 24 frames), at time 3.57/3.57 sec. Using the last valid frame instead.\n",
      "  warnings.warn(\n",
      "frame_index:  45%|████████▌          | 91/202 [00:12<00:15,  7.09it/s, now=None]/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/moviepy/video/io/ffmpeg_reader.py:178: UserWarning: In file instagram_content/scene_0_svd.mp4, 1769472 bytes wanted but 0 bytes read at frame index 26 (out of a total 24 frames), at time 3.71/3.57 sec. Using the last valid frame instead.\n",
      "  warnings.warn(\n",
      "frame_index:  47%|████████▊          | 94/202 [00:12<00:15,  7.00it/s, now=None]/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/moviepy/video/io/ffmpeg_reader.py:178: UserWarning: In file instagram_content/scene_0_svd.mp4, 1769472 bytes wanted but 0 bytes read at frame index 27 (out of a total 24 frames), at time 3.86/3.57 sec. Using the last valid frame instead.\n",
      "  warnings.warn(\n",
      "frame_index:  48%|█████████          | 97/202 [00:13<00:14,  7.25it/s, now=None]/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/moviepy/video/io/ffmpeg_reader.py:178: UserWarning: In file instagram_content/scene_0_svd.mp4, 1769472 bytes wanted but 0 bytes read at frame index 28 (out of a total 24 frames), at time 4.00/3.57 sec. Using the last valid frame instead.\n",
      "  warnings.warn(\n",
      "frame_index:  50%|█████████         | 101/202 [00:13<00:13,  7.37it/s, now=None]/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/moviepy/video/io/ffmpeg_reader.py:178: UserWarning: In file instagram_content/scene_0_svd.mp4, 1769472 bytes wanted but 0 bytes read at frame index 29 (out of a total 24 frames), at time 4.14/3.57 sec. Using the last valid frame instead.\n",
      "  warnings.warn(\n",
      "frame_index:  51%|█████████▎        | 104/202 [00:14<00:13,  7.35it/s, now=None]/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/moviepy/video/io/ffmpeg_reader.py:178: UserWarning: In file instagram_content/scene_0_svd.mp4, 1769472 bytes wanted but 0 bytes read at frame index 30 (out of a total 24 frames), at time 4.29/3.57 sec. Using the last valid frame instead.\n",
      "  warnings.warn(\n",
      "frame_index:  96%|█████████████████▎| 194/202 [00:26<00:01,  7.40it/s, now=None]/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/moviepy/video/io/ffmpeg_reader.py:178: UserWarning: In file instagram_content/scene_1_svd.mp4, 1769472 bytes wanted but 0 bytes read at frame index 25 (out of a total 24 frames), at time 3.57/3.57 sec. Using the last valid frame instead.\n",
      "  warnings.warn(\n",
      "frame_index:  98%|█████████████████▌| 197/202 [00:26<00:00,  7.42it/s, now=None]/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/moviepy/video/io/ffmpeg_reader.py:178: UserWarning: In file instagram_content/scene_1_svd.mp4, 1769472 bytes wanted but 0 bytes read at frame index 26 (out of a total 24 frames), at time 3.71/3.57 sec. Using the last valid frame instead.\n",
      "  warnings.warn(\n",
      "frame_index: 100%|█████████████████▉| 201/202 [00:27<00:00,  7.39it/s, now=None]/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/moviepy/video/io/ffmpeg_reader.py:178: UserWarning: In file instagram_content/scene_1_svd.mp4, 1769472 bytes wanted but 0 bytes read at frame index 27 (out of a total 24 frames), at time 3.86/3.57 sec. Using the last valid frame instead.\n",
      "  warnings.warn(\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready instagram_content/final_reel.mp4\n",
      "Final video saved to instagram_content/final_reel.mp4\n",
      "\n",
      "--- AUTOMATION COMPLETE ---\n",
      "Final Video: instagram_content/final_reel.mp4\n",
      "Suggested Instagram Caption Text:\n",
      "A short segment about Exploring ancient underwater cities. Concluding thoughts on the future.\n",
      "Suggested Hashtags: #Exploringancientunderwatercities #AI #GeneratedContent\n",
      "Cleaning up any remaining models from VRAM...\n",
      "VRAM cleared and memory collected.\n",
      "Cleanup finished.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- IMPORTANT SETUP ---\n",
    "    # 1. Install dependencies:\n",
    "    #    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "    #    pip install transformers accelerate bitsandbytes diffusers TTS moviepy Pillow safetensors sentencepiece\n",
    "    #    (Make sure CUDA version in pytorch matches your system's CUDA Toolkit)\n",
    "    # 2. Download models:\n",
    "    #    The first time you run, Hugging Face models will download.\n",
    "    #    For Coqui TTS, ensure you have the model files (XTTSv2 needs manual download or will try).\n",
    "    # 3. For XTTSv2 voice cloning:\n",
    "    #    Set `speaker_reference_audio` to a path of a clean WAV file of the voice you want to clone.\n",
    "    #    It must be a 16-bit PCM WAV file, ideally >15 seconds long.\n",
    "    # 4. Fonts for MoviePy: Make sure 'Arial-Bold' or your chosen font is available to MoviePy/ImageMagick.\n",
    "    #    If not, ImageMagick might need to be installed and configured, or use a default font.\n",
    "\n",
    "    # --- RUN THE SCRIPT ---\n",
    "    # topic_for_reel = \"The impact of AI on creative arts\"\n",
    "    topic_for_reel = \"Exploring ancient underwater cities\"\n",
    "    # topic_for_reel = \"The secrets of black holes\"\n",
    "\n",
    "    # Set to True for SDXL -> SVD flow (better \"subject\" control)\n",
    "    # Set to False for direct Text-to-Video (ModelScope, simpler but maybe less coherent)\n",
    "    use_image_to_video_flow = True\n",
    "\n",
    "    # Ensure you have a speaker reference audio file if using XTTSv2\n",
    "    # Create a dummy file for testing if you don't have one, but quality will be default.\n",
    "    if not os.path.exists(\"record_out.wav\") and \"xtts\" in TTS_MODEL_ID.lower():\n",
    "        print(\"WARNING: XTTSv2 speaker reference audio not found. TTS might use a default voice or fail.\")\n",
    "        print(\"Please set 'speaker_reference_audio' in main_automation_flow to a valid .wav file.\")\n",
    "        # As a placeholder, you might allow it to run with default (if model supports) or skip audio:\n",
    "        # speaker_reference_audio = None # This might make XTTSv2 use its default speaker.\n",
    "\n",
    "    main_automation_flow(topic_for_reel, use_svd_flow=use_image_to_video_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0fce79-048c-4b48-9aee-f3d443d6d760",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
