{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "965b298e-2918-446f-b1d7-f8f7533ec025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988612fda3d04acbbcadc60f00157b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating script and prompts for topic (chat template): How beautiful the nature is!\n",
      "LLM Response (model generated part only with chat template):\n",
      " {\n",
      "  \"narration\": [\n",
      "    {\n",
      "      \"scene\": 1,\n",
      "      \"text\": \"Nature is a symphony of colors and sounds.\",\n",
      "      \"duration\": 3\n",
      "    },\n",
      "    {\n",
      "      \"scene\": 2,\n",
      "      \"text\": \"From the tallest trees to the smallest flowers, it's a sight to behold.\",\n",
      "      \"duration\": 3\n",
      "    },\n",
      "    {\n",
      "      \"scene\": 3,\n",
      "      \"text\": \"The sun sets and the stars come out, painting the sky in hues of orange and pink.\",\n",
      "      \"duration\": 3\n",
      "    },\n",
      "    {\n",
      "      \"scene\": 4,\n",
      "      \"text\": \"Let's take a moment to appreciate the beauty that surrounds us.\",\n",
      "      \"duration\": 3\n",
      "    },\n",
      "    {\n",
      "      \"scene\": 5,\n",
      "      \"text\": \"Nature is a gift we should cherish and protect.\",\n",
      "      \"duration\": 3\n",
      "    }\n",
      "  ],\n",
      "  \"visuals\": [\n",
      "    {\n",
      "      \"scene\": 1,\n",
      "      \"prompt\": \"A lush green forest with sunlight filtering through the leaves, birds chirping in the background. Use a wide-angle lens and saturated colors to capture the vibrancy of the scene. The style should be realistic.\"\n",
      "    },\n",
      "    {\n",
      "      \"scene\": 2,\n",
      "      \"prompt\": \"Close-up shots of flowers in various colors, with a shallow depth of field to blur the background and draw attention to the blooms. Use a macro lens and bright colors to emphasize the intricate details of each petal. The style should be cartoonish.\"\n",
      "    },\n",
      "    {\n",
      "      \"scene\": 3,\n",
      "      \"prompt\": \"A time-lapse of the sunset, with warm oranges and pinks filling the sky. Use a slow motion effect to capture the gradual transition of colors. The style should be realistic.\"\n",
      "    },\n",
      "    {\n",
      "      \"scene\": 4,\n",
      "      \"prompt\": \"A serene lake surrounded by trees, with a peaceful atmosphere. Use a drone camera to capture the vastness of the scene. The style should be gibli.\"\n",
      "    },\n",
      "    {\n",
      "      \"scene\": 5,\n",
      "      \"prompt\": \"A group of volunteers cleaning up a beach, picking up litter and planting trees. Use a documentary-style camera to capture the impact of human actions on nature. The style should be realistic.\"\n",
      "    }\n",
      "  ],\n",
      "  \"hashtags\": [\n",
      "    \"NatureLovers\",\n",
      "    \"BeautifulEarth\",\n",
      "    \"ProtectThePlanet\"\n",
      "  ]\n",
      "}\n",
      "Warning: Total duration 15.0s differs from target 30.0s\n",
      "VRAM cleared and memory collected.\n",
      "Loading TTS model...\n",
      "Generating audio for: Nature is a symphony of colors and sounds.\n",
      "Audio saved to instagram_content/scene_0_audio.wav\n",
      "Generating audio for: From the tallest trees to the smallest flowers, it's a sight to behold.\n",
      "Audio saved to instagram_content/scene_1_audio.wav\n",
      "Generating audio for: The sun sets and the stars come out, painting the sky in hues of orange and pink.\n",
      "Audio saved to instagram_content/scene_2_audio.wav\n",
      "Generating audio for: Let's take a moment to appreciate the beauty that surrounds us.\n",
      "Audio saved to instagram_content/scene_3_audio.wav\n",
      "Generating audio for: Nature is a gift we should cherish and protect.\n",
      "Audio saved to instagram_content/scene_4_audio.wav\n",
      "VRAM cleared and memory collected.\n",
      "Loading T2I pipeline (SDXL)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffbd1eefa27949e5933b6b242cd39a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading I2V pipeline (SVD)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5227bf13ab964e73938093b0c396a8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9356417de7ea4f1aa52f3c55c40bba11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6cf3ca6b3a43a98874300322f1ac9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558ba3af3e634818a38e018a2a49fea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/19.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023dc03c24a44fae94bf1038b78560e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/412 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36286b93d98d4a13bd396c891ffde8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5527b65fa70e459ea72488ee52405146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b8db367e454a22b6715652b59d3228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66272c28473b45628a5cf951d5ca4f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/781 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae179c2eac0f46fdbe013f481cd73048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)ion_pytorch_model.safetensors.index.json:   0%|          | 0.00/72.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b396264966642cfa69d851070fa650d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/20.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1791560fb434c12861231683421e211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating keyframe image for: A lush green forest with sunlight filtering through the leaves, birds chirping in the background. Use a wide-angle lens and saturated colors to capture the vibrancy of the scene. The style should be realistic.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43583d2d57f4d148e87e311c6db561b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyframe image saved to instagram_content/scene_0_keyframe.png\n",
      "Generating video from image using SVD...\n",
      "An error occurred: LTXImageToVideoPipeline.__call__() got an unexpected keyword argument 'decode_chunk_size'\n",
      "Cleaning up any remaining models from VRAM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_31194/954474740.py\", line 508, in main_automation_flow\n",
      "    clip_path = generate_image_then_video(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31194/954474740.py\", line 298, in generate_image_then_video\n",
      "    video_frames = i2v_pipe(\n",
      "                   ^^^^^^^^^\n",
      "  File \"/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: LTXImageToVideoPipeline.__call__() got an unexpected keyword argument 'decode_chunk_size'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VRAM cleared and memory collected.\n",
      "Cleanup finished.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# main_script.py\n",
    "import os\n",
    "# Set PyTorch memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, T5EncoderModel, T5Tokenizer\n",
    "from diffusers import StableDiffusionXLPipeline, StableVideoDiffusionPipeline, DiffusionPipeline, LTXImageToVideoPipeline\n",
    "from diffusers.utils import load_image, export_to_video\n",
    "from TTS.api import TTS # Coqui TTS\n",
    "from moviepy import VideoFileClip, AudioFileClip, concatenate_videoclips, TextClip, CompositeVideoClip\n",
    "from moviepy.audio.AudioClip import concatenate_audioclips, AudioClip\n",
    "import gc # Garbage collection\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ContentConfig:\n",
    "    \"\"\"Configuration for content generation\"\"\"\n",
    "    # Video settings\n",
    "    target_video_length: float = 30.0  # Target total video length in seconds\n",
    "    max_scene_length: float = 3.0      # Maximum length of each scene in seconds\n",
    "    target_resolution: tuple = (1080, 1920)  # Instagram Reel 9:16\n",
    "    fps: int = 8\n",
    "    \n",
    "    # Scene settings\n",
    "    min_scenes: int = 2\n",
    "    max_scenes: int = 3\n",
    "    \n",
    "    # Model settings\n",
    "    use_svd_flow: bool = True  # Use SDXL -> SVD flow instead of direct T2V\n",
    "    \n",
    "    # Output settings\n",
    "    output_dir: str = \"instagram_content\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- 0. CONFIGURATION ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "OUTPUT_DIR = \"instagram_content\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# LLM paths (example, download these first)\n",
    "LLM_MODEL_ID = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "# TTS model (XTTSv2 example)\n",
    "TTS_MODEL_ID = \"tts_models/multilingual/multi-dataset/xtts_v2\" # or \"tts_models/en/ljspeech/tacotron2-DDC\"\n",
    "# T2I model\n",
    "T2I_MODEL_ID = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "T2I_REFINER_ID = \"stabilityai/stable-diffusion-xl-refiner-1.0\"\n",
    "# I2V model (SVD)\n",
    "I2V_MODEL_ID = \"stabilityai/stable-video-diffusion-img2vid-xt\"\n",
    "# T2V model (alternative)\n",
    "T2V_MODEL_ID = \"THUDM/CogVideoX-5b\"\n",
    "\n",
    "\n",
    "\n",
    "# --- 1. INITIALIZE MODELS (Load only when needed or keep loaded if VRAM allows) ---\n",
    "def load_llm():\n",
    "    print(\"Loading LLM...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_ID)\n",
    "    # Ensure pad_token is set if model doesn't have one; often same as eos_token for CausalLMs\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        LLM_MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    # We won't use the pipeline directly for chat templating, but generate manually\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def load_tts():\n",
    "    print(\"Loading TTS model...\")\n",
    "    # Make sure you have the model downloaded or it will download on first run\n",
    "    # For XTTSv2, you might need to specify a speaker_wav for voice cloning\n",
    "    return TTS(model_name=TTS_MODEL_ID, progress_bar=True).to(DEVICE)\n",
    "\n",
    "def load_t2i_pipeline():\n",
    "    print(\"Loading T2I pipeline (SDXL)...\")\n",
    "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "        T2I_MODEL_ID, torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    "    ).to(DEVICE)\n",
    "    # Optional: Load refiner if you want to use it\n",
    "    # refiner = DiffusionPipeline.from_pretrained(\n",
    "    #     T2I_REFINER_ID, text_encoder_2=pipe.text_encoder_2, vae=pipe.vae,\n",
    "    #     torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\"\n",
    "    # ).to(DEVICE)\n",
    "    # return pipe, refiner\n",
    "    return pipe, None # Simpler for now\n",
    "\n",
    "def load_i2v_pipeline(): # SVD\n",
    "    print(\"Loading I2V pipeline (SVD)...\")\n",
    "    single_file_url = \"https://huggingface.co/Lightricks/LTX-Video/ltx-video-2b-v0.9.safetensors\"\n",
    "    text_encoder = T5EncoderModel.from_pretrained(\n",
    "      \"Lightricks/LTX-Video\", subfolder=\"text_encoder\", torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\n",
    "      \"Lightricks/LTX-Video\", subfolder=\"tokenizer\", torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    pipe = LTXImageToVideoPipeline.from_single_file(\n",
    "      single_file_url, text_encoder=text_encoder, tokenizer=tokenizer, torch_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    pipe.enable_model_cpu_offload()\n",
    "\n",
    "    return pipe\n",
    "\n",
    "def load_t2v_pipeline(): # ModelScope Text-to-Video\n",
    "    print(\"Loading T2V pipeline (ModelScope)...\")\n",
    "    # Use DiffusionPipeline as per the documentation for damo-vilab/text-to-video-ms-1.7b\n",
    "    pipe = DiffusionPipeline.from_pretrained(\n",
    "        T2V_MODEL_ID, torch_dtype=torch.float16\n",
    "        # , variant=\"fp16\"\n",
    "    ).to(DEVICE)\n",
    "    # The .enable_model_cpu_offload() should still work if the loaded pipe supports it\n",
    "    # For ModelScope, this is standard.\n",
    "    pipe.enable_model_cpu_offload()\n",
    "    return pipe\n",
    "\n",
    "\n",
    "\n",
    "# --- UTILITY TO CLEAR VRAM ---\n",
    "def clear_vram(*models_or_pipelines):\n",
    "    for item in models_or_pipelines:\n",
    "        if hasattr(item, 'cpu') and callable(getattr(item, 'cpu')):\n",
    "            item.cpu() # If it's a pipeline/model with a .cpu() method\n",
    "        elif hasattr(item, 'model') and hasattr(item.model, 'cpu') and callable(getattr(item.model, 'cpu')):\n",
    "            item.model.cpu()\n",
    "    del models_or_pipelines\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"VRAM cleared and memory collected.\")\n",
    "\n",
    "\n",
    "\n",
    "# --- 2. TEXT GENERATION ---\n",
    "def generate_script_and_prompts_with_chat_template(topic, model, tokenizer, config: ContentConfig):\n",
    "    print(f\"Generating script and prompts for topic (chat template): {topic}\")\n",
    "\n",
    "    # Define the messages for the chat template\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an AI assistant creating content for an Instagram Reel. Your response must be in valid JSON format with the following structure: {\\\"narration\\\": [{\\\"scene\\\": 1, \\\"text\\\": \\\"text\\\", \\\"duration\\\": seconds}], \\\"visuals\\\": [{\\\"scene\\\": 1, \\\"prompt\\\": \\\"prompt\\\"}], \\\"hashtags\\\": [\\\"tag1\\\", \\\"tag2\\\"]}\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "            Create content for an Instagram Reel about \"{topic}\".\n",
    "            The Reel should be engaging and around {config.target_video_length} seconds long.\n",
    "            Each scene should be around {config.max_scene_length} seconds.\n",
    "            Generate between {config.min_scenes} and {config.max_scenes} scenes.\n",
    "            \n",
    "            Return your response in this exact JSON format:\n",
    "            {{\n",
    "                \"narration\": [\n",
    "                    {{\"scene\": 1, \"text\": \"First scene narration\", \"duration\": 3.0}},\n",
    "                    {{\"scene\": 2, \"text\": \"Second scene narration\", \"duration\": 3.0}}\n",
    "                ],\n",
    "                \"visuals\": [\n",
    "                    {{\"scene\": 1, \"prompt\": \"Detailed visual prompt for scene 1, with camera apurture to details of object and actors in scene, also tell the style if its gibli, realistic or cartoon\"}},\n",
    "                    {{\"scene\": 2, \"prompt\": \"Detailed visual prompt for scene 2, with camera apurture to details of object and actors in scene, also tell the style if its gibli, realistic or cartoon\"}}\n",
    "                ],\n",
    "                \"hashtags\": [\"tag1\", \"tag2\", \"tag3\"]\n",
    "            }}\n",
    "            \n",
    "            Make sure the total duration of all scenes matches approximately {config.target_video_length} seconds.\n",
    "            Each visual prompt should be detailed and suitable for image/video generation.\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Apply the chat template\n",
    "    tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate response\n",
    "    generation_kwargs = {\n",
    "        \"input_ids\": tokenized_chat,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"do_sample\": True,\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.95,\n",
    "        \"temperature\": 0.7,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "\n",
    "    outputs = model.generate(**generation_kwargs)\n",
    "    decoded_output = tokenizer.decode(outputs[0][tokenized_chat.shape[-1]:], skip_special_tokens=True)\n",
    "    print(\"LLM Response (model generated part only with chat template):\\n\", decoded_output)\n",
    "\n",
    "    try:\n",
    "        # Try to parse the JSON response\n",
    "        response_data = json.loads(decoded_output)\n",
    "        \n",
    "        # Extract and validate the data\n",
    "        narration_scenes = []\n",
    "        visual_prompts = []\n",
    "        hashtags = []\n",
    "        \n",
    "        # Sort scenes by scene number to ensure correct order\n",
    "        narration_data = sorted(response_data.get(\"narration\", []), key=lambda x: x[\"scene\"])\n",
    "        visuals_data = sorted(response_data.get(\"visuals\", []), key=lambda x: x[\"scene\"])\n",
    "        \n",
    "        for scene in narration_data:\n",
    "            narration_scenes.append({\n",
    "                \"text\": scene[\"text\"],\n",
    "                \"duration\": float(scene.get(\"duration\", config.max_scene_length))\n",
    "            })\n",
    "            \n",
    "        for scene in visuals_data:\n",
    "            visual_prompts.append(scene[\"prompt\"])\n",
    "            \n",
    "        hashtags = response_data.get(\"hashtags\", [])\n",
    "        \n",
    "        # Validate we have matching number of scenes\n",
    "        if len(narration_scenes) != len(visual_prompts):\n",
    "            raise ValueError(\"Mismatch between number of narration and visual scenes\")\n",
    "            \n",
    "        # Validate scene count\n",
    "        if not (config.min_scenes <= len(narration_scenes) <= config.max_scenes):\n",
    "            raise ValueError(f\"Scene count {len(narration_scenes)} outside allowed range [{config.min_scenes}, {config.max_scenes}]\")\n",
    "            \n",
    "        # Validate total duration\n",
    "        total_duration = sum(scene[\"duration\"] for scene in narration_scenes)\n",
    "        if abs(total_duration - config.target_video_length) > 5:  # Allow 5 second tolerance\n",
    "            print(f\"Warning: Total duration {total_duration}s differs from target {config.target_video_length}s\")\n",
    "            \n",
    "    except (json.JSONDecodeError, ValueError) as e:\n",
    "        print(f\"Error parsing LLM response: {e}\")\n",
    "        print(\"Using fallback content...\")\n",
    "        # Fallback content\n",
    "        narration_scenes = [\n",
    "            {\"text\": f\"A short segment about {topic}.\", \"duration\": config.max_scene_length},\n",
    "            {\"text\": \"Concluding thoughts on the future.\", \"duration\": config.max_scene_length}\n",
    "        ]\n",
    "        visual_prompts = [\n",
    "            f\"Cinematic shot of {topic}, vibrant colors, high detail, trending on artstation.\",\n",
    "            f\"Abstract representation of {topic}, thought-provoking, professional.\"\n",
    "        ]\n",
    "        hashtags = [f\"#{topic.replace(' ', '')}\", \"#AI\", \"#GeneratedContent\"]\n",
    "\n",
    "    return narration_scenes, visual_prompts, hashtags\n",
    "\n",
    "\n",
    "\n",
    "# --- 3. AUDIO GENERATION ---\n",
    "def generate_audio(text, tts_model, output_path, speaker_wav=None): # speaker_wav for XTTS\n",
    "    print(f\"Generating audio for: {text}\")\n",
    "    if \"xtts\" in TTS_MODEL_ID.lower() and speaker_wav:\n",
    "        tts_model.tts_to_file(text, speaker_wav=speaker_wav, language=\"en\", file_path=output_path)\n",
    "    else: # For other models like Tacotron\n",
    "        tts_model.tts_to_file(text, file_path=output_path)\n",
    "    print(f\"Audio saved to {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "\n",
    "\n",
    "# --- 4. VISUAL GENERATION ---\n",
    "# Option A: Image (SDXL) then Video (SVD) - Recommended for \"Subject to Video\"\n",
    "def generate_image_then_video(image_prompt, i2v_pipe, t2i_pipe, refiner_pipe, scene_idx, target_duration, config: ContentConfig):\n",
    "    # Generate Image (Keyframe)\n",
    "    print(f\"Generating keyframe image for: {image_prompt}\")\n",
    "    # For SDXL, you can add negative prompts, specify num_inference_steps, guidance_scale etc.\n",
    "    # Using SDXL\n",
    "    image = t2i_pipe(\n",
    "        prompt=image_prompt,\n",
    "        # negative_prompt=\"low quality, blurry, watermark\", # Example\n",
    "        num_inference_steps=30, # SDXL typically needs fewer steps\n",
    "        guidance_scale=7.5,\n",
    "        # If using refiner:\n",
    "        # output_type=\"latent\" if refiner_pipe else \"pil\",\n",
    "    ).images[0]\n",
    "    # if refiner_pipe:\n",
    "    #     image = refiner_pipe(prompt=image_prompt, image=image[None, :]).images[0]\n",
    "\n",
    "    image_path = os.path.join(config.output_dir, f\"scene_{scene_idx}_keyframe.png\")\n",
    "    image.save(image_path)\n",
    "    print(f\"Keyframe image saved to {image_path}\")\n",
    "\n",
    "    # Calculate number of frames needed based on target duration\n",
    "    num_frames = min(int(target_duration * config.fps), 32)  # Limit to 16 frames max\n",
    "    # Ensure we have at least 8 frames (minimum for SVD)\n",
    "    num_frames = max(8, num_frames)\n",
    "    \n",
    "    # Generate Video from Image (SVD)\n",
    "    print(f\"Generating video from image using SVD...\")\n",
    "    # SVD parameters\n",
    "    video_frames = i2v_pipe(\n",
    "        image,\n",
    "        decode_chunk_size=4,  # Reduced from 8 to 4\n",
    "        num_frames=num_frames,\n",
    "        motion_bucket_id=127, # Adjust for more/less motion\n",
    "        fps=config.fps,\n",
    "        noise_aug_strength=0.02 # Default\n",
    "    ).frames[0]\n",
    "\n",
    "    video_clip_path = os.path.join(config.output_dir, f\"scene_{scene_idx}_svd.mp4\")\n",
    "    export_to_video(video_frames, video_clip_path, fps=config.fps)\n",
    "    print(f\"SVD video clip saved to {video_clip_path}\")\n",
    "    return video_clip_path\n",
    "\n",
    "# Option B: Direct Text-to-Video (ModelScope)\n",
    "def generate_direct_video(video_prompt, t2v_pipe, scene_idx, target_duration, config: ContentConfig):\n",
    "    print(f\"Generating direct video for: {video_prompt}\")\n",
    "    # Calculate number of frames needed based on target duration\n",
    "    num_frames = int(target_duration * config.fps)\n",
    "    # Ensure we have at least 8 frames\n",
    "    num_frames = max(8, num_frames)\n",
    "    \n",
    "    # ModelScope parameters\n",
    "    # video_frames = t2v_pipe(video_prompt, num_inference_steps=25, num_frames=num_frames).frames[0]\n",
    "    video_clip_path = os.path.join(config.output_dir, f\"scene_{scene_idx}_t2v.mp4\")\n",
    "    # export_to_video(video_frames, video_clip_path, fps=config.fps)\n",
    "    # print(f\"T2V video clip saved to {video_clip_path}\")\n",
    "\n",
    "    negative_prompt = \"worst quality, inconsistent motion, blurry, jittery, distorted\"\n",
    "\n",
    "    video = t2v_pipe(\n",
    "        prompt=video_prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        width=704,\n",
    "        height=480,\n",
    "        num_frames=num_frames,\n",
    "        num_inference_steps=25,\n",
    "    ).frames[0]\n",
    "    export_to_video(video, video_clip_path, fps=config.fps)\n",
    "    print(f\"T2V video clip saved to {video_clip_path}\")\n",
    "    return video_clip_path\n",
    "\n",
    "\n",
    "\n",
    "# --- 5. VIDEO ASSEMBLY ---\n",
    "def assemble_final_video(video_clip_paths, audio_clip_paths, narration_parts, config: ContentConfig, output_filename=\"final_reel.mp4\"):\n",
    "    print(\"Assembling final video...\")\n",
    "    final_clips = []\n",
    "    source_clips_to_close = []\n",
    "\n",
    "    # Use a specific, existing font path\n",
    "    font_path_for_textclip = \"/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf\"\n",
    "    if not os.path.exists(font_path_for_textclip):\n",
    "        print(f\"Warning: Font file not found at: {font_path_for_textclip}\")\n",
    "        print(\"Using system default font...\")\n",
    "        font_path_for_textclip = None\n",
    "\n",
    "    for i, (video_path, audio_path, narration) in enumerate(zip(video_clip_paths, audio_clip_paths, narration_parts)):\n",
    "        # Load video and audio clips\n",
    "        video_clip_temp = VideoFileClip(video_path)\n",
    "        audio_clip_temp = AudioFileClip(audio_path)\n",
    "        source_clips_to_close.extend([video_clip_temp, audio_clip_temp])\n",
    "\n",
    "        # Get durations\n",
    "        video_duration = video_clip_temp.duration\n",
    "        audio_duration = audio_clip_temp.duration\n",
    "        target_duration = narration[\"duration\"]\n",
    "\n",
    "        # Resize video to target resolution\n",
    "        video_clip_resized = video_clip_temp.resized(height=config.target_resolution[1])\n",
    "        if video_clip_resized.w > config.target_resolution[0]:\n",
    "            video_clip_final_shape = video_clip_resized.cropped(x_center=video_clip_resized.w/2, width=config.target_resolution[0])\n",
    "        else:\n",
    "            video_clip_final_shape = video_clip_resized\n",
    "\n",
    "        # Position video in center\n",
    "        video_clip_positioned = video_clip_final_shape.with_position('center')\n",
    "\n",
    "        # Handle duration mismatches\n",
    "        if video_duration > target_duration:\n",
    "            # If video is longer, trim it\n",
    "            video_clip_timed = video_clip_positioned.subclipped(0, target_duration)\n",
    "        else:\n",
    "            # If video is shorter, loop it\n",
    "            n_loops = int(np.ceil(target_duration / video_duration))\n",
    "            video_clip_timed = concatenate_videoclips([video_clip_positioned] * n_loops)\n",
    "            video_clip_timed = video_clip_timed.subclipped(0, target_duration)\n",
    "\n",
    "        # Handle audio duration\n",
    "        if audio_duration > target_duration:\n",
    "            # If audio is longer, trim it\n",
    "            audio_clip_timed = audio_clip_temp.subclipped(0, target_duration)\n",
    "        else:\n",
    "            # If audio is shorter, pad with silence\n",
    "            silence_duration = target_duration - audio_duration\n",
    "            # Create a silent audio clip (zero signal) with specific duration\n",
    "            silence = AudioClip(frame_function=lambda t: 0, duration=silence_duration)\n",
    "            audio_clip_timed = concatenate_audioclips([audio_clip_temp, silence])\n",
    "\n",
    "        # Combine video and audio\n",
    "        video_clip_with_audio = video_clip_timed.with_audio(audio_clip_timed)\n",
    "\n",
    "        # Add text caption\n",
    "        txt_clip_temp = TextClip(\n",
    "            font_path_for_textclip,\n",
    "            text=narration[\"text\"],\n",
    "            font_size=60,\n",
    "            color='white',\n",
    "            stroke_color='black',\n",
    "            stroke_width=2,\n",
    "            method='caption',\n",
    "            size=(int(config.target_resolution[0]*0.8), None)\n",
    "        )\n",
    "        source_clips_to_close.append(txt_clip_temp)\n",
    "\n",
    "        txt_clip_final = txt_clip_temp.with_position(('center', 0.8), relative=True).with_duration(target_duration)\n",
    "\n",
    "        # Combine video and text\n",
    "        scene_composite = CompositeVideoClip([video_clip_with_audio, txt_clip_final], size=config.target_resolution)\n",
    "        final_clips.append(scene_composite)\n",
    "\n",
    "    if not final_clips:\n",
    "        print(\"No clips to assemble!\")\n",
    "        for clip_to_close in source_clips_to_close:\n",
    "            if hasattr(clip_to_close, 'close') and callable(getattr(clip_to_close, 'close')):\n",
    "                clip_to_close.close()\n",
    "        return None\n",
    "\n",
    "    # Concatenate all scenes\n",
    "    final_video = concatenate_videoclips(final_clips, method=\"compose\")\n",
    "    final_video_path = os.path.join(config.output_dir, output_filename)\n",
    "\n",
    "    try:\n",
    "        final_video.write_videofile(\n",
    "            final_video_path,\n",
    "            fps=config.fps,\n",
    "            codec=\"libx264\",\n",
    "            audio_codec=\"aac\",\n",
    "            threads=4,\n",
    "            preset=\"medium\",\n",
    "            logger='bar'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during video writing: {e}\")\n",
    "        print(\"Make sure ffmpeg is correctly installed and accessible by MoviePy.\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    finally:\n",
    "        # Clean up all clips\n",
    "        for clip_to_close in source_clips_to_close:\n",
    "            if hasattr(clip_to_close, 'close') and callable(getattr(clip_to_close, 'close')):\n",
    "                try:\n",
    "                    clip_to_close.close()\n",
    "                except Exception as e_close:\n",
    "                    print(f\"Error closing clip {type(clip_to_close)}: {e_close}\")\n",
    "        if hasattr(final_video, 'close') and callable(getattr(final_video, 'close')):\n",
    "            try:\n",
    "                final_video.close()\n",
    "            except Exception as e_close:\n",
    "                print(f\"Error closing final_video: {e_close}\")\n",
    "\n",
    "    print(f\"Final video saved to {final_video_path}\")\n",
    "    return final_video_path\n",
    "\n",
    "\n",
    "\n",
    "# --- MAIN WORKFLOW ---\n",
    "def main_automation_flow(topic, config: Optional[ContentConfig] = None):\n",
    "    if config is None:\n",
    "        config = ContentConfig()\n",
    "\n",
    "    # --- Load models ---\n",
    "    # Manage VRAM: Load one by one or use .cpu_offload()\n",
    "    llm = None\n",
    "    tts_model = None\n",
    "    t2i_pipe, refiner = None, None\n",
    "    i2v_pipe = None\n",
    "    t2v_pipe = None\n",
    "    final_video_path = None\n",
    "    speaker_reference_audio = \"record_out.wav\" # REQUIRED for XTTS voice cloning\n",
    "\n",
    "    try:\n",
    "        # 1. LLM for Script and Prompts\n",
    "        llm_model, llm_tokenizer = load_llm()\n",
    "        narration_scenes, visual_prompts_scenes, hashtags = generate_script_and_prompts_with_chat_template(\n",
    "            topic, llm_model, llm_tokenizer, config\n",
    "        )\n",
    "        clear_vram(llm_model)\n",
    "\n",
    "        # 2. TTS for Narration\n",
    "        tts_model = load_tts()\n",
    "        audio_paths = []\n",
    "        for i, scene in enumerate(narration_scenes):\n",
    "            audio_file = os.path.join(config.output_dir, f\"scene_{i}_audio.wav\")\n",
    "            generate_audio(\n",
    "                scene[\"text\"], \n",
    "                tts_model, \n",
    "                audio_file, \n",
    "                speaker_wav=speaker_reference_audio if \"xtts\" in TTS_MODEL_ID.lower() else None\n",
    "            )\n",
    "            audio_paths.append(audio_file)\n",
    "        clear_vram(tts_model)\n",
    "\n",
    "        # 3. Visual Generation\n",
    "        video_clip_paths = []\n",
    "        if config.use_svd_flow: # Image (SDXL) -> Video (SVD)\n",
    "            t2i_pipe, refiner = load_t2i_pipeline() # SDXL\n",
    "            i2v_pipe = load_i2v_pipeline()         # SVD\n",
    "            for i, (visual_prompt, scene) in enumerate(zip(visual_prompts_scenes, narration_scenes)):\n",
    "                clip_path = generate_image_then_video(\n",
    "                    visual_prompt, \n",
    "                    i2v_pipe, \n",
    "                    t2i_pipe, \n",
    "                    refiner, \n",
    "                    i,\n",
    "                    scene[\"duration\"],\n",
    "                    config\n",
    "                )\n",
    "                video_clip_paths.append(clip_path)\n",
    "            clear_vram(t2i_pipe, refiner, i2v_pipe)\n",
    "        else: # Direct Text-to-Video (ModelScope)\n",
    "            t2v_pipe = load_t2v_pipeline()\n",
    "            for i, (visual_prompt, scene) in enumerate(zip(visual_prompts_scenes, narration_scenes)):\n",
    "                clip_path = generate_direct_video(\n",
    "                    visual_prompt, \n",
    "                    t2v_pipe, \n",
    "                    i,\n",
    "                    scene[\"duration\"],\n",
    "                    config\n",
    "                )\n",
    "                video_clip_paths.append(clip_path)\n",
    "            clear_vram(t2v_pipe)\n",
    "\n",
    "        # 4. Video Assembly\n",
    "        if video_clip_paths and audio_paths:\n",
    "            final_video_path = assemble_final_video(\n",
    "                video_clip_paths, \n",
    "                audio_paths, \n",
    "                narration_scenes,\n",
    "                config\n",
    "            )\n",
    "        else:\n",
    "            print(\"Not enough assets generated to assemble video.\")\n",
    "\n",
    "        # 5. Output final info\n",
    "        if final_video_path:\n",
    "            print(\"\\n--- AUTOMATION COMPLETE ---\")\n",
    "            print(f\"Final Video: {final_video_path}\")\n",
    "            print(f\"Suggested Instagram Caption Text:\\n{' '.join([scene['text'] for scene in narration_scenes])}\")\n",
    "            print(f\"Suggested Hashtags: {', '.join(hashtags)}\")\n",
    "        else:\n",
    "            print(\"\\n--- AUTOMATION FAILED ---\")\n",
    "            print(\"Check logs for errors.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        # Ensure all models are cleared from VRAM if they were loaded\n",
    "        print(\"Cleaning up any remaining models from VRAM...\")\n",
    "        models_to_clear = [m for m in [llm, tts_model, t2i_pipe, refiner, i2v_pipe, t2v_pipe] if m is not None]\n",
    "        if models_to_clear:\n",
    "            clear_vram(*models_to_clear)\n",
    "        print(\"Cleanup finished.\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- IMPORTANT SETUP ---\n",
    "    # 1. Install dependencies:\n",
    "    #    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "    #    pip install transformers accelerate bitsandbytes diffusers TTS moviepy Pillow safetensors sentencepiece\n",
    "    #    (Make sure CUDA version in pytorch matches your system's CUDA Toolkit)\n",
    "    # 2. Download models:\n",
    "    #    The first time you run, Hugging Face models will download.\n",
    "    #    For Coqui TTS, ensure you have the model files (XTTSv2 needs manual download or will try).\n",
    "    # 3. For XTTSv2 voice cloning:\n",
    "    #    Set `speaker_reference_audio` to a path of a clean WAV file of the voice you want to clone.\n",
    "    #    It must be a 16-bit PCM WAV file, ideally >15 seconds long.\n",
    "    # 4. Fonts for MoviePy: Make sure 'Arial-Bold' or your chosen font is available to MoviePy/ImageMagick.\n",
    "    #    If not, ImageMagick might need to be installed and configured, or use a default font.\n",
    "\n",
    "    # --- RUN THE SCRIPT ---\n",
    "    # Example configuration\n",
    "    config = ContentConfig(\n",
    "        target_video_length=30.0,  # 30 seconds total\n",
    "        max_scene_length=3.0,      # 3 seconds per scene\n",
    "        min_scenes=2,\n",
    "        max_scenes=5,\n",
    "        use_svd_flow=True,         # Use SDXL -> SVD flow\n",
    "        fps=8\n",
    "    )\n",
    "\n",
    "    topic_for_reel = \"How beautiful the nature is!\"\n",
    "    main_automation_flow(topic_for_reel, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a6e59c-e1a9-4364-bb1c-6686818194b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
