{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "965b298e-2918-446f-b1d7-f8f7533ec025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181e91e0d09046eca8c87820bcc4754e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating script and prompts for topic (chat template): An 80 years old person guiding young generation.\n",
      "LLM Response (model generated part only with chat template):\n",
      " {\n",
      "  \"narration\": [\n",
      "    {\n",
      "      \"scene\": 1,\n",
      "      \"text\": \"Meet Grandpa George, a wise elder sharing his secrets with the young generation.\",\n",
      "      \"duration\": 3.0\n",
      "    },\n",
      "    {\n",
      "      \"scene\": 2,\n",
      "      \"text\": \"From gardening to cooking, Grandpa George teaches us the art of living sustainably.\",\n",
      "      \"duration\": 3.0\n",
      "    },\n",
      "    {\n",
      "      \"scene\": 3,\n",
      "      \"text\": \"Let's follow Grandpa George's lead and reduce our carbon footprint. Together, we can make a difference.\",\n",
      "      \"duration\": 3.0\n",
      "    },\n",
      "    {\n",
      "      \"scene\": 4,\n",
      "      \"text\": \"Grandpa George's wisdom extends beyond just the environment. He teaches us the importance of kindness, compassion, and respect.\",\n",
      "      \"duration\": 3.0\n",
      "    },\n",
      "    {\n",
      "      \"scene\": 5,\n",
      "      \"text\": \"Join us as we honor Grandpa George's legacy and carry his values into the future.\",\n",
      "      \"duration\": 3.0\n",
      "    }\n",
      "  ],\n",
      "  \"visuals\": [\n",
      "    {\n",
      "      \"scene\": 1,\n",
      "      \"prompt\": \"Grandpa George, a sprightly 80-year-old, sits on a porch swing surrounded by lush greenery. He wears a fedora and a vest, and his eyes twinkle with wisdom. The camera zooms in on his face as he speaks. The style is realistic.\",\n",
      "    },\n",
      "    {\n",
      "      \"scene\": 2,\n",
      "      \"prompt\": \"Grandpa George demonstrates how to grow vegetables in a small garden. He uses natural fertilizers and shows how to compost kitchen scraps. The camera follows his movements closely, capturing every detail. The style is realistic.\",\n",
      "    },\n",
      "    {\n",
      "      \"scene\": 3,\n",
      "      \"prompt\": \"Grandpa George sorts through recyclables and explains the importance of reducing waste. He shows how to make reusable bags from old t-shirts. The camera pans out to show the vast expanse of greenery behind him. The style is realistic.\",\n",
      "    },\n",
      "    {\n",
      "      \"scene\": 4,\n",
      "      \"prompt\": \"Grandpa George sits at a small table, surrounded by children. He reads a book aloud and encourages the kids to ask questions. The camera focuses on the children's faces as they listen intently. The style is cartoonish, with bright colors and exaggerated expressions.\",\n",
      "    },\n",
      "    {\n",
      "      \"scene\": 5,\n",
      "      \"prompt\": \"Grandpa George stands in front of a large crowd, dressed in a suit and tie. He speaks passionately about the need for kindness, compassion, and respect. The camera shows close-ups of the faces in the crowd, some with tears in their eyes. The style is realistic.\",\n",
      "    }\n",
      "  ],\n",
      "  \"hashtags\": [\n",
      "    \"#GrandpaGeorge\",\n",
      "    \"#Wisdom\",\n",
      "    \"#EcoFriendly\",\n",
      "    \"#Kindness\",\n",
      "    \"#Respect\"\n",
      "  ]\n",
      "}\n",
      "Error parsing LLM response: Expecting property name enclosed in double quotes: line 33 column 5 (char 1140)\n",
      "Using fallback content...\n",
      "VRAM cleared and memory collected.\n",
      "Loading TTS model...\n",
      "Generating audio for: A short segment about An 80 years old person guiding young generation..\n",
      "Audio saved to instagram_content/scene_0_audio.wav\n",
      "Generating audio for: Concluding thoughts on the future.\n",
      "Audio saved to instagram_content/scene_1_audio.wav\n",
      "VRAM cleared and memory collected.\n",
      "Loading T2I pipeline (SDXL)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71eb651a04b04fa6ad1f02a0c0844558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading I2V pipeline (SVD)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e240cf4aa2c41e1bc6ff05148b90c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating keyframe image for: Cinematic shot of An 80 years old person guiding young generation., vibrant colors, high detail, trending on artstation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58cfd78df0f640f690b32440ec1fc2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyframe image saved to instagram_content/scene_0_keyframe.png\n",
      "Generating video from image using SVD...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2c11b78f9f4a3d85b0469c61c02849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD video clip saved to instagram_content/scene_0_svd.mp4\n",
      "Generating keyframe image for: Abstract representation of An 80 years old person guiding young generation., thought-provoking, professional.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e248191506d49f7b8f6300cdf33d9f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyframe image saved to instagram_content/scene_1_keyframe.png\n",
      "Generating video from image using SVD...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda300648bfc409481d4ac4fd4871216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD video clip saved to instagram_content/scene_1_svd.mp4\n",
      "VRAM cleared and memory collected.\n",
      "Assembling final video...\n",
      "{'video_found': True, 'audio_found': False, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf61.1.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1024, 576], 'bitrate': 1037, 'fps': 8.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]', 'encoder': 'Lavc61.3.100 libx264'}}], 'input_number': 0}], 'duration': 3.0, 'bitrate': 1040, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1024, 576], 'video_bitrate': 1037, 'video_fps': 8.0, 'video_duration': 3.0, 'video_n_frames': 24}\n",
      "/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i instagram_content/scene_0_svd.mp4 -loglevel error -f image2pipe -vf scale=1024:576 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "{'video_found': True, 'audio_found': False, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf61.1.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [1024, 576], 'bitrate': 470, 'fps': 8.0, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]', 'encoder': 'Lavc61.3.100 libx264'}}], 'input_number': 0}], 'duration': 3.0, 'bitrate': 473, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [1024, 576], 'video_bitrate': 470, 'video_fps': 8.0, 'video_duration': 3.0, 'video_n_frames': 24}\n",
      "/home/gowrav/Development/influencer/.venv/lib/python3.12/site-packages/imageio_ffmpeg/binaries/ffmpeg-linux-x86_64-v7.0.2 -i instagram_content/scene_1_svd.mp4 -loglevel error -f image2pipe -vf scale=1024:576 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "MoviePy - Building video instagram_content/final_reel.mp4.\n",
      "MoviePy - Writing audio in final_reelTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "MoviePy - Writing video instagram_content/final_reel.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready instagram_content/final_reel.mp4\n",
      "Final video saved to instagram_content/final_reel.mp4\n",
      "\n",
      "--- AUTOMATION COMPLETE ---\n",
      "Final Video: instagram_content/final_reel.mp4\n",
      "Suggested Instagram Caption Text:\n",
      "A short segment about An 80 years old person guiding young generation.. Concluding thoughts on the future.\n",
      "Suggested Hashtags: #An80yearsoldpersonguidingyounggeneration., #AI, #GeneratedContent\n",
      "Cleaning up any remaining models from VRAM...\n",
      "VRAM cleared and memory collected.\n",
      "Cleanup finished.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Developed by Gowrav Vishwakarma \n",
    "# https://www.linkedin.com/in/gowravvishwakarma/\n",
    "\n",
    "# main_script.py\n",
    "import os\n",
    "# Set PyTorch memory management\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from diffusers import StableDiffusionXLPipeline, StableVideoDiffusionPipeline, DiffusionPipeline\n",
    "from diffusers.utils import load_image, export_to_video\n",
    "from TTS.api import TTS # Coqui TTS\n",
    "from moviepy import VideoFileClip, AudioFileClip, concatenate_videoclips, TextClip, CompositeVideoClip\n",
    "from moviepy.audio.AudioClip import concatenate_audioclips, AudioClip\n",
    "import gc # Garbage collection\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ContentConfig:\n",
    "    \"\"\"Configuration for content generation\"\"\"\n",
    "    # Video settings\n",
    "    target_video_length: float = 30.0  # Target total video length in seconds\n",
    "    max_scene_length: float = 3.0      # Maximum length of each scene in seconds\n",
    "    target_resolution: tuple = (1080, 1920)  # Instagram Reel 9:16\n",
    "    fps: int = 8\n",
    "    \n",
    "    # Scene settings\n",
    "    min_scenes: int = 2\n",
    "    max_scenes: int = 3\n",
    "    \n",
    "    # Model settings\n",
    "    use_svd_flow: bool = True  # Use SDXL -> SVD flow instead of direct T2V\n",
    "    \n",
    "    # Output settings\n",
    "    output_dir: str = \"instagram_content\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- 0. CONFIGURATION ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "OUTPUT_DIR = \"instagram_content\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# LLM paths (example, download these first)\n",
    "LLM_MODEL_ID = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "# TTS model (XTTSv2 example)\n",
    "TTS_MODEL_ID = \"tts_models/multilingual/multi-dataset/xtts_v2\" # or \"tts_models/en/ljspeech/tacotron2-DDC\"\n",
    "# T2I model\n",
    "T2I_MODEL_ID = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "T2I_REFINER_ID = \"stabilityai/stable-diffusion-xl-refiner-1.0\"\n",
    "# I2V model (SVD)\n",
    "I2V_MODEL_ID = \"stabilityai/stable-video-diffusion-img2vid-xt\"\n",
    "# T2V model (alternative)\n",
    "T2V_MODEL_ID = \"THUDM/CogVideoX-5b\"\n",
    "\n",
    "\n",
    "\n",
    "# --- 1. INITIALIZE MODELS (Load only when needed or keep loaded if VRAM allows) ---\n",
    "def load_llm():\n",
    "    print(\"Loading LLM...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_ID)\n",
    "    # Ensure pad_token is set if model doesn't have one; often same as eos_token for CausalLMs\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        LLM_MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    # We won't use the pipeline directly for chat templating, but generate manually\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def load_tts():\n",
    "    print(\"Loading TTS model...\")\n",
    "    # Make sure you have the model downloaded or it will download on first run\n",
    "    # For XTTSv2, you might need to specify a speaker_wav for voice cloning\n",
    "    return TTS(model_name=TTS_MODEL_ID, progress_bar=True).to(DEVICE)\n",
    "\n",
    "def load_t2i_pipeline():\n",
    "    print(\"Loading T2I pipeline (SDXL)...\")\n",
    "    pipe = StableDiffusionXLPipeline.from_pretrained(\n",
    "        T2I_MODEL_ID, torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    "    ).to(DEVICE)\n",
    "    # Optional: Load refiner if you want to use it\n",
    "    # refiner = DiffusionPipeline.from_pretrained(\n",
    "    #     T2I_REFINER_ID, text_encoder_2=pipe.text_encoder_2, vae=pipe.vae,\n",
    "    #     torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\"\n",
    "    # ).to(DEVICE)\n",
    "    # return pipe, refiner\n",
    "    return pipe, None # Simpler for now\n",
    "\n",
    "def load_i2v_pipeline(): # SVD\n",
    "    print(\"Loading I2V pipeline (SVD)...\")\n",
    "    pipe = StableVideoDiffusionPipeline.from_pretrained(\n",
    "        I2V_MODEL_ID, torch_dtype=torch.float16, variant=\"fp16\"\n",
    "    ).to(DEVICE)\n",
    "    pipe.enable_model_cpu_offload() # If VRAM is tight with other models\n",
    "    return pipe\n",
    "\n",
    "def load_t2v_pipeline(): # ModelScope Text-to-Video\n",
    "    print(\"Loading T2V pipeline (ModelScope)...\")\n",
    "    # Use DiffusionPipeline as per the documentation for damo-vilab/text-to-video-ms-1.7b\n",
    "    pipe = DiffusionPipeline.from_pretrained(\n",
    "        T2V_MODEL_ID, torch_dtype=torch.float16\n",
    "        # , variant=\"fp16\"\n",
    "    ).to(DEVICE)\n",
    "    # The .enable_model_cpu_offload() should still work if the loaded pipe supports it\n",
    "    # For ModelScope, this is standard.\n",
    "    pipe.enable_model_cpu_offload()\n",
    "    return pipe\n",
    "\n",
    "\n",
    "\n",
    "# --- UTILITY TO CLEAR VRAM ---\n",
    "def clear_vram(*models_or_pipelines):\n",
    "    for item in models_or_pipelines:\n",
    "        if hasattr(item, 'cpu') and callable(getattr(item, 'cpu')):\n",
    "            item.cpu() # If it's a pipeline/model with a .cpu() method\n",
    "        elif hasattr(item, 'model') and hasattr(item.model, 'cpu') and callable(getattr(item.model, 'cpu')):\n",
    "            item.model.cpu()\n",
    "    del models_or_pipelines\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"VRAM cleared and memory collected.\")\n",
    "\n",
    "\n",
    "\n",
    "# --- 2. TEXT GENERATION ---\n",
    "def generate_script_and_prompts_with_chat_template(topic, model, tokenizer, config: ContentConfig):\n",
    "    print(f\"Generating script and prompts for topic (chat template): {topic}\")\n",
    "\n",
    "    # Define the messages for the chat template\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an AI assistant creating content for an Instagram Reel. Your response must be in valid JSON format with the following structure: {\\\"narration\\\": [{\\\"scene\\\": 1, \\\"text\\\": \\\"text\\\", \\\"duration\\\": seconds}], \\\"visuals\\\": [{\\\"scene\\\": 1, \\\"prompt\\\": \\\"prompt\\\"}], \\\"hashtags\\\": [\\\"tag1\\\", \\\"tag2\\\"]}\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "            Create content for an Instagram Reel about \"{topic}\".\n",
    "            The Reel should be engaging and around {config.target_video_length} seconds long.\n",
    "            Each scene should be around {config.max_scene_length} seconds.\n",
    "            Generate between {config.min_scenes} and {config.max_scenes} scenes.\n",
    "            \n",
    "            Return your response in this exact JSON format:\n",
    "            {{\n",
    "                \"narration\": [\n",
    "                    {{\"scene\": 1, \"text\": \"First scene narration\", \"duration\": 3.0}},\n",
    "                    {{\"scene\": 2, \"text\": \"Second scene narration\", \"duration\": 3.0}}\n",
    "                ],\n",
    "                \"visuals\": [\n",
    "                    {{\"scene\": 1, \"prompt\": \"Detailed visual prompt for scene 1, with camera apurture to details of object and actors in scene, also tell the style if its gibli, realistic or cartoon\"}},\n",
    "                    {{\"scene\": 2, \"prompt\": \"Detailed visual prompt for scene 2, with camera apurture to details of object and actors in scene, also tell the style if its gibli, realistic or cartoon\"}}\n",
    "                ],\n",
    "                \"hashtags\": [\"tag1\", \"tag2\", \"tag3\"]\n",
    "            }}\n",
    "            \n",
    "            Make sure the total duration of all scenes matches approximately {config.target_video_length} seconds.\n",
    "            Each visual prompt should be detailed and suitable for image/video generation.\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Apply the chat template\n",
    "    tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate response\n",
    "    generation_kwargs = {\n",
    "        \"input_ids\": tokenized_chat,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"do_sample\": True,\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.95,\n",
    "        \"temperature\": 0.7,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "\n",
    "    outputs = model.generate(**generation_kwargs)\n",
    "    decoded_output = tokenizer.decode(outputs[0][tokenized_chat.shape[-1]:], skip_special_tokens=True)\n",
    "    print(\"LLM Response (model generated part only with chat template):\\n\", decoded_output)\n",
    "\n",
    "    try:\n",
    "        # Try to parse the JSON response\n",
    "        response_data = json.loads(decoded_output)\n",
    "        \n",
    "        # Extract and validate the data\n",
    "        narration_scenes = []\n",
    "        visual_prompts = []\n",
    "        hashtags = []\n",
    "        \n",
    "        # Sort scenes by scene number to ensure correct order\n",
    "        narration_data = sorted(response_data.get(\"narration\", []), key=lambda x: x[\"scene\"])\n",
    "        visuals_data = sorted(response_data.get(\"visuals\", []), key=lambda x: x[\"scene\"])\n",
    "        \n",
    "        for scene in narration_data:\n",
    "            narration_scenes.append({\n",
    "                \"text\": scene[\"text\"],\n",
    "                \"duration\": float(scene.get(\"duration\", config.max_scene_length))\n",
    "            })\n",
    "            \n",
    "        for scene in visuals_data:\n",
    "            visual_prompts.append(scene[\"prompt\"])\n",
    "            \n",
    "        hashtags = response_data.get(\"hashtags\", [])\n",
    "        \n",
    "        # Validate we have matching number of scenes\n",
    "        if len(narration_scenes) != len(visual_prompts):\n",
    "            raise ValueError(\"Mismatch between number of narration and visual scenes\")\n",
    "            \n",
    "        # Validate scene count\n",
    "        if not (config.min_scenes <= len(narration_scenes) <= config.max_scenes):\n",
    "            raise ValueError(f\"Scene count {len(narration_scenes)} outside allowed range [{config.min_scenes}, {config.max_scenes}]\")\n",
    "            \n",
    "        # Validate total duration\n",
    "        total_duration = sum(scene[\"duration\"] for scene in narration_scenes)\n",
    "        if abs(total_duration - config.target_video_length) > 5:  # Allow 5 second tolerance\n",
    "            print(f\"Warning: Total duration {total_duration}s differs from target {config.target_video_length}s\")\n",
    "            \n",
    "    except (json.JSONDecodeError, ValueError) as e:\n",
    "        print(f\"Error parsing LLM response: {e}\")\n",
    "        print(\"Using fallback content...\")\n",
    "        # Fallback content\n",
    "        narration_scenes = [\n",
    "            {\"text\": f\"A short segment about {topic}.\", \"duration\": config.max_scene_length},\n",
    "            {\"text\": \"Concluding thoughts on the future.\", \"duration\": config.max_scene_length}\n",
    "        ]\n",
    "        visual_prompts = [\n",
    "            f\"Cinematic shot of {topic}, vibrant colors, high detail, trending on artstation.\",\n",
    "            f\"Abstract representation of {topic}, thought-provoking, professional.\"\n",
    "        ]\n",
    "        hashtags = [f\"#{topic.replace(' ', '')}\", \"#AI\", \"#GeneratedContent\"]\n",
    "\n",
    "    return narration_scenes, visual_prompts, hashtags\n",
    "\n",
    "\n",
    "\n",
    "# --- 3. AUDIO GENERATION ---\n",
    "def generate_audio(text, tts_model, output_path, speaker_wav=None): # speaker_wav for XTTS\n",
    "    print(f\"Generating audio for: {text}\")\n",
    "    if \"xtts\" in TTS_MODEL_ID.lower() and speaker_wav:\n",
    "        tts_model.tts_to_file(text, speaker_wav=speaker_wav, language=\"en\", file_path=output_path)\n",
    "    else: # For other models like Tacotron\n",
    "        tts_model.tts_to_file(text, file_path=output_path)\n",
    "    print(f\"Audio saved to {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "\n",
    "\n",
    "# --- 4. VISUAL GENERATION ---\n",
    "# Option A: Image (SDXL) then Video (SVD) - Recommended for \"Subject to Video\"\n",
    "def generate_image_then_video(image_prompt, i2v_pipe, t2i_pipe, refiner_pipe, scene_idx, target_duration, config: ContentConfig):\n",
    "    # Generate Image (Keyframe)\n",
    "    print(f\"Generating keyframe image for: {image_prompt}\")\n",
    "    # For SDXL, you can add negative prompts, specify num_inference_steps, guidance_scale etc.\n",
    "    # Using SDXL\n",
    "    image = t2i_pipe(\n",
    "        prompt=image_prompt,\n",
    "        # negative_prompt=\"low quality, blurry, watermark\", # Example\n",
    "        num_inference_steps=30, # SDXL typically needs fewer steps\n",
    "        guidance_scale=7.5,\n",
    "        # If using refiner:\n",
    "        # output_type=\"latent\" if refiner_pipe else \"pil\",\n",
    "    ).images[0]\n",
    "    # if refiner_pipe:\n",
    "    #     image = refiner_pipe(prompt=image_prompt, image=image[None, :]).images[0]\n",
    "\n",
    "    image_path = os.path.join(config.output_dir, f\"scene_{scene_idx}_keyframe.png\")\n",
    "    image.save(image_path)\n",
    "    print(f\"Keyframe image saved to {image_path}\")\n",
    "\n",
    "    # Calculate number of frames needed based on target duration\n",
    "    num_frames = min(int(target_duration * config.fps), 32)  # Limit to 16 frames max\n",
    "    # Ensure we have at least 8 frames (minimum for SVD)\n",
    "    num_frames = max(8, num_frames)\n",
    "    \n",
    "    # Generate Video from Image (SVD)\n",
    "    print(f\"Generating video from image using SVD...\")\n",
    "    # SVD parameters\n",
    "    video_frames = i2v_pipe(\n",
    "        image,\n",
    "        decode_chunk_size=4,  # Reduced from 8 to 4\n",
    "        num_frames=num_frames,\n",
    "        motion_bucket_id=127, # Adjust for more/less motion\n",
    "        fps=config.fps,\n",
    "        noise_aug_strength=0.02 # Default\n",
    "    ).frames[0]\n",
    "\n",
    "    video_clip_path = os.path.join(config.output_dir, f\"scene_{scene_idx}_svd.mp4\")\n",
    "    export_to_video(video_frames, video_clip_path, fps=config.fps)\n",
    "    print(f\"SVD video clip saved to {video_clip_path}\")\n",
    "    return video_clip_path\n",
    "\n",
    "# Option B: Direct Text-to-Video (ModelScope)\n",
    "def generate_direct_video(video_prompt, t2v_pipe, scene_idx, target_duration, config: ContentConfig):\n",
    "    print(f\"Generating direct video for: {video_prompt}\")\n",
    "    # Calculate number of frames needed based on target duration\n",
    "    num_frames = int(target_duration * config.fps)\n",
    "    # Ensure we have at least 8 frames\n",
    "    num_frames = max(8, num_frames)\n",
    "    \n",
    "    # ModelScope parameters\n",
    "    video_frames = t2v_pipe(video_prompt, num_inference_steps=25, num_frames=num_frames).frames[0]\n",
    "    video_clip_path = os.path.join(config.output_dir, f\"scene_{scene_idx}_t2v.mp4\")\n",
    "    export_to_video(video_frames, video_clip_path, fps=config.fps)\n",
    "    print(f\"T2V video clip saved to {video_clip_path}\")\n",
    "    return video_clip_path\n",
    "\n",
    "\n",
    "\n",
    "# --- 5. VIDEO ASSEMBLY ---\n",
    "def assemble_final_video(video_clip_paths, audio_clip_paths, narration_parts, config: ContentConfig, output_filename=\"final_reel.mp4\"):\n",
    "    print(\"Assembling final video...\")\n",
    "    final_clips = []\n",
    "    source_clips_to_close = []\n",
    "\n",
    "    # Use a specific, existing font path\n",
    "    font_path_for_textclip = \"/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf\"\n",
    "    if not os.path.exists(font_path_for_textclip):\n",
    "        print(f\"Warning: Font file not found at: {font_path_for_textclip}\")\n",
    "        print(\"Using system default font...\")\n",
    "        font_path_for_textclip = None\n",
    "\n",
    "    for i, (video_path, audio_path, narration) in enumerate(zip(video_clip_paths, audio_clip_paths, narration_parts)):\n",
    "        # Load video and audio clips\n",
    "        video_clip_temp = VideoFileClip(video_path)\n",
    "        audio_clip_temp = AudioFileClip(audio_path)\n",
    "        source_clips_to_close.extend([video_clip_temp, audio_clip_temp])\n",
    "\n",
    "        # Get durations\n",
    "        video_duration = video_clip_temp.duration\n",
    "        audio_duration = audio_clip_temp.duration\n",
    "        target_duration = narration[\"duration\"]\n",
    "\n",
    "        # Resize video to target resolution\n",
    "        video_clip_resized = video_clip_temp.resized(height=config.target_resolution[1])\n",
    "        if video_clip_resized.w > config.target_resolution[0]:\n",
    "            video_clip_final_shape = video_clip_resized.cropped(x_center=video_clip_resized.w/2, width=config.target_resolution[0])\n",
    "        else:\n",
    "            video_clip_final_shape = video_clip_resized\n",
    "\n",
    "        # Position video in center\n",
    "        video_clip_positioned = video_clip_final_shape.with_position('center')\n",
    "\n",
    "        # Handle duration mismatches\n",
    "        if video_duration > target_duration:\n",
    "            # If video is longer, trim it\n",
    "            video_clip_timed = video_clip_positioned.subclipped(0, target_duration)\n",
    "        else:\n",
    "            # If video is shorter, loop it\n",
    "            n_loops = int(np.ceil(target_duration / video_duration))\n",
    "            video_clip_timed = concatenate_videoclips([video_clip_positioned] * n_loops)\n",
    "            video_clip_timed = video_clip_timed.subclipped(0, target_duration)\n",
    "\n",
    "        # Handle audio duration\n",
    "        if audio_duration > target_duration:\n",
    "            # If audio is longer, trim it\n",
    "            audio_clip_timed = audio_clip_temp.subclipped(0, target_duration)\n",
    "        else:\n",
    "            # If audio is shorter, pad with silence\n",
    "            silence_duration = target_duration - audio_duration\n",
    "            # Create a silent audio clip (zero signal) with specific duration\n",
    "            silence = AudioClip(frame_function=lambda t: 0, duration=silence_duration)\n",
    "            audio_clip_timed = concatenate_audioclips([audio_clip_temp, silence])\n",
    "\n",
    "        # Combine video and audio\n",
    "        video_clip_with_audio = video_clip_timed.with_audio(audio_clip_timed)\n",
    "\n",
    "        # Add text caption\n",
    "        txt_clip_temp = TextClip(\n",
    "            font_path_for_textclip,\n",
    "            text=narration[\"text\"],\n",
    "            font_size=60,\n",
    "            color='white',\n",
    "            stroke_color='black',\n",
    "            stroke_width=2,\n",
    "            method='caption',\n",
    "            size=(int(config.target_resolution[0]*0.8), None)\n",
    "        )\n",
    "        source_clips_to_close.append(txt_clip_temp)\n",
    "\n",
    "        txt_clip_final = txt_clip_temp.with_position(('center', 0.8), relative=True).with_duration(target_duration)\n",
    "\n",
    "        # Combine video and text\n",
    "        scene_composite = CompositeVideoClip([video_clip_with_audio, txt_clip_final], size=config.target_resolution)\n",
    "        final_clips.append(scene_composite)\n",
    "\n",
    "    if not final_clips:\n",
    "        print(\"No clips to assemble!\")\n",
    "        for clip_to_close in source_clips_to_close:\n",
    "            if hasattr(clip_to_close, 'close') and callable(getattr(clip_to_close, 'close')):\n",
    "                clip_to_close.close()\n",
    "        return None\n",
    "\n",
    "    # Concatenate all scenes\n",
    "    final_video = concatenate_videoclips(final_clips, method=\"compose\")\n",
    "    final_video_path = os.path.join(config.output_dir, output_filename)\n",
    "\n",
    "    try:\n",
    "        final_video.write_videofile(\n",
    "            final_video_path,\n",
    "            fps=config.fps,\n",
    "            codec=\"libx264\",\n",
    "            audio_codec=\"aac\",\n",
    "            threads=4,\n",
    "            preset=\"medium\",\n",
    "            logger='bar'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during video writing: {e}\")\n",
    "        print(\"Make sure ffmpeg is correctly installed and accessible by MoviePy.\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    finally:\n",
    "        # Clean up all clips\n",
    "        for clip_to_close in source_clips_to_close:\n",
    "            if hasattr(clip_to_close, 'close') and callable(getattr(clip_to_close, 'close')):\n",
    "                try:\n",
    "                    clip_to_close.close()\n",
    "                except Exception as e_close:\n",
    "                    print(f\"Error closing clip {type(clip_to_close)}: {e_close}\")\n",
    "        if hasattr(final_video, 'close') and callable(getattr(final_video, 'close')):\n",
    "            try:\n",
    "                final_video.close()\n",
    "            except Exception as e_close:\n",
    "                print(f\"Error closing final_video: {e_close}\")\n",
    "\n",
    "    print(f\"Final video saved to {final_video_path}\")\n",
    "    return final_video_path\n",
    "\n",
    "\n",
    "\n",
    "# --- MAIN WORKFLOW ---\n",
    "def main_automation_flow(topic, config: Optional[ContentConfig] = None):\n",
    "    if config is None:\n",
    "        config = ContentConfig()\n",
    "\n",
    "    # --- Load models ---\n",
    "    # Manage VRAM: Load one by one or use .cpu_offload()\n",
    "    llm = None\n",
    "    tts_model = None\n",
    "    t2i_pipe, refiner = None, None\n",
    "    i2v_pipe = None\n",
    "    t2v_pipe = None\n",
    "    final_video_path = None\n",
    "    speaker_reference_audio = \"record_out.wav\" # REQUIRED for XTTS voice cloning\n",
    "\n",
    "    try:\n",
    "        # 1. LLM for Script and Prompts\n",
    "        llm_model, llm_tokenizer = load_llm()\n",
    "        narration_scenes, visual_prompts_scenes, hashtags = generate_script_and_prompts_with_chat_template(\n",
    "            topic, llm_model, llm_tokenizer, config\n",
    "        )\n",
    "        clear_vram(llm_model)\n",
    "\n",
    "        # 2. TTS for Narration\n",
    "        tts_model = load_tts()\n",
    "        audio_paths = []\n",
    "        for i, scene in enumerate(narration_scenes):\n",
    "            audio_file = os.path.join(config.output_dir, f\"scene_{i}_audio.wav\")\n",
    "            generate_audio(\n",
    "                scene[\"text\"], \n",
    "                tts_model, \n",
    "                audio_file, \n",
    "                speaker_wav=speaker_reference_audio if \"xtts\" in TTS_MODEL_ID.lower() else None\n",
    "            )\n",
    "            audio_paths.append(audio_file)\n",
    "        clear_vram(tts_model)\n",
    "\n",
    "        # 3. Visual Generation\n",
    "        video_clip_paths = []\n",
    "        if config.use_svd_flow: # Image (SDXL) -> Video (SVD)\n",
    "            t2i_pipe, refiner = load_t2i_pipeline() # SDXL\n",
    "            i2v_pipe = load_i2v_pipeline()         # SVD\n",
    "            for i, (visual_prompt, scene) in enumerate(zip(visual_prompts_scenes, narration_scenes)):\n",
    "                clip_path = generate_image_then_video(\n",
    "                    visual_prompt, \n",
    "                    i2v_pipe, \n",
    "                    t2i_pipe, \n",
    "                    refiner, \n",
    "                    i,\n",
    "                    scene[\"duration\"],\n",
    "                    config\n",
    "                )\n",
    "                video_clip_paths.append(clip_path)\n",
    "            clear_vram(t2i_pipe, refiner, i2v_pipe)\n",
    "        else: # Direct Text-to-Video (ModelScope)\n",
    "            t2v_pipe = load_t2v_pipeline()\n",
    "            for i, (visual_prompt, scene) in enumerate(zip(visual_prompts_scenes, narration_scenes)):\n",
    "                clip_path = generate_direct_video(\n",
    "                    visual_prompt, \n",
    "                    t2v_pipe, \n",
    "                    i,\n",
    "                    scene[\"duration\"],\n",
    "                    config\n",
    "                )\n",
    "                video_clip_paths.append(clip_path)\n",
    "            clear_vram(t2v_pipe)\n",
    "\n",
    "        # 4. Video Assembly\n",
    "        if video_clip_paths and audio_paths:\n",
    "            final_video_path = assemble_final_video(\n",
    "                video_clip_paths, \n",
    "                audio_paths, \n",
    "                narration_scenes,\n",
    "                config\n",
    "            )\n",
    "        else:\n",
    "            print(\"Not enough assets generated to assemble video.\")\n",
    "\n",
    "        # 5. Output final info\n",
    "        if final_video_path:\n",
    "            print(\"\\n--- AUTOMATION COMPLETE ---\")\n",
    "            print(f\"Final Video: {final_video_path}\")\n",
    "            print(f\"Suggested Instagram Caption Text:\\n{' '.join([scene['text'] for scene in narration_scenes])}\")\n",
    "            print(f\"Suggested Hashtags: {', '.join(hashtags)}\")\n",
    "        else:\n",
    "            print(\"\\n--- AUTOMATION FAILED ---\")\n",
    "            print(\"Check logs for errors.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        # Ensure all models are cleared from VRAM if they were loaded\n",
    "        print(\"Cleaning up any remaining models from VRAM...\")\n",
    "        models_to_clear = [m for m in [llm, tts_model, t2i_pipe, refiner, i2v_pipe, t2v_pipe] if m is not None]\n",
    "        if models_to_clear:\n",
    "            clear_vram(*models_to_clear)\n",
    "        print(\"Cleanup finished.\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- IMPORTANT SETUP ---\n",
    "    # 1. Install dependencies:\n",
    "    #    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "    #    pip install transformers accelerate bitsandbytes diffusers TTS moviepy Pillow safetensors sentencepiece\n",
    "    #    (Make sure CUDA version in pytorch matches your system's CUDA Toolkit)\n",
    "    # 2. Download models:\n",
    "    #    The first time you run, Hugging Face models will download.\n",
    "    #    For Coqui TTS, ensure you have the model files (XTTSv2 needs manual download or will try).\n",
    "    # 3. For XTTSv2 voice cloning:\n",
    "    #    Set `speaker_reference_audio` to a path of a clean WAV file of the voice you want to clone.\n",
    "    #    It must be a 16-bit PCM WAV file, ideally >15 seconds long.\n",
    "    # 4. Fonts for MoviePy: Make sure 'Arial-Bold' or your chosen font is available to MoviePy/ImageMagick.\n",
    "    #    If not, ImageMagick might need to be installed and configured, or use a default font.\n",
    "\n",
    "    # --- RUN THE SCRIPT ---\n",
    "    # Example configuration\n",
    "    config = ContentConfig(\n",
    "        target_video_length=30.0,  # 30 seconds total\n",
    "        max_scene_length=3.0,      # 3 seconds per scene\n",
    "        min_scenes=2,\n",
    "        max_scenes=5,\n",
    "        use_svd_flow=True,         # Use SDXL -> SVD flow\n",
    "        fps=8\n",
    "    )\n",
    "\n",
    "    topic_for_reel = \"An 80 years old person guiding young generation.\"\n",
    "    main_automation_flow(topic_for_reel, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a6e59c-e1a9-4364-bb1c-6686818194b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
